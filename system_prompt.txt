<purpose>
  You, as the AI ASSISTANT, must take (a) a list of 100 ranked solutions from [[solutions_100]] and (b) the target deployment constraints from [[constraints_text]], then produce a NEW ranking of consolidated “Solution Bundles” (a.k.a. stacks).

  Each bundle MUST cover, at least, all four domains:
    1) tag recommendation (keyword extraction and/or classification),
    2) example-based similarity search (vector indexes, ANN libraries, or search engine features),
    3) document clustering (topic modeling, clustering engines, or graph-based methods),
    4) near-duplicate detection (hashing, LSH, or deduplication tooling).

  Primary objective: rank bundles from easiest to hardest to deploy on the environment described in [[constraints_text]].
  Secondary objective: maximize coverage quality and operational simplicity.

  Success criteria:
    - Every ranked bundle covers at least all four domains.
    - Any uncertainty is explicitly diminished by leveraging external knowledge via search queries.
</purpose>

<context>
  <grounding_rules>
    <rule>Feel free to search for external knowledge to find any dependency/library/tool/runtime requirement that is not explicitly stated within the [[solutions_100]] and [[constraints_text]].</rule>
    <rule>When ARM64/aarch64 compatibility is not explicitly stated, leverage external knowledge via search queries to find out.</rule>
    <rule>When you make a scoring decision, include at least one supporting quote from [[constraints_text]] or cite the exact solution text that motivated it.</rule>
  </grounding_rules>

  <ranking_principles>
    <principle>Prefer fewer moving parts: fewer services, fewer build steps, fewer always-on daemons.</principle>
    <principle>Prefer components that match the available runtime/toolchain described in [[constraints_text]].</principle>
    <principle>Disregard docker/kvm/virtualization approaches because those capabilities are supported in [[constraints_text]].</principle>
  </ranking_principles>

  <scoring_model>
    <ease_of_deployment>
      <definition>
        A 0–100 score estimating how straightforward it is to install, build (if needed), and operate the bundle on the target machine.
        Score MUST be computed via the rubric below; do not invent package availability.
      </definition>
      <rubric>
        <![CDATA[
        Start at 100 and subtract:
          -40 if requires GPU/CUDA or any GPU-only dependency (unless [[constraints_text]] explicitly supports it, like 'mali G610 (rk3588)' GPU-based approaches).
          -25 if requires a runtime that could not be installed/built within [[constraints_text]] OR requires kernel (5.10) or libc6 (2.31) upgrades.
          -15 for each always-on external service required (e.g., JVM service, DB, search engine) beyond the first.
          -20 if compilation from source on ARM64 is likely AND no prebuilt artifact is stated in [[solutions_100]]; leverage external knowledge via search queries to find out.
          -10 if the bundle implies high peak memory usage AND [[constraints_text]] mentions “no swap” (or other memory constraints).
          -5  for each additional distinct ecosystem/toolchain introduced (Python + Java + Go, etc.).

        Add back (cap at 100):
          +10 if the bundle can be a single-process deployment.
          +10 if it supports incremental updates (append-only index rebuild avoidance) as explicitly stated.
        ]]>
      </rubric>
    </ease_of_deployment>

    <coverage>
      <definition>
        A 0–100 score estimating how explicitly and robustly the bundle covers each required domain.
      </definition>
      <rubric>
        <![CDATA[
        25 points per domain if the bundle names an explicit approach AND at least one explicit tool/library.
        +5 bonus if the approach includes evaluation/metrics hooks (explicitly described).
        -10 penalty if any domain is covered only implicitly (no explicit method/tool named).
        ]]>
      </rubric>
    </coverage>
  </scoring_model>
</context>

<input_data>
  <solutions_100>[[
`````````solutions_100
~~~~~~
"""
Title: Software Solutions for Document Analysis

URL Source: https://x0.at/mqOp.pdf

Published Time: Wed, 14 Jan 2026 15:41:13 GMT

Number of Pages: 20

Markdown Content:
1. Apache Solr – Open-source search engine with features like MoreLikeThis queries for finding similar documents and an update processor for de-duplication via content fingerprints . Solr can also integrate clustering plugins to group search results, making it a broad solution for semantic search, recommendations, clustering, and duplicate detection in one platform. 

2. Elasticsearch – Distributed search and analytics engine that supports a “More Like This” query to recommend similar documents based on text and can be extended with plugins for semantic vector search . Widely used for tag aggregation and suggestion, it can also detect near-duplicates (e.g., via hashing or ingest pipelines) and forms the backbone of many search applications. 

3. OpenSearch – Community-driven fork of Elasticsearch with comparable full-text search capabilities and built-in support for k-Nearest Neighbor vector search for semantic queries . It retains features like the MoreLikeThis query and aggregations, making it suitable for example-based document search and leveraging vector embeddings for finding related content. 

4. Vespa – Scalable open-source search engine by Yahoo/Oath that natively integrates real-time text search with vector search and machine learning models for ranking . Vespa can serve recommendations (“more like this”) using embeddings, perform on-the-fly document clustering, and even execute custom ML inference per query, enabling tag prediction and duplicate detection at search time. 

5. Apache Lucene – Low-level Java search library powering Solr/Elasticsearch. It provides inverted indexing and features such as MoreLikeThis queries for similarity search. As a toolkit, it can be extended with custom similarity functions or plugins (e.g., for LSH) to handle near-duplicate detection and clustering, forming the foundation for many IR systems. 

6. Manticore Search – Open-source search server (fork of Sphinx) that offers full-text search and recent support for vector embeddings, enabling hybrid search (lexical + semantic) with similarity ranking .It’s designed for high performance and supports features like real-time indexing and relevancy tuning; with the new vector search, it can find related documents by semantic similarity and help flag duplicates. 

7. Typesense – Lightweight, typo-tolerant search engine with an easy API, now supporting vector search to combine natural language queries and dense embeddings for relevant results . It excels at instant search and faceting. With hybrid search models, Typesense can power “more like this” recommendations and incorporate tag filters, though large-scale clustering would be handled client-side. 

8. MeiliSearch – Fast, developer-friendly search engine focused on instant search experiences. Primarily lexical, it’s evolving toward semantic capabilities and can be extended with external vector search for “more-like-this” features. Its relevance model and filters can be used to implement basic tag suggestion (e.g., via facets) and to group results, although advanced clustering would require additional tooling. 

9. Open Semantic Search – Suite combining Apache Solr/Elasticsearch with text mining pipelines for enterprise search. It includes semantic enrichment (NER, clustering, summarization) to improve finding related documents and tag suggestions . By indexing with extracted keywords and concepts, it   

> 12
> 1
> 3
> 4
> 5
> 6
> 78

1enables smarter search results, automatic tagging, and can flag duplicate documents using content fingerprints. 

10. Xapian – Proven open-source search library providing ranked retrieval and flexible indexing. While primarily keyword-based, it can be used with statistical techniques or custom embeddings to implement document similarity search and basic clustering. Xapian’s extensibility (e.g., value ranges, posting sources) lets developers add modules for recommending related documents or filtering out near-duplicates. 

11. Weaviate – Open-source vector database that stores data objects with high-dimensional embeddings, allowing semantic queries (“find similar items”) via vector similarity. It offers built-in kNN search and even classification modules to auto-label new documents based on nearest neighbors .This enables example-based search (given one document, find similar ones) and can facilitate document clustering by topic or detecting duplicates through high embedding similarity. 

12. Milvus – Cloud-native vector database designed for billion-scale embedding vectors. It provides fast similarity search and has been used in recommendation and multimedia retrieval . By indexing document embeddings, Milvus enables semantic document search and grouping. It’s often used to find near-duplicates or related documents in large corpora (e.g., detecting redundant news articles via vector closeness). 

13. Qdrant – High-performance vector search engine that supports filtering and payload metadata. Optimized for real-time nearest neighbor search and recommendation systems , Qdrant can store document vectors along with tags/fields and answer “more like this” queries with hybrid (vector + filter) conditions. It’s well-suited for example-based semantic search and pinpointing duplicates when combined with a similarity threshold. 

14. Pinecone – Fully managed vector database service (proprietary) known for speedy similarity search and simple integration. Widely used in production semantic search scenarios, Pinecone handles embedding indexing and kNN retrieval under the hood . It doesn’t directly cluster or tag documents, but as a service it excels at finding related items and powering “if you liked this document, you might like these” features. 

15. Marqo – An open-source semantic search engine that indexes data (text, images) into vectors. It supports multimodal search using embeddings, so you can perform “more-like-this” queries across text and image content . Marqo handles the heavy lifting of embedding via deep-learning models and returns semantically similar documents, which can help in recommending tags (by looking at neighbors’ tags) and grouping related documents on the fly. 

16. Chroma – Simple embedding database and retrieval library for LLM applications. It stores text embeddings and offers a straightforward API for similarity search, making it easy to add semantic “find similar document” functionality to apps . Often used with frameworks like LangChain, Chroma can help cluster content by vector similarity and assist in tag suggestion by retrieving top-N similar documents and aggregating their tags. 

17. Zilliz – Enterprise-grade vector database built by the creators of Milvus. It extends Milvus with a managed platform for production, supporting massive scale and multimodal data . Zilliz is geared towards semantic search and AI applications – for example, it can serve as the backbone for finding near-duplicate multimedia documents or clustering documents by topics derived from embeddings, with enterprise features like security and monitoring. 

> 9
> 10
> 11
> 12
> 13
> 14
> 15

218. Vald – Kubernetes-native distributed vector search engine. It scales horizontally and uses a highly efficient HNSW indexing approach over gRPC . Vald can handle large volumes of vectors and update indices in real-time, which is useful for dynamic document sets. It enables semantic searches (e.g., finding documents similar to a query document’s content) and can form the basis of a deduplication service by continually inserting document vectors and querying for close neighbors. 

19. Redis (Vector Search Module) – In-memory data store Redis now includes a module for vector similarity search using HNSW. This allows combining semantic embedding queries with Redis’s filtering and real-time performance . Developers can store document embeddings alongside tags or metadata in Redis and perform hybrid queries – e.g., “find similar documents to X that share tag Y.” It’s a building block for recommendation systems and duplicate detection in scenarios demanding low-latency. 

20. Apache Cassandra (Vector) – A NoSQL database adding vector search in its 5.0 release .Cassandra can now store embedding vectors and execute similarity searches via new indexing capabilities. This brings semantic search and deduplication use-cases to Cassandra’s scalable, distributed environment – for instance, one could maintain an index of document vectors across a cluster and efficiently query for near-duplicates or related documents using Cassandra’s query language. 

21. PostgreSQL + pgVector – The pgVector extension brings vector similarity queries to Postgres, enabling semantic searches using embeddings. It’s widely adopted to find similar text documents by cosine or inner-product distance within a relational DB . This allows integrating example-based document search or tag suggestion directly into SQL queries (e.g., “find top 5 most similar documents to this article”) and can help identify duplicates by a simple nearest-neighbor query on the embeddings. 

22. Vearch – Open-source vector search system that combines a distributed storage backend with efficient similarity search. Originally developed by Tencent, it’s designed for deep learning vectors and supports indexing and searching billions of vectors. In practice, Vearch can be used to power features like semantic document retrieval (given a query document, find others like it) and duplicate detection at large scale (by periodically checking new document vectors against existing ones). 

23. AquilaDB – A vector database focused on simplicity and distributed operation. It offers a simple REST interface for storing and querying vectors with approximate nearest neighbor search. AquilaDB can be employed for semantic search tasks such as finding related documents via embeddings or grouping documents by content similarity, and it’s built to be easy to deploy for developers who need quick vector search capabilities. 

24. txtai – Python framework for building semantic search applications . It provides an integrated solution to index text with Transformer embeddings and query in natural language. With txtai, one can implement “More Like This” functionality by embedding documents and performing similarity lookup, do zero-shot tag classification (via integrated models), cluster documents by topic, and even detect duplicates by checking for high semantic similarity scores among indexed texts. 

25. Semantra – A multipurpose tool for semantically searching documents . It allows indexing of text documents and uses transformer-based embeddings to enable semantic queries. Semantra can retrieve conceptually similar documents to an example query and is often used to augment traditional search with neural similarity. It’s useful for scenarios like finding related support tickets or knowledge base articles (example-based search) and can highlight duplicates by surfacing very close matches.  

> 16
> 17
> 18
> 19 20
> 21
> 21

326. Infinity – An “AI-native” vector database that offers both vector and traditional full-text search. Geared toward LLM and generative AI applications, Infinity provides fast similarity lookup for embeddings alongside keyword search . This combination allows enriched search experiences – e.g., retrieving documents that are lexically or semantically similar. It supports use-cases like dynamic document clustering and tag generation by similarity, especially in pipelines where new data is continuously being ingested and queried. 

27. SuperDuperDB – A vector database that integrates with traditional databases and allows for on-the-fly model execution. It brings machine learning capabilities (like vector search and even model inference) into the database layer . With SuperDuperDB, developers can store documents and their embeddings and perform similarity queries directly in the DB, enabling semantic search and duplicate detection without a separate ML service. It also can auto-generate tags or perform classification inside the DB using embedded models. 

28. FAISS – Facebook AI Research’s library for efficient similarity search on dense vectors. It supports multiple indexing strategies (IVF, HNSW, PQ) and even clustering of vectors, making it an industry standard for embedding-based search . FAISS is often used behind the scenes in recommendation systems and deduplication pipelines – for example, to quickly find the nearest neighbors of a document vector (potential duplicates or candidates for MoreLikeThis results) in a corpus with millions of entries. 

29. Annoy – Spotify’s C++/Python library (“Approximate Nearest Neighbors Oh Yeah”) using a forest of random projection trees for ANN search. It’s very fast for high-dimensional similarity lookups in memory and well-suited for static datasets . Annoy is commonly used to power item-to-item recommendations or MoreLikeThis -type features on the fly (such as suggesting similar songs or articles) and can handle millions of points efficiently on a single machine. 

30. HNSWlib – A popular C++ library (with Python bindings) implementing the Hierarchical Navigable Small World graph algorithm for approximate nearest neighbors. It yields high recall in finding nearest neighbors with logarithmic search complexity. HNSWlib is integrated into many vector search tools (e.g., as the backend for OpenSearch’s kNN) and can be used stand-alone to build a similarity index for documents – useful for clustering documents by content or quickly finding duplicates based on embedding proximity. 

31. NMSLIB – Non-Metric Space Library, an efficient toolkit for similarity search on vector spaces (including support for non-metric distances). It provides multiple algorithms (like VP-tree, SW-graph, HNSW) and has been used for high-speed nearest neighbor search in research . NMSLIB can index document embeddings and answer semantic similarity queries at scale, enabling functionalities like finding related documents or performing fuzzy de-duplication (with adjustable distance thresholds). 

32. ScaNN – Google’s Scalable Nearest Neighbors library that uses tree quantization and asymmetric hashing for fast vector search. Optimized for Google’s production workloads, it handles large-scale semantic retrieval efficiently and can be tuned for different recall/speed trade-offs. ScaNN is suitable for embedding-based search of documents (e.g., finding similar research papers via embeddings) and can assist in duplicate detection by quickly narrowing down candidate matches from huge corpora. 

33. Elastiknn – An open-source Elasticsearch plugin for approximate nearest neighbor search . It lets you store vectors in Elasticsearch and run kNN queries using various algorithms (LSH, HNSW, etc.), bringing vector search capabilities to the Elastic stack. With Elastiknn, one can enhance an Elasticsearch index to support semantic “more like this” queries on neural embeddings and perform near-duplicate 

> 22
> 21
> 23
> 24
> 20
> 25

4document detection as part of the search queries (for instance, filtering or grouping nearly identical results). 

34. NearPy – Python library for locality-sensitive hashing (LSH) based ANN search. It allows indexing of high-dimensional vectors using hashing techniques (e.g., random binary projections) to bucket similar items together. NearPy can be a quick solution to find candidates for duplicate documents: by hashing document text or embeddings into buckets, it narrows down comparisons. It’s also useful for example-based search where a user provides a document and the system retrieves others from the same hash buckets as potential matches. 

35. SPTAG – Microsoft’s open-source ANN library (“Space Partition Tree and Graph”) for high-dimensional data. It combines tree-based partitioning and a KNN graph to achieve fast search, and was designed to power services like Bing’s search suggestions. SPTAG can index document embeddings and quickly return nearest neighbors, enabling semantic search and clustering on large datasets (e.g., finding all documents that are likely plagiarized versions of each other by embedding similarity). 

36. PyNNDescent – Python implementation of the Nearest Neighbor Descent algorithm (also used in UMAP) for ANN search. It builds an approximate neighborhood graph of points through iterative refinement. PyNNDescent can handle millions of points and update incrementally. For text applications, you can feed it document embeddings and use it to derive clusters of similar documents or find nearest neighbors of a given document efficiently, which is valuable for duplicate detection and recommendation tasks. 

37. NGT – Neighborhood Graph and Tree library from Yahoo Japan for fast ANN search . It provides commands and APIs to build efficient indexes (an optimized combination of a tree and graph) and has been used as a backend in some production systems. NGT can power similarity search for documents or images; in context, it could index document vectors and enable quick retrieval of closely related documents (useful for “find similar document” features or grouping nearly identical items). 

38. FLANN – Fast Library for Approximate Nearest Neighbors (C++ with bindings). An older, battle-tested library that supports multiple ANN algorithms (k-d trees, composite indexing) and automatically chooses the best algorithm for the dataset. FLANN has been used historically for image and text feature matching. In a document context, one could use FLANN to index TF-IDF or paragraph vectors of documents and then query for similar items or duplicates. It’s less commonly used now (surpassed by newer libraries like FAISS/HNSWlib) but remains a solid option for moderate-sized problems. 

39. N2 (Toros) – A lightweight approximate nearest neighbor search library in C++ (with Python interface) implementing the HNSW algorithm. N2 is optimized for memory and speed, making it suitable to deploy within applications. It has been used in production at applications like Upstage (Toros). By using N2 to index document embeddings, one can get very fast “more like this” queries or duplicate lookups in an application with tight latency requirements. 

40. Haystack – A modular Python framework (by deepset) for building search pipelines with modern NLP . It integrates Transformers for embedding, retrievers (Elasticsearch/FAISS/etc.), and readers for QA. Haystack can be configured to perform semantic document retrieval (given an example document or query, find similar docs), question answering, and even generative tag suggestions using LLMs. It’s production-ready and often used to power knowledge base search where clustering of content and duplicate question detection are needed. 

> 26
> 27

541. Jina AI – Open-source framework for building neural search and multimodal applications . It uses a flow 1. Apache Solr – Open-source search engine with features like MoreLikeThis queries for finding similar documents and an update processor for de-duplication via content fingerprints .Solr can also integrate clustering plugins to group search results, making it a broad solution for semantic search, recommendations, clustering, and duplicate detection in one platform. 

2. Elasticsearch – Distributed search and analytics engine that supports a “More Like This” query to recommend similar documents based on text and can be extended with plugins for semantic vector search . Widely used for tag aggregation and suggestion, it can also detect near-duplicates (e.g., via hashing or ingest pipelines) and forms the backbone of many search applications. 

3. OpenSearch – Community-driven fork of Elasticsearch with comparable full-text search capabilities and built-in support for k-Nearest Neighbor vector search for semantic queries . It retains features like the MoreLikeThis query and aggregations, making it suitable for example-based document search and leveraging vector embeddings for finding related content. 

4. Vespa – Scalable open-source search engine by Yahoo/Oath that natively integrates real-time text search with vector search and machine learning models for ranking . Vespa can serve recommendations (“more like this”) using embeddings, perform on-the-fly document clustering, and even execute custom ML inference per query, enabling tag prediction and duplicate detection at search time. 

5. Apache Lucene – Low-level Java search library powering Solr/Elasticsearch. It provides inverted indexing and features such as MoreLikeThis queries for similarity search. As a toolkit, it can be extended with custom similarity functions or plugins (e.g., for LSH) to handle near-duplicate detection and clustering, forming the foundation for many IR systems. 

6. Manticore Search – Open-source search server (fork of Sphinx) that offers full-text search and recent support for vector embeddings, enabling hybrid search (lexical + semantic) with similarity ranking .It’s designed for high performance and supports features like real-time indexing and relevancy tuning; with the new vector search, it can find related documents by semantic similarity and help flag duplicates. 

7. Typesense – Lightweight, typo-tolerant search engine with an easy API, now supporting vector search to combine natural language queries and dense embeddings for relevant results . It excels at instant search and faceting. With hybrid search models, Typesense can power “more like this” recommendations and incorporate tag filters, though large-scale clustering would be handled client-side. 

8. MeiliSearch – Fast, developer-friendly search engine focused on instant search experiences. Primarily lexical, it’s evolving toward semantic capabilities and can be extended with external vector search for “more-like-this” features. Its relevance model and filters can be used to implement basic tag suggestion (e.g., via facets) and to group results, although advanced clustering would require additional tooling. 

9. Open Semantic Search – Suite combining Apache Solr/Elasticsearch with text mining pipelines for enterprise search. It includes semantic enrichment (NER, clustering, summarization) to improve finding related documents and tag suggestions . By indexing with extracted keywords and concepts, it enables smarter search results, automatic tagging, and can flag duplicate documents using content fingerprints.   

> 28
> 12
> 1
> 3
> 4
> 5
> 6
> 78

610. Xapian – Proven open-source search library providing ranked retrieval and flexible indexing. While primarily keyword-based, it can be used with statistical techniques or custom embeddings to implement document similarity search and basic clustering. Xapian’s extensibility (e.g., value ranges, posting sources) lets developers add modules for recommending related documents or filtering out near-duplicates. 

11. Weaviate – Open-source vector database that stores data objects with high-dimensional embeddings, allowing semantic queries (“find similar items”) via vector similarity. It offers built-in kNN search and even classification modules to auto-label new documents based on nearest neighbors .This enables example-based search (given one document, find similar ones) and can facilitate document clustering by topic or detecting duplicates through high embedding similarity. 

12. Milvus – Cloud-native vector database designed for billion-scale embedding vectors. It provides fast similarity search and has been used in recommendation and multimedia retrieval . By indexing document embeddings, Milvus enables semantic document search and grouping. It’s often used to find near-duplicates or related documents in large corpora (e.g., detecting redundant news articles via vector closeness). 

13. Qdrant – High-performance vector search engine that supports filtering and payload metadata. Optimized for real-time nearest neighbor search and recommendation systems , Qdrant can store document vectors along with tags/fields and answer “more like this” queries with hybrid (vector + filter) conditions. It’s well-suited for example-based semantic search and pinpointing duplicates when combined with a similarity threshold. 

14. Pinecone – Fully managed vector database service (proprietary) known for speedy similarity search and simple integration. Widely used in production semantic search scenarios, Pinecone handles embedding indexing and kNN retrieval under the hood . It doesn’t directly cluster or tag documents, but as a service it excels at finding related items and powering “if you liked this document, you might like these” features. 

15. Marqo – An open-source semantic search engine that indexes data (text, images) into vectors. It supports multimodal search using embeddings, so you can perform “more-like-this” queries across text and image content . Marqo handles the heavy lifting of embedding via deep-learning models and returns semantically similar documents, which can help in recommending tags (by looking at neighbors’ tags) and grouping related documents on the fly. 

16. Chroma – Simple embedding database and retrieval library for LLM applications. It stores text embeddings and offers a straightforward API for similarity search, making it easy to add semantic “find similar document” functionality to apps . Often used with frameworks like LangChain, Chroma can help cluster content by vector similarity and assist in tag suggestion by retrieving top-N similar documents and aggregating their tags. 

17. Zilliz – Enterprise-grade vector database built by the creators of Milvus. It extends Milvus with a managed platform for production, supporting massive scale and multimodal data . Zilliz is geared towards semantic search and AI applications – for example, it can serve as the backbone for finding near-duplicate multimedia documents or clustering documents by topics derived from embeddings, with enterprise features like security and monitoring. 

18. Vald – Kubernetes-native distributed vector search engine. It scales horizontally and uses a highly efficient HNSW indexing approach over gRPC . Vald can handle large volumes of vectors and update 

> 9
> 10
> 11
> 12
> 13
> 14
> 15
> 16

7indices in real-time, which is useful for dynamic document sets. It enables semantic searches (e.g., finding documents similar to a query document’s content) and can form the basis of a deduplication service by continually inserting document vectors and querying for close neighbors. 

19. Redis (Vector Search Module) – In-memory data store Redis now includes a module for vector similarity search using HNSW. This allows combining semantic embedding queries with Redis’s filtering and real-time performance . Developers can store document embeddings alongside tags or metadata in Redis and perform hybrid queries – e.g., “find similar documents to X that share tag Y.” It’s a building block for recommendation systems and duplicate detection in scenarios demanding low-latency. 

20. Apache Cassandra (Vector) – A NoSQL database adding vector search in its 5.0 release .Cassandra can now store embedding vectors and execute similarity searches via new indexing capabilities. This brings semantic search and deduplication use-cases to Cassandra’s scalable, distributed environment – for instance, one could maintain an index of document vectors across a cluster and efficiently query for near-duplicates or related documents using Cassandra’s query language. 

21. PostgreSQL + pgVector – The pgVector extension brings vector similarity queries to Postgres, enabling semantic searches using embeddings. It’s widely adopted to find similar text documents by cosine or inner-product distance within a relational DB . This allows integrating example-based document search or tag suggestion directly into SQL queries (e.g., “find top 5 most similar documents to this article”) and can help identify duplicates by a simple nearest-neighbor query on the embeddings. 

22. Vearch – Open-source vector search system that combines a distributed storage backend with efficient similarity search. Originally developed by Tencent, it’s designed for deep learning vectors and supports indexing and searching billions of vectors. In practice, Vearch can be used to power features like semantic document retrieval (given a query document, find others like it) and duplicate detection at large scale (by periodically checking new document vectors against existing ones). 

23. AquilaDB – A vector database focused on simplicity and distributed operation. It offers a simple REST interface for storing and querying vectors with approximate nearest neighbor search. AquilaDB can be employed for semantic search tasks such as finding related documents via embeddings or grouping documents by content similarity, and it’s built to be easy to deploy for developers who need quick vector search capabilities. 

24. txtai – Python framework for building semantic search applications . It provides an integrated solution to index text with Transformer embeddings and query in natural language. With txtai, one can implement “More Like This” functionality by embedding documents and performing similarity lookup, do zero-shot tag classification (via integrated models), cluster documents by topic, and even detect duplicates by checking for high semantic similarity scores among indexed texts. 

25. Semantra – A multipurpose tool for semantically searching documents . It allows indexing of text documents and uses transformer-based embeddings to enable semantic queries. Semantra can retrieve conceptually similar documents to an example query and is often used to augment traditional search with neural similarity. It’s useful for scenarios like finding related support tickets or knowledge base articles (example-based search) and can highlight duplicates by surfacing very close matches. 

26. Infinity – An “AI-native” vector database that offers both vector and traditional full-text search. Geared toward LLM and generative AI applications, Infinity provides fast similarity lookup for  

> 17
> 18
> 19 20
> 21
> 21

8embeddings alongside keyword search . This combination allows enriched search experiences – e.g., retrieving documents that are lexically or semantically similar. It supports use-cases like dynamic document clustering and tag generation by similarity, especially in pipelines where new data is continuously being ingested and queried. 

27. SuperDuperDB – A vector database that integrates with traditional databases and allows for on-the-fly model execution. It brings machine learning capabilities (like vector search and even model inference) into the database layer . With SuperDuperDB, developers can store documents and their embeddings and perform similarity queries directly in the DB, enabling semantic search and duplicate detection without a separate ML service. It also can auto-generate tags or perform classification inside the DB using embedded models. 

28. FAISS – Facebook AI Research’s library for efficient similarity search on dense vectors. It supports multiple indexing strategies (IVF, HNSW, PQ) and even clustering of vectors, making it an industry standard for embedding-based search . FAISS is often used behind the scenes in recommendation systems and deduplication pipelines – for example, to quickly find the nearest neighbors of a document vector (potential duplicates or candidates for MoreLikeThis results) in a corpus with millions of entries. 

29. Annoy – Spotify’s C++/Python library (“Approximate Nearest Neighbors Oh Yeah”) using a forest of random projection trees for ANN search. It’s very fast for high-dimensional similarity lookups in memory and well-suited for static datasets . Annoy is commonly used to power item-to-item recommendations or MoreLikeThis -type features on the fly (such as suggesting similar songs or articles) and can handle millions of points efficiently on a single machine. 

30. HNSWlib – A popular C++ library (with Python bindings) implementing the Hierarchical Navigable Small World graph algorithm for approximate nearest neighbors. It yields high recall in finding nearest neighbors with logarithmic search complexity. HNSWlib is integrated into many vector search tools (e.g., as the backend for OpenSearch’s kNN) and can be used stand-alone to build a similarity index for documents – useful for clustering documents by content or quickly finding duplicates based on embedding proximity. 

31. NMSLIB – Non-Metric Space Library, an efficient toolkit for similarity search on vector spaces (including support for non-metric distances). It provides multiple algorithms (like VP-tree, SW-graph, HNSW) and has been used for high-speed nearest neighbor search in research . NMSLIB can index document embeddings and answer semantic similarity queries at scale, enabling functionalities like finding related documents or performing fuzzy de-duplication (with adjustable distance thresholds). 

32. ScaNN – Google’s Scalable Nearest Neighbors library that uses tree quantization and asymmetric hashing for fast vector search. Optimized for Google’s production workloads, it handles large-scale semantic retrieval efficiently and can be tuned for different recall/speed trade-offs. ScaNN is suitable for embedding-based search of documents (e.g., finding similar research papers via embeddings) and can assist in duplicate detection by quickly narrowing down candidate matches from huge corpora. 

33. Elastiknn – An open-source Elasticsearch plugin for approximate nearest neighbor search . It lets you store vectors in Elasticsearch and run kNN queries using various algorithms (LSH, HNSW, etc.), bringing vector search capabilities to the Elastic stack. With Elastiknn, one can enhance an Elasticsearch index to support semantic “more like this” queries on neural embeddings and perform near-duplicate document detection as part of the search queries (for instance, filtering or grouping nearly identical results). 

> 22
> 21
> 23
> 24
> 20
> 25

934. NearPy – Python library for locality-sensitive hashing (LSH) based ANN search. It allows indexing of high-dimensional vectors using hashing techniques (e.g., random binary projections) to bucket similar items together. NearPy can be a quick solution to find candidates for duplicate documents: by hashing document text or embeddings into buckets, it narrows down comparisons. It’s also useful for example-based search where a user provides a document and the system retrieves others from the same hash buckets as potential matches. 

35. SPTAG – Microsoft’s open-source ANN library (“Space Partition Tree and Graph”) for high-dimensional data. It combines tree-based partitioning and a KNN graph to achieve fast search, and was designed to power services like Bing’s search suggestions. SPTAG can index document embeddings and quickly return nearest neighbors, enabling semantic search and clustering on large datasets (e.g., finding all documents that are likely plagiarized versions of each other by embedding similarity). 

36. PyNNDescent – Python implementation of the Nearest Neighbor Descent algorithm (also used in UMAP) for ANN search. It builds an approximate neighborhood graph of points through iterative refinement. PyNNDescent can handle millions of points and update incrementally. For text applications, you can feed it document embeddings and use it to derive clusters of similar documents or find nearest neighbors of a given document efficiently, which is valuable for duplicate detection and recommendation tasks. 

37. NGT – Neighborhood Graph and Tree library from Yahoo Japan for fast ANN search . It provides commands and APIs to build efficient indexes (an optimized combination of a tree and graph) and has been used as a backend in some production systems. NGT can power similarity search for documents or images; in context, it could index document vectors and enable quick retrieval of closely related documents (useful for “find similar document” features or grouping nearly identical items). 

38. FLANN – Fast Library for Approximate Nearest Neighbors (C++ with bindings). An older, battle-tested library that supports multiple ANN algorithms (k-d trees, composite indexing) and automatically chooses the best algorithm for the dataset. FLANN has been used historically for image and text feature matching. In a document context, one could use FLANN to index TF-IDF or paragraph vectors of documents and then query for similar items or duplicates. It’s less commonly used now (surpassed by newer libraries like FAISS/HNSWlib) but remains a solid option for moderate-sized problems. 

39. N2 (Toros) – A lightweight approximate nearest neighbor search library in C++ (with Python interface) implementing the HNSW algorithm. N2 is optimized for memory and speed, making it suitable to deploy within applications. It has been used in production at applications like Upstage (Toros). By using N2 to index document embeddings, one can get very fast “more like this” queries or duplicate lookups in an application with tight latency requirements. 

40. Haystack – A modular Python framework (by deepset) for building search pipelines with modern NLP . It integrates Transformers for embedding, retrievers (Elasticsearch/FAISS/etc.), and readers for QA. Haystack can be configured to perform semantic document retrieval (given an example document or query, find similar docs), question answering, and even generative tag suggestions using LLMs. It’s production-ready and often used to power knowledge base search where clustering of content and duplicate question detection are needed. 

41. Jina AI – Open-source framework for building cloud-native neural search apps using a flow-based architecture . Jina simplifies the creation of multi-modal search (text, images, etc.) by orchestrating embedding, indexing, and querying microservices. It can implement semantic document search (“find 

> 26
> 27
> 28

10 similar documents”) and clustering as part of its pipelines, and has been used for tasks like duplicate question detection and content recommendation. 

42. LangChain – Toolkit for developing applications that combine LLMs with external data. LangChain provides components to integrate language models with vector databases and other tools. For example, one can use LangChain to generate embeddings for documents and query a vector store for similar documents (enabling semantic MoreLikeThis ), or to automate tag generation by prompting an LLM with document content and existing tag examples. 

43. LlamaIndex (GPT Index) – Library that connects large language models with document indexes. It helps create structured indices (including vector indices) over your textual data for LLMs to efficiently retrieve relevant information. With LlamaIndex, you can enable example-based searches (the LLM finds contextually similar documents via the index) and even perform clustering by asking the LLM to reason about groups of related documents fetched from the index. 

44. Pyserini (Anserini) – Pyserini is a Python toolkit wrapping Anserini (an IR research platform on Lucene). It provides ready-made methods for sparse retrieval (BM25) and dense retrieval (using Transformer embeddings). With Pyserini, you can easily do k-NN search on document embeddings or use hybrid retrieval, facilitating MoreLikeThis searches (via dense vectors) and clustering experiments on datasets (via its integration with evaluation suites). It’s commonly used to reproduce academic results on document ranking and clustering. 

45. Carrot2 – Open-source text clustering engine that groups related documents and labels clusters with short key terms or phrases. Carrot2 can take search results or document sets and automatically organize them into thematic clusters (e.g., grouping news articles by topic). This is useful for identifying semantically similar documents (those in the same cluster) and for suggesting tags/topics that describe each cluster. It includes algorithms like Lingo and others for real-time clustering. 

46. BERTopic – A topic modeling library that leverages BERT embeddings and HDBSCAN clustering to find topics in documents. It effectively clusters semantically similar documents and generates human-readable topic labels. BERTopic is useful for organizing large corpora into topic clusters (which can reveal groups of related or duplicate documents) and for tag recommendation, as the generated topic labels can serve as high-level tags for documents with similar content. 

47. Top2Vec – Unsupervised topic modeling tool that embeds documents in a vector space (using doc2vec or Universal Sentence Encoder) and finds topic clusters by identifying dense areas in the embedding space. It simultaneously learns topic representations and assigns documents to these topics. This helps in clustering semantically-similar documents and automatically generating descriptive keywords for each cluster. It can be used to implement MoreLikeThis by comparing document vectors and grouping near-duplicates by shared topic. 

48. Gensim – Python library for topic modeling and similarity analysis. It provides implementations of algorithms like Word2Vec, Doc2Vec, LSI, and LDA, which can be used to create vector representations of documents. Using Gensim, one can build an index of document vectors (tf-idf or semantic) and query it for similar documents, or perform clustering using LDA to group documents by latent topics. Gensim’s Doc2Vec, in particular, allows example-based searches (finding documents with vectors closest to a given document’s vector) and can aid in finding duplicates by high vector similarity. 

49. MALLET – Java-based machine learning toolkit for text analytics that includes document classification, clustering, and topic modeling (via LDA). MALLET’s topic modeling can group documents 

11 into clusters of similar content, which helps identify themes and potential duplicates. Its document classifiers (e.g., Naive Bayes, MaxEnt) can be trained to assign tags or categories to documents (given training data), automating tag recommendation or classification tasks. 

50. HDBSCAN – A high-performance clustering library for density-based clustering. Often used with document embeddings (e.g., in BERTopic) to discover clusters of semantically similar texts without pre-specifying the number of clusters. HDBSCAN can identify groups of near-duplicate or topically related documents by finding dense areas in the vector space, and it can mark outliers (potentially very unique documents). It’s useful for unsupervised grouping of a document index into meaningful clusters. 

51. Apache Spark (MLlib) – Big-data processing engine whose MLlib library offers scalable algorithms for clustering (e.g., K-Means, LDA) and approximate similarity search (MinHash LSH). Spark can handle very large document collections distributed across a cluster. For example, using MinHash LSH in Spark, one can efficiently detect duplicate or highly similar documents in a big dataset. Spark’s implementations of K-Means or LDA can cluster documents to reveal topics, and its Streaming module could even update clusters in real time as new documents arrive. 

52. Apache Mahout – Machine learning library that includes algorithms for clustering and collaborative filtering. Although older, it provides distributed implementations of K-Means, fuzzy K-Means, spectral clustering, etc., which can be applied to group similar documents at scale. Mahout also had implementations of similarity measures and recommenders; for example, one could use Mahout to compute item-to-item similarities (treating documents as items with word vectors) to power “related documents” suggestions. Its legacy has been partly supplanted by Spark, but it’s still relevant for scalable clustering on Hadoop infrastructures. 

53. datasketch – Python library of probabilistic data structures for approximate search. Notably includes MinHash and MinHash LSH for estimating Jaccard similarity, which can efficiently detect near-duplicate documents by comparing sets of shingles (word n-grams). Using datasketch, one can index documents by MinHash and query for candidates with high Jaccard similarity very quickly, then optionally refine by exact comparison. It’s a practical tool for large-scale duplicate detection (e.g., finding almost identical news articles or detecting plagiarism in a corpus). 

54. Simhash (simhash-py) – Library implementing Charikar’s SimHash algorithm for near-duplicate detection. It computes a fingerprint such that similar documents have similar 64-bit hashes; comparing these (by Hamming distance) allows fast identification of documents with highly overlapping content. Simhash is famously used by Google for finding duplicate webpages. With simhash-py (or similar implementations), one can hash all documents in an index and quickly flag pairs of documents that differ in only a few bits (indicating substantial content overlap). 

55. text-dedup – Comprehensive Python toolkit for text deduplication. It implements multiple algorithms: MinHash LSH for near-duplicate detection, 64/128-bit SimHash, suffix array-based substring matching for exact or partial duplicates, and Bloom filters for exact duplicates. This all-in-one tool can be run on a corpus to cluster duplicate documents together or remove redundancy. It’s very useful for cleaning an index by merging duplicates and for finding document clusters that are almost identical (e.g., multiple versions of a report) using adjustable thresholds. 

56. Akin – Python library using Locality Sensitive Hashing to find near-duplicate texts in a corpus at scale. It generates MinHash signatures for documents and uses an efficient LSH index to return candidates with high Jaccard similarity. Akin is designed to handle large datasets (millions of documents) and can output clusters or pairs of duplicates. It’s effective for use-cases like detecting 

12 redundant entries in a database of documents or recommending deduplication by flagging records with overlapping text. 

57. NearDup (NearDuplicates Finder) – A Java-based tool for finding duplicate and near-duplicate documents. It extracts text from files (PDF, Word, HTML, etc.), computes fingerprints, and identifies clusters of near-identical documents, providing reports for review . NearDup uses approaches like those in research by Broder et al. (shingling and hashing) to group documents. It can also show “chains” of versions for documents that evolved. This is particularly useful in enterprise or forensic settings where you want to detect copies or versions of the same document across an organization. 

58. Annif – Multi-algorithm automated subject indexing tool. Developed for libraries, it integrates several NLP algorithms (TF-IDF, fastText, transformers, etc.) to suggest subjects or tags for documents based on their content. Annif can be trained on an index of documents with known tags and then recommend tags for new documents, effectively learning from the existing index’s tagging patterns. This addresses the tag recommendation problem by treating it as a text classification task (possibly multi-label). It can also use nearest neighbor approaches under the hood: for example, one backend of Annif uses similarity to known documents’ tags. 

59. KeyBERT – Keyword extraction library that uses BERT embeddings to find terms most similar to a document’s content. It generates a document embedding and candidate phrase embeddings, then selects keywords/keyphrases with the highest cosine similarity to the document embedding. KeyBERT provides an easy way to get a set of descriptive tags for a document in an unsupervised manner. These keywords can serve as recommended tags for indexing or as features for clustering (since similar documents would have overlapping keywords). 

60. YAKE – “Yet Another Keyword Extractor,” an unsupervised algorithm that identifies important keywords in a single document by analyzing term frequency, term context, and positional properties. It scores and ranks keywords without needing a training corpus. YAKE is useful for quick tag suggestion: given a new document, it can output the top N keywords that characterize it. In an indexing scenario, these keywords can be used to tag the document, and documents sharing high-scoring keywords would likely be considered related. 

61. RAKE – Rapid Automatic Keyword Extraction algorithm that splits text into candidate phrases (by removing stopwords), then scores them by frequency and co-occurrence patterns. It returns a set of key phrases that represent the document. RAKE is lightweight and doesn’t require training; it’s often used as a baseline for keyphrase extraction. While its output might be simpler than more advanced methods, it can still provide useful tags for a document or be used to cluster documents by the presence of shared key phrases. 

62. spaCy + PyTextRank – spaCy is an NLP toolkit, and PyTextRank is an add-on that implements the TextRank algorithm for keyphrase extraction. Together, they can extract significant phrases from documents (TextRank finds phrases that frequently co-occur in the text, akin to how PageRank works on networks). These keyphrases can be used as tags. SpaCy’s pipeline can also provide named entities and noun chunks as candidate tags. This approach can enrich an index with consistent tags and is unsupervised. It won’t cluster documents on its own, but if many documents share keyphrases, that implicitly forms clusters of related content. 

63. FastText – Library by Facebook for efficient text classification and word embedding. In the tag recommendation context, FastText can be trained on existing documents with tags (as a multi-label classifier) and then predict tags for new documents. It learns subword information, which helps with  

> 29 30

13 accuracy. FastText can also produce vector representations for entire documents (by averaging word vectors); these can be used to find similar documents via cosine similarity or to cluster documents. It’s very fast both in training and inference, suitable for large-scale applications of multi-label tagging or similarity search. 

64. Hugging Face Transformers – A broad library of pre-trained NLP models. Using this, one can fine-tune pre-trained models (like BERT, RoBERTa, or DistilBERT) for specific tasks such as multi-label document classification (to recommend multiple tags) or semantic similarity (to predict if two documents are duplicates). There are also off-the-shelf pipelines like zero-shot-classification 

that, given a set of candidate labels (tags), will score a document against them (useful for tag suggestion when you have a predefined taxonomy). Additionally, transformer models can produce embeddings for documents; these embeddings can feed into vector searches or clustering algorithms to group similar content. 

65. scikit-learn – General-purpose machine learning library with features for text vectorization (CountVectorizer, TfidfVectorizer) and algorithms for clustering (KMeans, Agglomerative) and classification (e.g., logistic regression, SVM). One can use scikit-learn to transform documents into TF-IDF vectors and then perform k-means clustering to group similar documents (example: cluster research papers by topics using their abstract vectors). Its classifiers can be trained for multi-label tagging or category prediction if given labeled data. While not specialized to text, its versatility makes it a baseline for many of these problems (e.g., a simple cosine similarity search using TF-IDF vectors, or using DBSCAN to identify near-duplicate groups by treating dense clusters of points as duplicates). 

66. scikit-multilearn – An extension of scikit-learn for multi-label classification. It provides algorithms and data structures specifically for predicting multiple labels for a single document (tags) and can handle correlations between tags. Using scikit-multilearn, one could train a model on an index’s tag assignments to predict a set of relevant tags for new documents. This addresses the tag recommendation problem with supervised learning, complementing the unsupervised keyword extraction approaches. The library includes techniques like problem transformation (Binary Relevance, Classifier Chains) and adaptive algorithm designs for multi-label data. 

67. IBM Watson Discovery – Commercial AI-powered search and text analytics platform. It offers features like smart document understanding, which includes automatic clustering of documents by concepts and extraction of keywords and entities. Watson Discovery can perform semantic queries (including natural language questions) and even uses passage-level similarity for finding answers. In practice, it can recommend related content (example-based search), highlight duplicates (through near-duplicate clustering in results), and suggest tags via its enrichment of content (e.g., identifying key topics or categories in documents) – all as part of an enterprise search solution. 

68. Microsoft Azure Cognitive Search – Cloud search service with AI enrichment capabilities. It combines traditional indexing with cognitive skills that can, for instance, extract key phrases, detect language, and recognize entities from documents. It also recently introduced semantic re-ranking and vector search. This means Azure Search can index embeddings for documents and allow semantic 

MoreLikeThis queries, as well as use the extracted key phrases as tags or facets. It’s a proprietary solution but significant in bringing NLP features (tag extraction, similarity search, synonyms) into a turnkey search product. 

69. Amazon Kendra – Managed enterprise search service that uses machine learning to deliver relevant results. Kendra supports natural language queries and leverages reading comprehension and embedding-based retrieval under the hood. It also features automatic FAQ matching and query 

14 suggestions. For our use cases, Kendra can find more like this documents by understanding query context (even if phrased as a question), cluster results by facets (like document type or topic), and it attempts to detect and merge duplicate content in results. Additionally, it can highlight key terms (which effectively works as tag suggestions) and learn relevancy from user interactions. 

70. Algolia – A proprietary hosted search engine known for its speed and typo-tolerance. Primarily a keyword search engine, Algolia has introduced vector search and hybrid search capabilities in recent times. With the Algolia AI Search add-ons, one can perform semantic searches and get vector-based similar results. Algolia also provides “related items” or “recommended for you” features via its analytics and rules engine, which solves example-based item-to-item search. While it doesn’t automatically cluster or deduplicate content, it allows custom business logic to hide duplicates and its facet system effectively provides tag-based navigation and filtering. 

71. Sinequa – An enterprise search platform that incorporates NLP for deep content analysis. Sinequa’s engine automatically extracts entities, topics, and relationships from documents, building a rich index. It supports semantic search (including understanding queries in natural language) and can cluster results by themes. Sinequa also has duplicate detection mechanisms (so that, for example, if the same document appears in multiple repositories, it can recognize and merge them). It is often used for unified search across enterprise data, where it provides relevant tag suggestions, similar content recommendations, and groups results by concepts. 

72. Coveo – AI-powered enterprise search and recommendation platform. Coveo’s index can ingest content from many sources and apply machine learning to tune relevance. It supports query suggestions and automatic facet generation (which can be seen as tag suggestions for filtering). Coveo also includes an item-to-item recommendation engine: for example, it can learn which knowledge base articles are often viewed together and recommend “related articles.” In doing so, it effectively uses collaborative signals and content similarity to solve the example-based search problem. Duplicate handling in Coveo can be configured through its indexing pipeline (e.g., using hashing to avoid indexing the same content twice). 

73. Lucidworks Fusion – A commercial platform built on Solr, adding machine learning, signal boosting, and NLP pipelines. Fusion can perform automatic query intent detection, use user behavior signals to boost relevant content, and integrate ML models for tasks like clustering and classification. It provides tools like Smart Cluster (clustering documents or queries into topics) and ML tagging (using ML to assign categories to documents). These capabilities mean Fusion can recommend tags for content (learned from prior tagging behavior), cluster search results by semantic similarity, and identify duplicate documents during indexing (to unify them or boost the original). 

74. Mindbreeze – An enterprise search appliance that uses AI to connect information silos. Mindbreeze performs entity extraction, sentiment analysis, and clustering on ingested content to create a“knowledge graph” of your documents. In search results, it will often group documents by concept and can recognize duplicates or near-duplicates (showing one and indicating additional sources). It provides what they call “insight apps” which often include pre-built semantic search and recommendation features (like suggesting related information or experts). For tagging, Mindbreeze’s analysis can automatically apply taxonomy terms to content based on what it finds (addressing tag assignment), and its semantic understanding helps find similar documents even if they don’t share keywords. 

75. Google Cloud Enterprise Search (Cloud Search) – Google’s managed search service for enterprise (part of Google Workspace). It leverages Google’s search algorithms applied to corporate data. Cloud Search incorporates knowledge of entity recognition and user context. It will surface “People also viewed” or “Related” content based on semantic and usage similarity. While specifics are not fully public, 

15 it’s known to handle duplicate suppression (not showing the same result twice) and can infer topics for faceted navigation. Essentially, it brings Google-like search (including spell correction, synonyms, and likely vector semantics) to internal data, addressing the user’s query intent and suggesting relevant content, which aligns with example-based retrieval and intelligent tagging (through its metadata and result clustering features). 

76. Google Vertex AI Matching Engine – A vector similarity search service as part of Google’s Vertex AI platform. It allows extremely fast ANN queries on vector data (billions of vectors) by partitioning and parallelizing searches. Matching Engine can thus be used to power semantic search for documents (finding the top N most similar documents to a query or example document) and is useful in pipelines for duplicate detection (e.g., finding documents within a threshold distance in embedding space). Paired with Google’s embedding models (like USE or BERT), it’s a production-grade solution for example-based searches and clustering via nearest neighbors. 

77. OpenAI Embeddings API – A commercial API that generates high-quality text embeddings (e.g., using the Ada model). While not a search engine by itself, these embeddings can be stored in any vector database to enable semantic search and clustering. Many developers use OpenAI’s embeddings to encode documents and then perform similarity search for tasks like “find similar customer support tickets” or “recommend relevant articles.” Essentially, OpenAI provides the semantic encoder, and you combine it with a vector index to solve example-based lookup and duplicate detection (by setting a similarity threshold) – notable due to the quality of the embeddings produced. 

78. Cohere – Provides APIs for generating text embeddings and for classification. Cohere’s multilingual embeddings can be used similarly to OpenAI’s for semantic document search – you encode your documents with Cohere’s model and use a vector search backend to find similar ones. They also offer a classification API where you can feed examples (documents with tags) and get a model to assign tags to new documents, directly addressing tag recommendation as a service. It’s a notable proprietary alternative that focuses on ease of use for developers to add NLP to their apps (common use-cases include semantic search, content recommendation, and automatic labeling). 

79. Epsilla – A high-performance vector database system (nicknamed “Hippocampus for AI”). It emphasizes fast similarity search and horizontal scalability for very large vector sets. While relatively new, Epsilla aims to support hybrid queries (vector + filters) and is optimized for scenarios like real-time recommendation and anomaly (duplicate) detection. In practice, you could use Epsilla to store embeddings of all documents in an index and query it for the nearest neighbors of any given document or query embedding, obtaining instant “more like this” results or flagging duplicates when a new document’s nearest neighbor is extremely close. 

80. LangStream – An open-source project that combines event streaming with LLM technologies. It allows for setting up flows where events (e.g., new documents) are processed through LLM-based components. For instance, LangStream could take a new document, use an embedding model to vectorize it, stream that to a vector search component, and then perhaps route similar document findings to another service. This means it can automate example-based search or tagging in real-time streams – as documents come in, it can find similar ones on the fly or even assign tags by prompting an LLM with the document content. It’s a novel approach to integrate retrieval and generation in event-driven architectures. 

81. PUFFINN – A specialized library implementing parameterless LSH (Locality Sensitive Hashing) for fast nearest neighbors. It automatically tunes itself and uses hashing techniques to bucket similar items. PUFFINN can be used to index a large set of document shingle vectors or embeddings and retrieve likely near-neighbors quickly. It’s particularly useful when you want a quick and reasonably 

16 accurate way to detect duplicates or do similarity joins in a big dataset without fiddling with algorithm parameters – you just feed in the data and query, and it manages the rest. 

82. TarsosLSH – A Java library for locality-sensitive hashing based approximate search . It allows indexing of high-dimensional vectors and querying by example. TarsosLSH was originally created for music/audio similarity, but it can be applied to text embeddings as well. By hashing document vectors, it can retrieve candidates that are likely similar (with configurable recall/precision trade-offs). It’s an approachable way to add a “more like this” feature or duplicate finder into Java-based systems where using a full vector database might be overkill. 

83. PASE (Postgres ANN Extension) – An extension for PostgreSQL that enables ultra-high-dimensional vector search within the database. PASE allows Postgres to perform approximate nearest neighbor queries using methods like PQ (Product Quantization). This means you can use pure SQL to do things like: “SELECT * FROM documents ORDER BY embedding <-> :query_embedding LIMIT 10” for semantic search. It’s powerful for integrating semantic similarity search directly into analytical workflows or applications that already rely on Postgres, unifying structured and unstructured data queries (and could be used to find duplicates by running a self-join where distance < threshold). 

84. RPForest – A Python library implementing Random Projection forests for ANN. It builds multiple random projection trees to partition the vector space and then finds nearest neighbors by traversing these trees. RPForest is light and can be quicker to set up than heavy graph-based methods for certain use cases. You could use RPForest to index document embeddings and get an approximate set of nearest neighbors for any document – useful for quick clustering (by checking neighbor lists) or recommendation. It’s another tool in the ANN toolkit, offering a balance of simplicity and performance for moderately large datasets. 

85. MRPT – Multi-Random Projection Trees , an ANN algorithm and library that uses multiple random projection trees to approximate nearest neighbors. It was shown to be effective in some benchmarks and is available as a C++ library with Python bindings. MRPT’s strength is in memory efficiency and speed for high-dimensional data. As an application, one could plug MRPT in to find similar documents via their vectors (for example, for each new document, quickly retrieve a shortlist of candidates that are likely duplicates or topically similar). It’s less widely used than FAISS or Annoy but can be a good choice in memory-constrained environments. 

86. Autofaiss – Automation tool to build and optimize FAISS indices. It helps in choosing the right index type and parameters for a given dataset and memory constraints. Autofaiss will partition data and train quantizers as needed to reach a target recall or index size. This is very useful when you have a large document collection to index for similarity search: instead of manually tuning FAISS (which can be complex), Autofaiss can produce an index that meets your needs (fast MoreLikeThis queries, limited memory usage, etc.) with minimal effort. It essentially makes deploying vector search at scale easier, which indirectly supports all the tasks (tagging via nearest neighbors, clustering, deduping via FAISS indexes) by ensuring FAISS is used optimally. 

87. Distributed FAISS – Framework to scale FAISS across multiple machines. It enables building a sharded FAISS index or distributed querying so that billion-scale vector searches can be performed over a cluster. If you have an extremely large index of document embeddings (say for all web pages or huge archives), Distributed FAISS allows you to still query for similar documents with low latency. It’s key for industrial-scale MoreLikeThis or duplicate detection systems (for instance, checking a new document against an index of 100 million others). By distributing both data and query load, it achieves near-linear scaling with the number of nodes. 

> 31

17 88. NLTK – Natural Language Toolkit for Python, offering tokenization, stemming, etc. While not focused on search, it provides building blocks for text processing; for instance, one can use NLTK to extract keywords or noun phrases and compare them (via Jaccard or cosine) to suggest tags or detect copies. It also has basic clustering and classification (Naive Bayes) that can be applied to small-scale problems. NLTK is more of a teaching and prototyping library, but it can support tasks like splitting documents into shingles for hashing or doing simple fuzzy matching as part of a custom pipeline for duplicate detection or tagging. 

89. Datafari – Open-source enterprise search solution (by France Labs) built on Solr and OpenSearch. It includes a rich set of features like crawlers, security trimming, analytics, and “AI” modules. Datafari provides faceted navigation and results clustering out-of-the-box. It can perform document deduplication at indexing time (to avoid indexing the same content twice) and can integrate with NLP pipelines for entity extraction and tagging. Essentially, it’s a ready-made platform that addresses our problems by allowing plug-ins: e.g., one can plug a duplicate detector, use tags as facets to filter search results, or enable a “more like this” component on result pages with minimal coding. 

90. SentenceTransformers – Framework for creating sentence and document embeddings (e.g., SBERT models). It provides many pre-trained models fine-tuned for semantic textual similarity. By encoding all documents in an index with SentenceTransformers, you can use a simple cosine similarity to perform example-based searches (find which documents have the highest cosine similarity to a given document’s embedding) or to cluster documents (via grouping embeddings). This library is widely used for tasks like duplicate question detection on forums, semantic search in FAQs, and recommending similar short texts, due to its ease of use and high-quality representations. 

91. BigARTM – Open-source library for advanced topic modeling (Additive Regularization of Topic Models). It allows combining various modalities and criteria in topic models and can handle very large data. BigARTM can produce a rich topic model where each document is associated with a distribution of topics. Those topics can be seen as clusters or tags. For example, you could train a BigARTM model on your document index to discover, say, 100 topics, and then use the top topic(s) of each document as its tags. It also supports incremental learning, so it can update topics as new documents come in. This addresses clustering (by topics), tag suggestion (using topic keywords as tags), and even near-duplicate detection (documents with extremely similar topic distributions might be duplicates). 

92. GATE – General Architecture for Text Engineering, a long-standing open-source framework for building NLP pipelines. GATE has components for language detection, tokenization, POS tagging, NER, and even document classification (via plugins like the “Text Classification” component). With GATE, one can create a workflow that, for instance, annotates documents with entities and key phrases (creating tags), classifies documents into categories (automated tagging), and clusters documents by similarity of annotations. It’s quite modular and has a graphical interface for designing pipelines. While GATE itself isn’t a search engine, it can preprocess and enrich documents before indexing (adding lots of metadata tags that solve the tag recommendation problem) and can be used to build a duplicate detection system by writing scripts that compare document annotations. 

93. RapidFuzz – Efficient fuzzy string matching library in Python (successor to FuzzyWuzzy). It uses optimized Levenshtein distance computations (and other string metrics) to find similar strings with much better performance than pure-Python alternatives. RapidFuzz is most useful for short text comparisons – for example, detecting if two titles or sentences are almost the same (duplicate detection in titles or metadata) . In the context of documents, you might use RapidFuzz to compare important fields (like document titles, or perhaps the first sentence of each document) to quickly catch obvious duplicates. It can also be used to match extracted keywords from a new document against existing tags in an index (to suggest the closest matching existing tag even if wording differs slightly). 

> 32

18 94. Dedupe – A Python library for entity resolution and record linkage using machine learning. While often applied to structured data (like finding duplicate customer records), it can handle textual fields and learn what constitutes a duplicate . Dedupe uses a combination of blocking (to narrow comparisons) and an active learning model to decide if two records are the same. For document de-duplication, one could feed it features like normalized title, content length, or even content embeddings, and it would attempt to cluster documents that are likely the same. It’s useful when duplicates aren’t exact – Dedupe can learn from a few labeled examples what minor differences to ignore. 

95. AllenNLP – An open-source NLP research library from AI2 offering modules for tasks like text classification, sequence tagging, and semantic similarity. With AllenNLP, one can build and train custom models (e.g., a classifier that reads a document and outputs relevant tags, or a Siamese network that determines if two documents are duplicates). It provides high-level APIs to load pre-trained transformers or define new architectures. For instance, one could use AllenNLP to implement a 

MoreLikeThis by training a model on pairs of similar/dissimilar documents (learning a similarity function), or simply use it to fine-tune a BERT model on a tagging task (addressing automatic tag assignment). Essentially, it’s a framework to roll out your own solutions for these problems when off-the-shelf models aren’t directly available. 

96. Stanford CoreNLP – A full-stack NLP toolkit providing linguistic analysis from tokenization to parsing to NER. It includes a classifier component (the Stanford Classifier ) that can be trained to label texts into one of K classes. CoreNLP can be used for document tagging by training its classifier on an indexed dataset’s tags or categories. It also has a simple similarity tool (based on TF-IDF or embedding) and can output NLP annotations useful for clustering (like coreference chains or topic signatures). While CoreNLP doesn’t have a built-in clustering feature, its rich annotations (POS tags, dependency parses, etc.) can feed into custom clustering algorithms – for example, clustering documents by the key entities and noun phrases CoreNLP extracts. 

97. Flair – A Python NLP framework from Zalando that simplifies training state-of-the-art text classifiers and sequence taggers. Flair provides pre-trained embeddings (including contextual string embeddings and transformer-based embeddings) and models for tasks like NER and sentiment analysis. Using Flair, you can quickly train a document classifier on your own tags (multi-label or single-label), which directly addresses tag recommendation (the model learns to predict tags from content). Additionally, Flair’s embeddings can be used to represent documents – these embeddings could then be clustered or fed into an ANN index to find similar documents. Flair’s strength is its ease of use for high-quality NLP, making advanced techniques accessible for tasks like tag prediction or duplicate identification (by classification: train a model to predict if two docs are duplicates). 

98. Microsoft SharePoint Search – The search component of SharePoint (part of Microsoft 365) widely used in enterprises. It is a proprietary search engine that indexes documents across SharePoint sites and beyond. SharePoint Search automatically generates refiners (facets) from document metadata and content, which effectively act as tag suggestions for filtering. It also groups results by relevance and will hide near-duplicates (for instance, if the same document exists in multiple libraries, it can show one and suppress others). While not highly customizable, it’s significant for its built-in duplicate collapsing and use of user context to recommend results (it might promote documents frequently accessed together, akin to an implicit “related documents” feature). 

99. Lingo3G – A commercial real-time text clustering engine (by Carrot Search) that groups documents into hierarchically organized, clearly-labeled topical folders . It can cluster thousands of documents in milliseconds, producing human-readable labels for each cluster (using frequent phrases) and supports multiple languages. Lingo3G is often used on top of search results to provide an overview of 

> 32
> 33

19 themes. In our context, it addresses clustering of semantically similar documents (e.g., grouping search results or index contents by topic) and can implicitly help with duplicate detection (documents that are near-duplicates will end up in the same cluster due to having very similar content). Its labeled clusters can also serve as dynamic tags or facets for navigating documents (since the cluster labels are essentially keywords describing the cluster) .

100. AutoPhrase – An automated phrase mining tool for extracting quality phrases from large text corpora. It uses distant supervision from Wikipedia to learn what constitutes a “quality phrase” and can operate in multiple languages. AutoPhrase will output a ranked list of multi-word phrases that are prominent in the corpus (and in each document, it can highlight those phrases). This is useful for tag recommendation because it can pull out domain-specific terms and key phrases from documents without manual effort. By incorporating those phrases as candidate tags, you improve search (indexing important phrases) and can cluster documents by shared phrases. It essentially helps bridge raw text to structured tags/topics by discovering the significant phrases that people might use as labels or queries. Each of these solutions contributes to one or more of the target problems – from tag recommendation (through keyword extraction or classification), example-based similarity search (via vector indexes, ANN libraries, or search engine features), document clustering (through topic modeling, clustering engines, or graph-based methods), to near-duplicate detection (via hashing algorithms, LSH, or specialized deduplication tools). They range from low-level libraries to full-fledged enterprise platforms, covering a wide spectrum of open-source and proprietary options, all of which are actively maintained and feasible for addressing the stated problems in modern information retrieval systems.

Title: Top 100 Solutions for Tagging, Similarity Search, Clustering & Deduplication

URL Source: https://x0.at/MYMJ.pdf

Published Time: Wed, 14 Jan 2026 15:41:22 GMT

Number of Pages: 21

Markdown Content:
# Top 100 Solutions for Tagging, Similarity Search, Clustering & Deduplication 

Below is a ranked list of 100 software solutions, libraries, and frameworks (both open-source and commercial) that address one or more of the specified problems: automatic tag recommendation, example-based document search, semantic document clustering, and duplicate content detection. The ranking is based on the breadth of features relevant to these tasks (solutions covering multiple areas appear higher). Each item includes a brief description highlighting its capabilities: 

Elasticsearch – A widely-used open-source search engine built on Lucene. It supports More Like This queries to find documents similar to a given text or document and offers recent support for vector embeddings (k-NN search) for semantic similarity. While primarily a full-text search platform, Elasticsearch can be extended for clustering (via plugins) and duplicate detection (e.g. by comparing document fingerprints). Active development and broad adoption make it a top choice for building search and recommendation features. 

OpenSearch – OpenSearch is an open-source fork of Elasticsearch (maintained by Amazon) that includes similar capabilities with an emphasis on extensibility. It supports dense vector fields and an integrated k-NN plugin for semantic search . Users can perform MoreLikeThis style queries for similarity and use OpenSearch’s aggregation framework to assist in clustering. It inherits Elasticsearch’s strengths in tag-based querying and can be used to detect near-duplicates by searching for shared significant terms or vectors. 

Haystack (deepset) – An open-source NLP framework for building search and question-answering systems. Haystack allows you to combine keyword search (e.g. via Elasticsearch) with embedding-based retrieval, enabling example-based semantic search . It provides pipelines that can generate document embeddings and find similar documents, and it integrates with clustering algorithms externally . While Haystack doesn’t natively cluster or deduplicate on its own, it simplifies these tasks by providing components for embedding generation and integration with tools like FAISS or scikit-learn for clustering . It’s well-maintained and geared towards production-ready intelligent search applications. 

Weaviate – A cloud-native open-source vector database designed for semantic search and data enrichment. Weaviate stores high-dimensional vectors and offers hybrid search (vector +keyword) out-of-the-box. Notably, it includes modules for automatic classification , meaning it can assign categories or tags to items using machine learning models (reducing the need for manual tagging) . Weaviate excels at example-based document search using vector similarity and can power recommendations and clustering by vector proximity. Its built-in near-duplicate detection and filtering capabilities stem from fast k-NN search on vectors, making it acomprehensive solution for semantic search and related tasks. 

Vespa – An open-source big data search engine by Yahoo/Oath, tailored for large-scale retrieval and recommendations. Vespa supports both full-text search and approximate nearest neighbor vector search, allowing semantic similarity queries. It enables deploying machine learning models (e.g. for ranking or classification) directly in the serving layer . With Vespa, one can implement “More Like This” features and even perform on-the-fly clustering or grouping of 1. 

> 1

2. 

> 2

3. 

> 3
> 4

4. 

> 5

5. 

> 6

1results by similarity. It’s designed for high throughput and can detect duplicates or nearly identical content by configurable document similarity functions. 

Qdrant – An open-source high-performance vector search engine optimized for real-time nearest neighbor queries. Qdrant is built to serve semantic search and recommendation use cases, letting you index document embeddings and query by similarity . It supports payload filters, which means you can store and filter by metadata like tags. While Qdrant’s focus is on 

example-based search (find similar documents given an embedding), it can be part of a pipeline for clustering (via vector distance) and even duplicate detection by identifying items with very high cosine similarity. 

Milvus – A popular open-source vector database designed for billion-scale embedding data. Milvus provides extremely fast approximate similarity search for vectors and is often used for image and text retrieval. It doesn’t do tagging or clustering by itself, but as a backend it shines in 

semantic similarity search and can support duplicate detection by finding near-identical vectors. Combined with an embedding model (like Sentence-BERT), Milvus enables “find more like this document” and grouping similar documents by their vector closeness. The project is actively maintained (with an enterprise spin-off, Zilliz). 

Pinecone – A fully managed commercial vector database service known for its ease of use and scalability. Pinecone allows developers to store embeddings and perform instant similarity searches with automatic indexing and scaling . While proprietary, it’s widely used for semantic search applications. You can feed document vectors into Pinecone and query for similar documents or potential duplicates (very high similarity scores). Pinecone focuses purely on vector search (no built-in clustering or tagging), but its reliability and performance in the vector similarity domain place it among top solutions. 

Apache Solr – An open-source search platform based on Lucene. Solr offers robust full-text search with features like the MoreLikeThis handler to get similar documents based on text fields (useful for example-based retrieval). It supports faceted search on tags and metadata, which helps in tag recommendation workflows. For clustering, Solr can integrate with Carrot2 via a plugin to cluster search results into topics . While Solr doesn’t natively compute semantic embeddings, it can index document vectors if provided. It’s a mature, actively maintained project suitable for tagging (via its faceting and Query Suggesters ) and finding duplicates by identifying nearly identical text through hashing or the MLT query. 

Jina AI – A neural search framework that allows building search and indexing flows with deep learning. Jina provides an orchestration layer to index documents, generate embeddings (using plugins or executors for various encoders), and query by similarity. It’s cloud-native and multi-modal (text, images, etc.), enabling semantic search pipelines . With Jina, you can implement “more like this document” functionality by encoding a seed document and retrieving neighbors. It also supports segmenting documents and using shingling or embedding chunk comparisons for duplicate detection. Jina can be extended to cluster documents by chaining a clustering executor (like KMeans on embeddings), making it a flexible framework for all tasks listed. 

DeepLake (Activeloop) – An open-source database for AI that integrates vector storage and versioning. DeepLake can store datasets with embeddings and provides fast similarity search, often used for LLM applications. It’s optimized for managing embeddings and associated labels, which means you can perform semantic queries on stored documents. DeepLake’s core strengths are in example-based retrieval and serving as a vector store for pipelines (like with LangChain or 6. 

> 7

7. 

> 8

8. 

> 9

9. 

> 10

10. 11. 

2Haystack). While it doesn’t directly cluster or tag, it works with Python ML tools to do so, and is actively maintained as part of the Activeloop ecosystem .

Typesense (with Vector Search) – An open-source search engine focused on simplicity and speed, originally for typo-tolerant keyword search. Recent versions of Typesense introduced 

vector search support, enabling hybrid search that combines traditional indices with semantic relevance . This means you can index document embeddings in Typesense and perform example-based searches that rank by vector similarity. It’s not as feature-rich in ML as Elastic or Vespa, but for small-to-medium applications it provides quick setup for “find similar documents” 

and can use its filtering/faceting for tag-based suggestions. Active development and an easy-to-use API are pluses. 

Chroma – An open-source embedding database and retrieval library, commonly used for prototyping semantic search and LLM applications. Chroma is lightweight and developer-friendly: you can add documents with embeddings and query by similarity in just a few lines .It’s especially popular for “semantic memory” in chatbots and QA systems. While Chroma doesn’t have built-in clustering or tagging, you can retrieve nearest neighbors for a given document (for duplicate checking or recommendations), and maintain tags/metadata alongside each vector for filtering. It’s a newer project but quickly evolving. 

Zilliz (Enterprise) – The commercial cloud solution built by the creators of Milvus. Zilliz offers a managed, enterprise-grade vector database with the same core capabilities as Milvus (fast ANN search) plus scalability and security features . It can be used to power semantic search in production. From the perspective of our tasks, Zilliz excels at example-based document search and deduplication (via vector similarity) at massive scale. It may not directly do tag recommendation or clustering, but it provides the backbone for such solutions when combined with other ML services. 

LangChain – An open-source framework for developing applications that integrate language models with external data. While LangChain is primarily known for chaining LLM calls, it has robust support for retrieval components: you can plug in vector stores (FAISS, Pinecone, etc.), do similarity search for relevant documents , and even incorporate filters like tags. It’s more of an orchestration toolkit than a search engine, but one can use LangChain to build pipelines that tag documents (via LLM classification), find similar docs (via vector search), cluster content (by asking an LLM to group them or using embedded vectors), and identify duplicates. LangChain’s flexibility and large community make it a notable inclusion, though it leverages other underlying tools for heavy lifting. 

Redis (RediSearch Vector) – The Redis in-memory data store, with the RediSearch module, now supports vector similarity search using HNSW indexes . This transforms Redis into alightweight vector database capable of semantic queries. It’s valuable if you need fast example-based search or deduplication checks within an application cache or small dataset. Additionally, Redis can store sets/tags and compute intersections, which can help in recommending tags (e.g., by tracking tag co-occurrence). It’s not specialized in clustering, but one could use Redis to group documents by hash signatures for duplicate detection. Being a general-purpose system, it requires more custom work, but is actively maintained and widely used. 

FAISS – Facebook’s library for fast vector similarity search and clustering. FAISS is a C++/Python toolkit that provides numerous indexing methods (IVF, PQ, HNSW, etc.) to perform approximate nearest neighbor search on large collections of vectors . It’s not a standalone server but is often integrated into search engines and ML pipelines. FAISS excels at finding similar documents 

> 11

12. 

> 12

13. 

> 13

14. 

> 14

15. 

16. 

> 15

17. 

> 16

3given their embeddings and can also perform clustering (it includes k-means clustering on GPU). Many duplicate detection systems use FAISS to quickly find potential duplicates by embedding comparison. As an underlying library, it’s one of the industry standards for vector-based retrieval tasks. 

Annoy – Annoy (Approximate Nearest Neighbors Oh Yeah) is a C++/Python library by Spotify for efficient approximate nearest neighbor search on static data . It builds multiple small randomized trees to index vector data. Annoy is very useful for item-to-item recommendations or example-based queries in memory-constrained or offline scenarios. You can embed all documents, use Annoy to query for the top N similar vectors, and thereby implement “more like this” features or cluster by proximity. It’s less flexible with updates (best for read-mostly scenarios) and doesn’t directly handle tagging or duplicate flags, but its simplicity and speed make it a common choice in recommendation pipelines. 

HNSWlib – An open-source C++ library (with Python bindings) implementing the Hierarchical Navigable Small World graphs for similarity search. HNSWlib is known for its excellent performance in vector search – many other tools (FAISS, Elasticsearch, etc.) use HNSW under the hood. Using HNSWlib directly, one can index document embeddings and perform very fast k-NN queries for similar items. This is great for example-based search and near-duplicate detection (a duplicate will be the nearest neighbor). It doesn’t include clustering or tagging by itself, but its focused functionality and speed have made it a go-to solution for developers implementing custom semantic search or deduplication logic. 

NMSLIB – The Non-Metric Space Library is an efficient toolkit for similarity search that also supports HNSW and other algorithms. It was one of the earlier libraries for ANN search and can handle various distance metrics. NMSLIB can power features like finding related documents by embedding similarity, though it requires developer integration (not a turn-key solution). Some clustering methods (like hierarchical clustering) can be applied using distance computations from NMSLIB. While perhaps less popular now than FAISS or HNSWlib, it’s still maintained and forms the basis of some search solutions. 

ScaNN – Google’s Scalable Nearest Neighbors library, designed for efficient vector search with learned indexes and quantization. ScaNN is open source and provides high recall with speed, often outperforming traditional ANN methods on certain workloads. It’s used to implement semantic retrieval at scale (e.g., in Google’s production systems). As a library, ScaNN is specifically for vector similarity search – you would use it to find top-K similar documents to a query embedding. It doesn’t handle tagging or clustering on its own, but can be a powerful component in a larger pipeline for duplicate detection or grouping similar items. 

PostgreSQL + PGVector – PostgreSQL, the popular open-source RDBMS, can be extended with the PGVector extension to support vector similarity search in SQL. This combination enables you to store document embeddings as a column and run queries to find nearest neighbors. It’s useful for applications that want semantic search but also need relational data handling or transactions. With PostgreSQL, one can also leverage SQL for grouping and filtering (for example, grouping similar docs by a certain threshold to identify duplicates). This solution covers 

example-based search in a more traditional database environment and is actively maintained via the PGVector project. 

MongoDB Atlas Search – MongoDB’s cloud platform includes Atlas Search, which recently introduced vector search capabilities. This means you can perform hybrid queries (text search combined with vector similarity) on data in MongoDB. For our use cases: you could store tag 18. 

> 17

19. 20. 21. 22. 23. 

4fields and do automatic tag suggestion by similarity, or find similar documents by providing an embedding (the system will return documents with close vectors). While not open-source, it’s a significant commercial offering. It supports basic duplicate detection by querying a document’s own vector to see if any other returns at a near-100% similarity. MongoDB’s ease of use and integration (with JSON documents) makes this appealing for developers who want semantic search without managing separate infrastructure. 

Apache Carrot2 – An open-source text clustering engine specialized in organizing search results into thematic clusters. Carrot2 takes a list of documents (e.g., search result snippets) and automatically groups them by topics, labeling each cluster with key terms . It includes algorithms like Lingo and STC for clustering short text. Carrot2 can be used on any document set; for example, you could feed it your entire index to discover clusters of similar documents, effectively performing document clustering . It also integrates with Solr/Elasticsearch as a plugin to cluster query results . While Carrot2 doesn’t handle tag recommendation or deduping, it excels in real-time clustering of semantically similar documents and is actively maintained (latest release in 2026) .

BERTopic – A Python library that leverages transformers and clustering algorithms to perform topic modeling on documents. BERTopic generates embeddings for documents (using Sentence Transformers or similar) and then uses techniques like UMAP dimensionality reduction and HDBSCAN to cluster documents into topics, assigning each cluster a set of representative words. This effectively groups semantically-similar documents together. It’s very useful for discovering latent topics in your corpus or clustering an index for analysis. BERTopic doesn’t explicitly do tag recommendation or deduplication, but the topics it produces can serve as high-level tags, and very small clusters might indicate near-duplicates. It’s actively developed and popular in the NLP community. 

Top2Vec – An open-source Python library for topic modeling that simultaneously learns topic vectors and document vectors in the same semantic space. Top2Vec will cluster documents such that similar documents form topic groups, and it can output the nearest words that describe each cluster. The result is a set of topics and the documents that belong to each. This addresses 

clustering of documents by semantic similarity in an unsupervised way. Like BERTopic, Top2Vec can reveal groups of duplicates or highly similar content as small tight clusters. It’s a self-contained tool (no need for external embedding model if you use the default) and provides an intuitive way to explore large document sets. 

Gensim – A popular open-source NLP library that provides tools for topic modeling and similarity analysis. Gensim’s Doc2Vec and Word2Vec implementations can produce vector representations of documents and words, which then allow similarity queries. Gensim also offers Latent Dirichlet Allocation (LDA) and LSI for topic clustering of documents. With Gensim, one can train a model on the existing corpus tags or content and then infer topics or recommend tags for new documents based on similar content. For instance, using Doc2Vec you can find similar documents 

by comparing inferred vectors, or with LDA you can cluster docs. It’s a well-maintained, go-to library for many unsupervised NLP tasks. 

scikit-learn – The general-purpose machine learning library in Python provides numerous algorithms relevant to these tasks. For example, scikit-learn has KMeans , DBSCAN , and 

Agglomerative Clustering for grouping documents (once you represent them as vectors, e.g., TF-IDF or embeddings). It also supports NearestNeighbors search which can be used for example-based retrieval on smaller scales. For tag recommendation, scikit-learn’s multi-label classification (using OneVsRestClassifier or ClassifierChain ) can train models to 24.  

> 18 19
> 10
> 20

25. 26. 27. 28. 

5predict tags based on document features. And for duplicates, one could use similarity metrics (Jaccard, cosine) provided by scikit-learn or its pairwise distances to identify pairs of very similar documents. It’s a foundational toolkit (BSD-licensed) and is actively maintained with a wide user community. 

HDBSCAN – A specific clustering library (Python) that is excellent for clustering data with varying densities. HDBSCAN is often used in conjunction with embedding techniques; for instance, after converting documents to vectors, HDBSCAN can identify clusters of documents that are closely packed in the semantic space, effectively performing document clustering without needing to specify the number of clusters in advance. It’s the clustering engine under the hood of BERTopic. HDBSCAN can also be useful in separating out a cluster of nearly identical documents (potential duplicates) from the rest. It’s well-maintained and widely used for unsupervised clustering tasks. 

MALLET – The Matrix of LATent Labelers toolkit is a Java-based machine learning library focused on natural language processing, particularly known for topic modeling (LDA) and sequence tagging. MALLET’s LDA can cluster large document collections into topics (semantic clusters), and it also supports clustering via its implementations of algorithms like k-Means or hierarchical clustering. While older, MALLET is still used in academic settings for text clustering tasks. It doesn’t directly do “tag recommendation” except by outputting probable topic labels, but those can be interpreted as tags. MALLET can be part of a pipeline to find duplicates by checking which documents share an overwhelming majority of topic assignments, for example. 

Annif – An open-source toolkit specifically for automated subject tagging (developed by the National Library of Finland). Annif is trained on a set of documents with known tags (subjects) and then can suggest tags for new documents based on learned semantics . It integrates multiple NLP algorithms (TF-IDF, fastText, transformer models, etc.) to predict relevant tags for adocument’s content . This directly addresses the problem of recommending tags for new documents based on an existing index of tagged documents. Annif can be configured with custom vocabularies (even controlled vocabularies like thesauri) and has a REST API and CLI for integration . It’s actively maintained and already used by libraries and archives for subject indexing, making it one of the top solutions for automatic tag recommendation. 

fastText – An open-source library by Facebook AI Research for efficient text classification and representation learning . fastText can train supervised models very quickly, including multi-label classification models for tagging. Given a corpus of documents with tags, fastText can learn to predict the tags of a new document (essentially recommending relevant tags from the index). It represents words and documents as vectors, which also means you can use those vectors for approximate document similarity queries. fastText is known for being fast and requiring modest resources, and it’s been used for tasks like language identification, tag prediction (e.g., predicting Stack Overflow tags), and more. It’s actively maintained (in C++ with Python bindings) and a strong choice when you need a lightweight tag suggestion or text classification solution. 

Sentence Transformers (SBERT) – A library that provides pretrained models (and tools to fine-tune new models) to produce sentence or document embeddings that capture semantic similarity . By converting documents into embeddings using SBERT, you can directly compute similarities between a new document and those in your index to find the most similar ones or to aggregate tags. This is essentially the backbone for many semantic search systems. SBERT doesn’t handle indexing or searching itself, but combined with a vector index (like FAISS or Annoy), it lets you implement example-based document search easily. It’s also used in clustering: 29. 30. 31. 

> 21
> 22
> 22

32. 

> 23

33. 

> 24

6embed all docs and then cluster in vector space. SBERT is actively updated (part of HuggingFace ecosystem) and widely used in industry and academia for semantic textual similarity tasks. 

Hugging Face Transformers – The Transformers library provides a wide range of pre-trained models (BERT, RoBERTa, GPT, etc.) and pipelines for NLP tasks. For our scope, this library can be used to fine-tune or infer models for multi-label classification (tagging), extract embeddings for documents (for similarity search and clustering), and even perform textual entailment which could be used to detect duplicates (e.g., if two documents entail each other’s content). It’s not an end-to-end solution, but it’s a core building block. For example, one could use a zero-shot classification pipeline to recommend tags (treating tags as labels and using a model like 

facebook/bart-large-mnli ), or use a QA model to check duplicate question pairs. The versatility of HF Transformers (and the model hub) means you can craft custom solutions to all four problems. It’s extremely well-maintained by the open-source community and Hugging Face. 

PySolr / SolrTextTagger – Apache Solr’s ecosystem includes the SolrTextTagger plugin, which can automatically annotate documents with terms from a provided dictionary or ontology .This is useful for tag recommendation when you have a controlled vocabulary or existing tag list: the tagger will scan new documents and suggest existing tags that appear in the text (with variations). It’s essentially a dictionary-based entity/keyword extraction tool integrated with Solr. Additionally, using PySolr (a Python client for Solr), developers can leverage Solr’s MoreLikeThis 

handler and faceting to do similarity search and find popular tags among similar documents. The SolrTextTagger and Solr’s robust text analysis pipelines together form a solution for auto-tagging content based on an index’s tag dictionary. 

Apache Stanbol – An open-source content enhancement engine (now an Apache Attic project, but its components live on in other forms). Stanbol provided services for entity linking and automatic tagging of content using referenced knowledge bases . It could be used to suggest tags for documents by recognizing named entities and concepts present in the text. While not as active today, Stanbol’s idea continues in projects like FISE and in some enterprise search pipelines. It’s relevant historically for tag recommendation via entity extraction and can be combined with index knowledge to enrich documents semantically. 

BioSolr Ontology Annotator – An extension for Solr (often used in the life sciences domain) that can annotate and tag documents using ontology terms . This tool takes RDF or SKOS vocabularies and automatically tags incoming documents with matching concepts (similar to SolrTextTagger but geared towards ontologies). In our context, if the “tags” are part of an ontology or taxonomy, BioSolr can recommend those tags for new documents. It effectively helps structure documents by known categories and is an example of bridging search with semantic web data. While niche, it addresses tag suggestion and classification in an interesting way. 

Fast Entity Linker – An open-source project for entity linking that uses machine learning to disambiguate and link text spans to knowledge base entries . By identifying key entities/ topics in a document, one can derive tags or keywords that should be associated with that document. In a system with an existing index of entities or tags, an entity linker can suggest which tags from that index apply to the new text. This project highlights the use of NLP for tag recommendation (through entity extraction) and is relevant where tags correspond to real-world entities or concepts. 34. 35. 

> 25

36. 

> 25

37. 

> 26

38. 

> 25

7Dexter – An open-source entity linking framework (developed in academic context) that tags documents by linking phrases to Wikipedia or other knowledge bases. It can be thought of as a tag recommendation system where the tags are Wikipedia concepts. While Dexter itself might not be actively updated, it showed effective methods for automatically annotating documents. It can be repurposed for tagging by treating the link targets as tags, or even for duplicate detection by comparing the sets of extracted entities between documents (if two docs share a nearly identical set of rare entities, they might be duplicates). Dexter thus contributes indirectly to both tag suggestion and similarity assessment. 

IBM Watson Discovery – A commercial enterprise search platform by IBM that incorporates AI for document understanding. Watson Discovery can ingest large sets of documents, perform natural language queries, and also analyze document similarity. It offers features like Smart Document Understanding (to structure docs) and uses ML to find related documents and even identify duplicates or near-duplicates (e.g., cluster news articles that are reprints of each other). Additionally, it can do automatic keyword extraction and concept tagging as part of its enrichment pipeline. As a proprietary solution, it’s notable for combining search, NLP, and clustering – addressing all four problem areas with a polished interface (albeit at enterprise cost). 

Amazon Kendra – A fully managed enterprise search service from AWS that uses machine learning to enhance search results. Kendra supports natural language queries and can intelligently rank results. It also provides features like FAQ detection and extraction. While details are proprietary, Kendra’s ML algorithms likely include semantic similarity matching (to bring up related content even if keywords differ) and could cluster or group content internally to avoid duplicate answers. It can also suggest facets (like tags or categories) to refine search. For a user wanting a managed solution, Kendra addresses example-based search (you can ask a question or give a document and get similar answers) and will filter out duplicates or very similar content in results. 

Microsoft Azure Cognitive Search – Azure’s search-as-a-service offering, which has recently added semantic ranking capability and vector search in preview. This service can ingest documents, index text and vectors, and allow semantic queries. It automatically provides facets which can act like tag recommendations for refining queries. With AI enrichment skills, it can extract key phrases and entities from documents (useful for tagging). It’s a maintained commercial solution where clustering is not exposed as a feature, but the underlying tech groups results by relevance. Duplicate detection would be up to the user (e.g., using their API to find near-identical content via fingerprint skill). Azure Cognitive Search is significant as a cloud alternative covering many IR tasks out-of-box. 

Azure Cognitive Services – Text Analytics – This is Azure’s API for NLP (separate from search). It can perform key phrase extraction , named entity recognition , and even text similarity and 

language detection . For our purposes, using the key phrase extraction on a new document can yield a set of candidate tags (which might overlap with existing tags in an index). It also has an 

anomaly detector and duplicate detection could be approximated by comparing key phrase sets or using the Similarity API (if available). As a managed service, it’s a quick way to add tag suggestion (by pulling out important terms) and content similarity checks without building models from scratch. 

Amazon Comprehend – Amazon’s NLP service that offers functions like keyphrase extraction, entity recognition, and document classification. Comprehend can be used to recommend tags by extracting the dominant topics or phrases from a document. If you have a set of predefined tags, 39. 40. 41. 42. 43. 44. 

8you could also use Comprehend’s custom classification to assign those tags to new content. For clustering and deduplication, Comprehend’s topic modeling feature can group documents into topics, and two documents sharing an almost identical topic distribution might indicate redundancy. It’s a commercial service, but relevant for quick integration of tag and topic intelligence into applications. 

Google Cloud Natural Language API – Google’s NLP service that provides entity extraction, category classification, and sentiment analysis. In our context, the NL API can automatically label documents with content categories (a form of tagging) and pull out entities (which could map to tags or be used as keywords). For duplicate detection or similarity, one could use the entity output or embed the content via Google’s embeddings and then compare vectors (Google’s API doesn’t directly give a similarity score between two documents, but the categories/entities could be used to roughly cluster documents). As a part of Google Cloud, it’s continuously improved and leverages Google’s deep NLP expertise. 

Lucidworks Fusion – A commercial platform built on Apache Solr, with added machine learning and analytics features. Fusion provides learning-to-rank, signals-based recommendations, and cognitive search capabilities. It can learn from user behavior which tags or results are relevant, improving tag suggestions and search results over time. Fusion’s “More Like This” queries and its ability to vectorize content via built-in ML models allow robust similar document recommendations . It also has a built-in deduplication pipeline (to collapse near-duplicate results) and can cluster results or documents using Spark under the hood. It’s geared for enterprise search deployments that need advanced relevancy tuning and AI features. 

Coveo – Coveo is a commercial enterprise search solution that heavily uses analytics and machine learning to optimize search and recommendations. It can automatically learn relationships between content pieces (e.g., which documents are often viewed together or share topics) to recommend similar items. For tagging, it can suggest query filters and related terms that act like tags. Coveo’s platform likely includes duplicate detection to avoid showing the same content multiple times, and might cluster content by topic for personalization. As a cloud service, it abstracts the complexity but is known to handle large-scale search with semantic tuning. 

Sinequa – A leading enterprise search platform that integrates NLP for deep content analysis. Sinequa can extract entities, detect themes, and classify documents into categories as it indexes data. These extracted concepts effectively serve as tags, enabling advanced faceted search and recommendations. Sinequa also offers a similar document feature through its analytics, and can cluster documents by identified topics or taxonomies. It’s used in scenarios like corporate knowledge bases where duplicate documents (e.g., multiple versions of a report) need to be identified – something Sinequa can assist by analyzing and comparing content. It’s a proprietary solution known for strong out-of-the-box NLP capabilities. 

Mindbreeze – An enterprise search appliance (and cloud service) that provides federated search across data sources with AI enhancements. Mindbreeze performs content analysis to create a “knowledge graph” of extracted information from documents – essentially tagging entities and concepts. This enhances search (you can filter or query by these tags) and also helps in finding related documents by shared tags or context. Mindbreeze typically de-duplicates search results and can present “clusters” or facets of information, grouping similar items. It is a commercial product, often compared with Solr/Lucidworks or SharePoint search, and covers tagging and similarity in a user-friendly way for enterprise use. 45. 46. 47. 48. 49. 

9LensKit – An open-source framework for building recommender systems (originally from GroupLens research). LensKit is more focused on collaborative filtering, but it includes item-item similarity algorithms. If you treat documents as “items” and perhaps users as some abstract concept (or just use item-item with content features), you could utilize LensKit to recommend similar documents. For example, using TF-IDF vectors of documents as features, LensKit could compute cosine similarities and recommend “more like this” documents. It can also handle the scenario of tag recommendation by treating tags as items and documents as “users” who have those tags – then recommending additional tags that similar documents have. LensKit is maintained in a research context and provides a variety of recommender algorithms to experiment with. 

LightFM – A Python library for hybrid (content + collaborative) recommendations. LightFM can train models that use both user-item interactions and item features. In a document tagging scenario, you could represent tags as items and documents as “users” with interactions (document has tag = interaction) and train a model to predict new tag interactions for a document – effectively recommending tags. Similarly, for example-based search, you could treat a given document’s tags or content vector as input and retrieve other documents with high predicted relevance. LightFM supports a content-based approach via item features (which could be derived from text). It’s actively maintained and known for being relatively easy to integrate, bridging the gap between pure content similarity and learned recommendations. 

Surprise (scikit-surprise) – A simple Python library for building and evaluating recommender system algorithms. Surprise focuses on matrix factorization and neighborhood methods. If the problem is reframed as a recommendation problem (documents ↔ tags, or documents ↔ other documents), Surprise can be used to compute similarities or latent factors. For instance, one could create a document-tag matrix and factorize it to recommend tags to documents based on learned patterns. Or a document-document similarity matrix can be built and used for “People who liked this document also liked…” -style recommendations. While Surprise is more limited to classic algorithms (not neural), it’s useful for baseline solutions and is easy to work with. 

Apache Mahout – An open-source machine learning library with implementations of clustering, classification, and collaborative filtering. Mahout, which can run on Hadoop or Spark, includes item-based recommender algorithms and clustering algorithms like k-Means and fuzzy k-Means that can be applied to document vectors. Mahout could cluster documents into groups of similar content (using its clustering jobs) for offline analysis of topics. It also had a feature for 

deduplication using clustering , where nearly identical documents might end up in the same cluster. While Mahout has lost some mindshare to newer ML tools, it’s still capable for large-scale tasks and covers a broad range of algorithms that can be applied to our problem set (multi-label naive Bayes for tagging, co-occurrence for tag suggestions, etc.). 

Tevatron – An open-source toolkit for training and running dense retrieval models (from the perspective of neural IR research). Tevatron allows you to fine-tune BERT-based models for retrieving documents given queries, which in turn can be used for example-based search (by treating a document as a query). Essentially, Tevatron produces dual-encoder models similar to DPR, where queries and documents are in the same vector space. After training on relevance data, it can be used to embed documents and find similar ones via dot product. It’s mainly a research tool, but very relevant if you need a custom semantic search model. Indirectly, it can help with duplicate detection (if trained to bring duplicates together) or tag recommendation (if tags are considered in training, or via query expansion for tags). 50. 51. 52. 53. 54. 

10 ColBERT – An academic/professional toolkit implementing the ColBERT (Collective BERT) model for efficient retrieval. ColBERT is a late-interaction neural search model that allows fine-grained similarity between a query and documents. While typically used for question answering or ad-hoc search, you can also use ColBERT for example-based document similarity: treat the example doc as a “query” and retrieve others. Because of late interaction, ColBERT can handle partial overlaps well (which is useful in detecting near-duplicates that may have extra content). It’s optimized to run on large corpora with GPU acceleration. For clustering, you could use the similarity scores from ColBERT to cluster or connect documents. ColBERT’s open-source release is used in research and some search engines for high-accuracy retrieval. 

Dense Passage Retriever (DPR) – An open-source method and toolkit by Facebook for open-domain question answering, which involves a dual encoder model to retrieve passages related to a query . While designed for Q&A, DPR can be repurposed: use the context encoder on documents to produce embeddings and use the query encoder on an example document (or its text) to get a query vector, then find similar docs by embedding dot product. Essentially it’s another way to do semantic document search . It requires a bit of machine learning expertise to fine-tune if your domain is specific. For duplicates, if two documents are paraphrases, a DPR model might embed them very closely. DPR is actively maintained as part of the Hugging Face Transformers and has seen real-world use in search applications. 

MatchZoo – An open-source deep text matching toolkit (in Python) that provides a collection of neural models for information retrieval tasks . It includes implementations of various “matching” neural networks (like DRMM, ARC-II, ConvKNRM etc.), which can be trained to learn similarity between pieces of text. Using MatchZoo, one can train a model to rank documents given an input document (“learning to recommend similar docs”). It’s more involved, but it directly addresses example-based search with ML. Some models could potentially be used for duplicate detection as well by setting a threshold on the matching score. MatchZoo is geared toward researchers but can be applied if a learning-to-match approach is desired for any of the similarity or tagging tasks (e.g., predict if a tag is relevant to a document by formulating it as a text matching problem). 

txtai – An open-source platform that builds semantic search and AI-powered indexing capabilities into applications. txtai provides an easy way to create embeddings for documents (using transformers) and offers a simple API to search for similar documents or answer questions . It also supports labeling: you can use zero-shot classifiers to tag documents and store those tags. For clustering, one could retrieve k-nearest neighbors for each document to group related items. txtai aims to be an all-in-one solution for adding semantic search to projects, covering many of the sub-tasks implicitly (similarity search, basic classification for tags, etc.). It’s actively maintained and abstracts away a lot of the heavy lifting by integrating with Hugging Face models. 

HyperTag (Ravn) – An open-source knowledge management tool that used deep learning and tags to organize files . HyperTag allowed semantic search over files and an expressive tag system, where it could suggest tags and build a graph of tags. It had features like visualizing the tag graph and performing fuzzy search on tag combinations. While the repository has been archived as of 2025, it demonstrated a practical approach to tag-based organization with semantic search (it could search files by content and auto-index tags) . Some experimental features included semantic text search and tag recommendation for files. HyperTag is a niche solution but directly tackled tagging and semantic retrieval in an integrated way. 55. 56. 

> 27

57. 

> 28

58. 

> 29

59. 

> 30
> 30

11 NearPy – A Python library for finding nearest neighbors via locality-sensitive hashing (LSH) .NearPy allows you to hash high-dimensional vectors (like document feature vectors) into buckets such that similar vectors end up in the same bucket. This is useful for approximate nearest neighbor search and thus example-based document retrieval . With LSH, it can quickly surface candidate duplicates or similar items. While NearPy is a bit older and less maintained, it’s simple to use and can be effective for moderate-sized problems where you want a quick way to find overlaps in content (using, say, random projection hashing). It’s more of a building block, but addresses the similarity and dedup problem space through algorithmic means. 

Simhash (simhash-py) – Simhash is an algorithm that computes a fingerprint (a binary hash) of a document such that similar documents have similar fingerprints. The Python library 

simhash-py (from SeoMoz/Mozilla) provides an efficient implementation of Charikar’s simhash for texts . By using simhash, one can index documents by their hashes and quickly identify when a new document is likely a near-duplicate of an existing one (if their simhash differs by only a few bits). Many web-scale deduplication systems (like for crawling or news aggregation) use Simhash due to its speed and low memory footprint. It’s not for tagging or semantic search per se, but specifically for finding documents with highly overlapping content and eliminating duplicates. The approach is unsupervised and language-agnostic (after proper tokenization). Simhash libraries are lightweight and widely used in practice for deduping. 

Datasketch (MinHash) – The datasketch Python library provides implementations of 

MinHash and locality-sensitive hashing for sets and documents. MinHash is useful to estimate Jaccard similarity between documents based on their shingles (substrings or word n-grams). Using datasketch, you can create a MinHash signature for each document and store them in an LSH index; then query for near-matches efficiently . This is a classic solution for near-duplicate detection : documents with a high Jaccard similarity (e.g., >0.8) are likely plagiarized or duplicate. It can also cluster documents by content similarity using these signatures. While datasketch is focused on the duplicate-finding aspect, it’s a crucial piece when you need to scan large archives for overlapping content. 

text-dedup – A modern open-source library by Chenghao Mou for comprehensive text deduplication in datasets . It includes multiple strategies: MinHash + LSH, SimHash (64/128-bit), suffix array based substring matching, and bloom filter exact deduplication . This tool can identify exact and near duplicates in a corpus and even cluster them (optionally saving clusters of duplicates). It’s particularly useful for cleaning large NLP datasets, but the same techniques apply to document management – you can discover if two documents have large overlaps. By using text-dedup, one can automatically flag documents that are likely duplicates (high content overlap) and even automatically remove or merge them. It’s actively maintained and provides a “ready-to-use” pipeline for deduping, which is otherwise a combination of tasks. 

SemHash (MinishLab) – SemHash is a tool/library for semantic text deduplication and representative sampling . It expands on traditional simhash by incorporating semantic information. The idea is to cluster and filter outliers in text datasets such that you remove duplicates and perhaps pick prototypical examples. It’s useful in data preprocessing for machine learning, but also for document management where you want one representative from a set of near-duplicates. SemHash can be considered an evolution of hashing-based dedup with some semantic awareness. For someone focusing on duplicates with a bit more nuance (semantic similarity, not just lexical), this could be a handy solution if maintained. 

Dedupe.io (Python library) – Dedupe is an open-source Python library for record linkage and de-duplication using machine learning. While often applied to structured data (like finding 60. 31 

61. 

> 32

62. 

> 33

63.  

> 34
> 35 34

64. 

> 36

65. 

12 duplicate customer entries), it can be applied to text documents by using portions of text or metadata as features. Dedupe trains a logistic regression model to decide if two records are duplicates based on labeled examples. In a documents context, you could feed it pairs of documents with a label “duplicate or not” (perhaps based on known duplicate news articles) and then it could learn which features (e.g. common 5-grams, Jaccard similarity, etc.) predict duplication. This supervised approach can catch duplicates even when phrasing differs but content is the same. It’s actively maintained and used in data cleaning tasks; while requiring some training data, it adds a smart layer on top of hashing methods. 

RapidFuzz – A fast Python library for fuzzy string matching (inspired by FuzzyWuzzy but much faster). RapidFuzz can compute edit distances and similarity ratios between strings very quickly. For short texts or document titles, RapidFuzz can be used to detect near duplicates (e.g., if two titles differ by a few characters or typos). It can also help in tag normalization (ensuring similar tags are treated as the same). While not a full deduplication system on its own, it’s a useful component for finding highly overlapping content when the differences are minor rearrangements or typos. Combined with other methods, RapidFuzz (or its underlying Levenshtein distance) can identify cases where one document is a lightly edited version of another. 

copydetect – An open-source library/tool for detecting text re-use or plagiarism. It works by identifying long common substrings between documents (shingling approach) and is geared towards finding where in the text the overlap occurs. This is useful not just for academic plagiarism, but also for detecting duplicate or merged documents (if one document contains large parts of another). copydetect provides detailed output on how similar two documents are and where the similarity lies. For our scope, it clearly addresses the overlapping content detection 

problem. If you have a large set of documents, copydetect (or similar algorithms) can flag pairs that have suspiciously large common passages, indicating duplication or one quoting the other extensively. 

LanceDB – A new open-source vector database built on Apache Arrow (columnar format) that emphasizes fast query times and local-first usage. LanceDB stores embeddings in an efficient format and can perform similarity search, often integrated in LLM workflows. It’s relevant to semantic search tasks as it can handle example-based queries and retrieval-augmented generation. For our list, LanceDB would be used to index document embeddings and quickly query for nearest neighbors (similar docs). It doesn’t inherently do clustering or tagging, but it’s a fresh alternative to more heavyweight vector DBs, and could be attractive for developers wanting an embedded-vector-store approach. 

Tantivy (with Vectors) – Tantivy is a Rust-based full-text search engine (equivalent to Lucene) known for its speed. It recently introduced optional support for vector embeddings to enable hybrid search . Using Tantivy, you can perform traditional keyword queries and also do a cosine similarity search on stored vectors. This means you can implement “find similar documents” functionality by storing a vector for each document (from, say, SBERT) and querying with a new document’s vector. The hybrid nature lets you balance keyword and semantic similarity. Tantivy itself doesn’t do clustering or deduplication, but as a search engine, one could fetch top candidates and then cluster those if needed. It’s actively maintained in the Rust community and is the core of the MeiliSearch engine (though Meili’s own vector support is in development). 

OpenSemanticSearch (Suite) – An open-source suite that bundles together Apache Lucene/Solr with various text mining tools for semantic search capabilities . It provides an integrated environment with faceted search, thesaurus-based query expansion, collaborative tagging, and 66. 67. 68. 69. 

> 37

70.  

> 38 39

13 analytics. Within OpenSemanticSearch, you can do full-text search combined with synonym expansion (to catch semantic matches) and it supports tagging and annotation of documents by users . It’s not a single algorithm but a collection: for example, it has connectors for OCR, an ontology-based auto-tagger (as we saw), and integration with Carrot2 for clustering. As a platform, it directly addresses tag recommendation (through thesauri and annotation interfaces) and can perform some level of automatic tagging with its ontology tagger . It also has exploratory search features that effectively cluster results by facets or concepts. This makes it a comprehensive solution if one wants an out-of-the-box system to experiment with all these features on their own server. 

TagSpaces – An open-source personal data management tool that allows tagging of files and documents. TagSpaces is primarily manual tagging and organizing, but it has some automation: for text files and PDFs, it can extract content to make them searchable, and for media, it can auto-extract metadata (like dominant colors for images) . While it doesn’t do AI-driven tag suggestion for text content, its search can find files by tags or content, which enables a user to simulate example-based finding of similar documents (via keyword overlap). TagSpaces represents a user-centric approach to tagging and finding documents – relevant in a list of solutions because it shows how tagging can be used in practice to organize documents and then retrieve similar ones (if consistently tagged). It’s actively developed for cross-platform use. 

Paperless-ngx – An open-source document management system that focuses on scanning and organizing digital documents. Paperless-ngx performs OCR on documents and can automatically 

classify documents (for example, identify if a document is an invoice, receipt, etc.) and tag them based on rules or content. Users report that it can indeed auto-tag documents by analyzing the text (with user-defined rules or ML models). This addresses the tag recommendation problem in a practical way – if you add a new document, Paperless might tag it “Utilities” or “Bank” based on learned cues. Its search function can find similar documents by content (since everything is OCR’ed and indexed). It also prevents duplicate storage of the same file. This system shows an applied combination of OCR, tagging, and search that’s very relevant to anyone dealing with scanned documents or PDFs. 

Wikidata-based Tagger – There are various tools that use Wikidata (or Wikipedia) to tag documents with entities or topics. For instance, one might use the Wikidata knowledge graph to annotate a document with relevant QIDs (entities). This is similar to entity linking but specifically drawing from Wikidata’s ontology. By tagging documents with Wikidata entities, you essentially attach semantic tags that can then be used to find similar documents (documents sharing many entities) or cluster documents by topics (all documents tagged with a certain entity could form a cluster). Several open-source projects and APIs (like TagMe, DBpedia Spotlight, etc.) fall in this category. They help bridge unstructured text and structured knowledge, solving tag recommendation in a semantic way and indirectly aiding similarity and clustering by those tags. 

KeyBERT – A simple yet powerful Python library for keyword extraction using BERT embeddings. KeyBERT takes a document, generates an embedding for the whole document and for each candidate keyword, and then finds the keywords that are most similar to the document embedding (hence presumably most representative of its content). This produces a set of key phrases for the document that often make good tags. As an unsupervised approach, KeyBERT is great for recommending tags based on content when no tagged training data is available. Those extracted keywords can also be used to find similar documents (e.g., by comparing keyword sets or using them as a query). KeyBERT is actively maintained and widely used for quick insight into document themes. 

> 40
> 41

71. 

> 42

72. 73. 74. 

14 YAKE (Yet Another Keyword Extractor) – An unsupervised keyphrase extraction algorithm that uses statistical features (like term frequency, word placement, and context) to identify important keywords in a document. YAKE is language-independent and doesn’t require training. It will output a list of keywords that could serve as tags for the document. This directly addresses tag recommendation by content analysis. While YAKE doesn’t know about tags in an existing index per se, if the document shares topics with existing ones, their YAKE keywords would likely overlap. It’s a fast and lightweight approach to tagging and can complement other methods by providing candidate tags that can be filtered against an index’s tag list. 

RAKE (Rapid Automatic Keyword Extraction) – One of the earlier algorithms for extracting key phrases from individual documents. RAKE works by looking at word frequency and co-occurrence, identifying multi-word phrases that capture salient concepts (ignoring stopwords). As a result, RAKE can suggest tags or keywords that summarize a document. It’s completely unsupervised and easy to implement. Though older, it’s still useful for quick tag suggestions and has implementations in many languages. RAKE could be used to pre-tag new documents and then match those tags with the ones in your index (or even add new tags if you allow). It doesn’t handle similarity or clustering beyond providing a set of content descriptors, but those descriptors can be used for similarity matching between documents. 

AutoPhrase – An open-source tool for quality phrase mining from text. AutoPhrase is designed to extract meaningful phrases from a large corpus using a small seed list of high-quality phrases. It uses statistical signals and optionally knowledge bases to score and extract phrases. In context, AutoPhrase can generate a controlled vocabulary of important phrases (potential tags) from your entire index and then annotate each document with those phrases if present. This results in consistent tagging across documents with key concepts. New documents can be tagged by checking which of the learned high-quality phrases they contain. This helps in recommending tags (from a set of “approved” phrases) and also grouping documents by these semantic phrases (useful for clustering by topics). AutoPhrase was developed in academia (UIUC) and can significantly aid in structuring unstructured text. 

pke (Pipeline for Keyphrase Extraction) – An open-source toolkit that implements a variety of keyphrase extraction algorithms (like TF-IDF, TextRank, KEA, TopicRank, etc.) under a unified API. With pke, one can try different methods to automatically suggest tags for documents and see which works best. For instance, TextRank (a graph-based ranking of terms) might highlight different tags than KEA (which is a supervised method if trained). pke allows integration of these keyphrases into search or tagging systems easily. By extracting keyphrases with any of its algorithms for both new and existing documents, you can create a similarity measure (documents share keyphrases) or cluster by keyphrases. It’s a valuable toolkit for experimenting with tag extraction techniques in one place. 

TextRank – Although an algorithm rather than a standalone tool, TextRank is important to mention as it’s behind many keyword extraction and summarization methods. It builds a graph of words (or sentences) and uses a PageRank-like algorithm to score them. The highest-scoring words often make good keywords (tags) for the document. It’s completely unsupervised and domain-independent. Many libraries (like the pke mentioned above or 

gensim.summarization ) have TextRank implementations. For tag recommendation, TextRank can give a quick set of likely tags from the content itself. It can also be extended to sentence level to find similar documents (e.g., by comparing summary sentences), but primarily it’s a tagging aid. It’s widely used due to its simplicity and effectiveness. 75. 76. 77. 78. 79. 

15 Google Vertex AI Matching Engine – A cloud service (part of Google Cloud’s Vertex AI) for high-scale vector similarity search. Matching Engine can index billions of embeddings and find nearest neighbors in milliseconds, leveraging Google’s ANN algorithms. For semantic document search, you can feed in document embeddings and then use Matching Engine to perform 

example-based lookups or cluster by pulling nearest neighbors for each point. It’s managed, so it scales easily for production. This service doesn’t do tagging or duplicate removal by itself, but as an infrastructure component it’s top-tier for the core similarity search functionality (with the reliability of Google’s cloud). Companies with massive document stores use such services to power recommendations and semantic search without maintaining their own vector servers. 

OpenAI Embeddings API – A commercial API endpoint provided by OpenAI that generates high-quality embeddings (e.g., using the Ada model) for given text. While not a full solution, using OpenAI’s embeddings in combination with any vector search library can supercharge semantic search and clustering. The embeddings are trained to capture semantic similarity (so example-based search works impressively well with minimal effort). You can also embed tags or metadata and compare vectors to recommend tags to documents (by embedding the document and each candidate tag description and finding highest cosine similarity). For duplicate detection, if two documents yield almost identical embeddings from a strong model, they are very likely semantically duplicate. OpenAI’s approach is proprietary, but as a tool it’s widely used to quickly add semantic capabilities – the developer only needs to handle the search part with something like FAISS or Pinecone as discussed. 

Cohere – Cohere is a provider of NLP models accessible via API, including multilingual embeddings and classification models. Cohere’s embedding model (e.g., embed-english-v2 )can be used similarly to OpenAI’s: generate vector representations for documents, tags, etc., and then perform example-based search or clustering using those vectors. Additionally, Cohere offers a classification API which can be trained to assign labels (tags) to text, which directly addresses the tag recommendation problem in a supervised way. As a commercial solution, it doesn’t require managing infrastructure or model training locally – one can leverage state-of-the-art semantic understanding with simple API calls. It’s relevant in our list as it provides high-quality semantic tools (though not an integrated “search engine”, it plugs into many of the pipelines we’ve discussed). 

Neo4j + Graph Data Science – Neo4j is a graph database, and its Graph Data Science (GDS) library includes algorithms for similarity and clustering on graph data. If documents are represented in a graph (for instance, as nodes connected by shared tags or shared content features), GDS can run community detection algorithms (like Louvain or label propagation) to cluster similar documents. It also has node similarity algorithms that can compute cosine similarity between attribute vectors on nodes, which could detect duplicates or recommend similar nodes. For tagging, one could propagate tags through a graph of document similarities (a form of collaborative tagging). While this is a more complex approach, it leverages relationships – e.g., build a graph where an edge means two docs share many words or tags, then cluster that graph. Neo4j is actively maintained and can handle these kinds of analysis at reasonably large scales. 

SharePoint Search (Microsoft) – In corporate environments, SharePoint’s built-in search has some capabilities relevant here. It can suggest refinements (like tags or document types) based on query context, find similar results (especially with the newer Microsoft Search which integrates Graph insights), and avoid showing duplicate documents (identical files aren’t listed twice). It also allows content enrichment via Microsoft Graph (which can add metadata tags, etc.). While not open-source, it’s a commonly encountered solution that addresses these problems in a 80. 81. 82. 83. 84. 

16 practical way: for example, SharePoint can show “See also” documents (similar by content or title) and uses indexing strategies to not index the same content twice. For organizations already using Microsoft’s ecosystem, this is often the default approach to tagging (via managed metadata services) and search. 

Elastic ELSER (Elastic Learned Sparse Encoder) – A feature in Elastic Stack (since 8.x) providing a built-in semantic search model that doesn’t require external vectors. ELSER is a model that converts text into a bag-of-words-like sparse vector that captures semantic meaning, which Elastic can index and search using its inverted index mechanics . This allows semantic “More Like This” without an external ML infrastructure. With ELSER, you can index documents, and then a query (or example doc) is encoded similarly and used to retrieve semantically similar docs. It’s useful for those who want semantic search but prefer not to maintain vector indexes or external models. For clustering, one could fetch semantic vectors and use them externally; for tag suggestion, one might use ELSER to find similar indexed docs and aggregate their tags. It’s a newer approach by Elastic, showing how search engines are merging ML directly into their core. 

Gensim’s Similarity Index – Aside from topic modeling, Gensim also provides a Similarity 

class that can create an index for a corpus (using bag-of-words, TF-IDF, or LSI vectors) and then quickly retrieve documents similar to a new document. This is essentially a lightweight example-based search tool. It’s not using neural embeddings but traditional vectors, which can still be effective for domain-specific text. Gensim’s similarity has advantages in simplicity and not needing GPU or large models. It can also be used for duplicate detection by setting a high similarity threshold – if the cosine similarity of TF-IDF vectors is extremely high, documents are likely near-duplicates. For small to medium corpora, this approach is often sufficient and very fast. Gensim is actively maintained (though moving towards more wrapper around external tools nowadays) and remains a friendly entry point for IR tasks. 

MLlib in Apache Spark – Spark’s machine learning library contains algorithms like MinHash LSH , TF-IDF vectorization , KMeans clustering , and more, which can be applied at big data scale. If you have a very large document collection, Spark can compute pairwise similarities using MinHash LSH to find duplicates or cluster items (e.g., via KMeans or spectral clustering on document vectors). Spark’s distributed processing means you can feasibly process tens of millions of documents to detect duplicates or form clusters, which would be challenging with single-machine tools. It’s not turnkey (you have to write the job), but it’s one of the few ways to address our tasks at web scale openly. Spark MLlib, for example, has been used to cluster news articles in production systems and to deduplicate web crawl data. 

AllenNLP – An open-source NLP research library by the Allen Institute for AI. With AllenNLP, one can build custom models for tasks like text classification (tags prediction), textual entailment (to check if one doc contains the content of another, useful for duplicates), or semantic textual similarity (direct regression of similarity score). It provides high-level APIs to train models on your data. So if you have a specialized need (say legal document tag classification or clustering by a certain criteria), you can implement and train with AllenNLP. It’s more effort than using pre-built tools, but offers flexibility. AllenNLP includes many pre-trained models too (for NER, classification, etc.) which can be leveraged for tagging or recognizing key info in documents. It’s well-maintained and widely used in academic NLP. 

Flair NLP – Flair is an open-source NLP library from Zalando that focuses on ease of use and state-of-the-art models for sequence labeling and text classification. It offers pre-trained models for things like NER, part-of-speech tagging, and also allows training custom text classifiers (including multi-label). Using Flair, you could train a multi-label tag classifier for your documents 85. 

> 2

86. 87. 88. 89. 

17 (or even use zero-shot learning via its integrations). Flair also has word embeddings (including contextual string embeddings) that can be used to create document embeddings for similarity search. It’s simpler than writing PyTorch models from scratch but gives you powerful models under the hood. For clustering, you’d manually cluster the embeddings Flair produces. It’s actively developed and often updated with new NLP advancements. 

NLTK & SpaCy (for basics) – These classic NLP libraries provide foundational processing that can assist our tasks. NLTK (Natural Language Toolkit) has utilities for shingling texts, computing similarity metrics, and basic classifiers. It could be used to implement a fuzzy duplicate detection by comparing Jaccard similarity of word sets or to train a simple Naive Bayes tag classifier. spaCy ,on the other hand, gives you a robust pipeline (tokenization, lemmatization, NER) and even has built-in similarity (each spaCy Doc or Span can produce a vector, either from static embeddings or transformer if enabled, and you can get a similarity score). spaCy’s similarity isn’t as advanced as SBERT, but it can capture topical similarity to an extent (especially if using transformer models via spaCy). Both libraries can extract entities or noun phrases that serve as candidate tags. They are actively maintained and serve as the backbone for many custom solutions. 

OpenRec – An open-source modular framework for deep learning-based recommender systems (by researchers at UCSD) . OpenRec allows you to construct recommendation models (including content-based ones) in a flexible way. For instance, you can define a recommender where the “items” are documents and the “users” could be thought of as tags or vice versa. It supports extending with your own modules, so you could plug in a text CNN or BERT encoder as part of the scoring function. Using OpenRec, one could build an item-to-item recommendation 

model that suggests similar documents (treating one doc as context and recommending others) or a tag recommendation model that suggests tags to a document based on learned interactions. It’s a more advanced route – essentially letting you research your own approach – but it’s designed for extensibility in the recommender domain. 

RecBole – A comprehensive open-source recommendation system library from a collaboration of Chinese universities. RecBole implements a plethora of algorithms (including content-based, collaborative, knowledge-based, and sequential models) under a unified framework. For our interests, RecBole could be used to experiment with algorithms like ItemKNN (item-item similarity based on interaction or attribute vectors) for document similarity, or with multi-label classification treated as multiple recommendation tasks. For example, you can treat each tag as an “item” and each document as a “user” with interactions (document has tag). RecBole can then apply any collaborative filtering algorithm to recommend new tags to a document (this is akin to treating tag recommendation as a recommendation problem). It’s a powerful toolkit if one is comfortable mapping the problem to a recommendation scenario and is actively updated with state-of-the-art methods. 

Microsoft Recommenders (Repo) – Microsoft has an open-source repository on GitHub called 

Recommenders that showcases best practices and implementations for recommendation systems (covering collaborative, content-based, deep learning, etc.). Within this repo, there are examples of using techniques like TF-IDF for content similarity, word embeddings for item similarity, and even how to use algorithms like SAR (Smart Adaptive Recommendations) which is a co-occurrence based method. Adapting these examples: one can treat document content as the basis for content-based filtering or use the co-occurrence of tags to recommend new tags (tags frequently seen together – a kind of clustering of tags). The repository is maintained by Microsoft and is a treasure trove of ready-made code that can often be repurposed for document and tag recommendation tasks without starting from scratch. 90. 91. 

> 43

92. 93. 

18 Graph-Based Deduplication ( like “Shingling + Connected Components”) – Not a product, but worth noting a common pattern: represent documents as nodes in a graph, connect two nodes with an edge if they share a shingle (or if their similarity is above a threshold), then find connected components or cliques in this graph. Each connected component would be a set of documents that are all similar (a cluster of near-duplicates). Tools like Apache Spark or even custom Python networkx can be used for this. This approach has been used in web crawlers (e.g., finding duplicate web pages) and could be applied with existing libraries: e.g., use 

datasketch to LSH into buckets and then networkx to connect those that fell into same buckets, then find components. It’s a bit low-level, but it’s effective for large-scale deduping and clustering without heavy ML. The “output” is essentially clusters of duplicate documents and maybe a representative for each cluster. 

BERT-based Semantic Clustering – A technique that uses BERT embeddings and then applies clustering on those embeddings to group documents. While we mentioned BERTopic and similar libraries that automate this, one can DIY with libraries like transformers to get embeddings and then use, say, Affinity Propagation or Agglomerative Clustering from scikit-learn to cluster them. Affinity Propagation is interesting because it chooses “exemplars” – actual documents that are central to each cluster – which could serve as representatives for duplicate sets or main topics. Many blog posts and GitHub projects demonstrate this approach on news articles or customer feedback. It’s not a single product but rather an approach combining existing libraries to achieve the goal of grouping semantically similar documents, which addresses the clustering and duplicate detection (if very tight clusters) aspects well. 

Plagiarism Detection Software (e.g., Turnitin, PlagAware) – These are typically commercial services but they illustrate advanced duplicate detection. Turnitin, for example, has a huge index of documents and uses proprietary algorithms (likely shingling and fingerprinting) to detect overlaps between a new document and any in their database, highlighting exact matching passages. For an organization dealing with many documents, using such a service can solve the 

overlapping content detection problem thoroughly – they will find even partial matches and paraphrases. Some plagiarism tools (like the open source WCopyfind) focus on exact string matches, while others like Copyleaks also use AI to catch paraphrased duplicates. Including these is relevant as they represent the state-of-the-art in automated duplicate/content overlap detection, albeit as services or closed software. 

Knowledge Graph-based Tagging (e.g., DBpedia Spotlight) – Similar to Wikidata tagger, DBpedia Spotlight is an open-source tool that tags text by linking it to DBpedia entries (the structured part of Wikipedia). By tagging documents with concepts from a knowledge graph, you effectively get a semantic fingerprint of the document which can be used for recommendation and clustering. For example, two documents tagged with many of the same DBpedia concepts are likely similar in topic (this can detect similarity even if wording differs, as long as they mention the same concepts). Spotlight can be self-hosted and customized to some extent. It directly gives you potential tags (the DBpedia URIs or names) for new documents based on content, addressing tag rec, and those tags can be used as features for clustering or duplicate detection (documents with nearly identical sets of DBpedia tags are probably duplicates). 

THRESH (Near-duplicate Detection Algorithm) – THRESH (Taerum, Holt, etc., as referenced in some literature) is an algorithm used by search engines like AltaVista for near-duplicate detection via word histograms. While not packaged, the concept is: create a vector of word frequencies for each document, quantize it (to reduce size), and then use a hashing or bitmask technique to quickly flag documents that have similar histograms. Implementation can be done with a combination of hashing libraries and bit operations in, say, Java or C++. Some open-source 94. 95. 96. 97. 98. 

19 search engines had similar functionality (e.g., Solr had a FingerprintFilter that could be used to generate a signature for duplicate collapse). This entry highlights that search engines often integrate threshold-based duplicate collapsing to avoid showing users repeated content – an important practical feature in IR. 

One-hot Tag Embeddings + ANN – If you have a large tag vocabulary and want to recommend tags, one method is to create an embedding for documents based solely on tags of similar documents. For instance, represent each document as a binary vector of tag occurrences (or weighted by TF-IDF of tags if tags have different importance). Then use an ANN library (like Annoy or FAISS) to find nearest neighbors for a new document’s vector (which might be incomplete as it has no tags or only a few tags). The neighbors in this “tag space” can suggest what tags are missing. Essentially, if many neighbors have tag X, recommend tag X. While simplistic, this approach uses existing tags to recommend new tags (collaborative filtering in tag-space). It requires an initial set of tags on documents (which we have in the index). It’s something that can be implemented with standard libraries and often yields sensible results, as similar documents (by content) often ended up with similar tag patterns. 

Stack Overflow Tag Predictor (Software) – Finally, to give an example of a specialized project: there have been numerous open-source projects and Kaggle competitions focused on predicting tags for Stack Overflow questions. These typically employ multi-label classification using text features. One notable approach was using gradient boosting on bag-of-words or using deep learning (like CNNs or RNNs) to output multiple tags. While not a single official software, the techniques and code from these projects can be repurposed for general document tag recommendation. They illustrate using multi-label neural networks or problem transformation methods (like training one model per tag or using label embedding) at scale. For instance, one could train a Transformer-based classifier that given a document outputs a probability for each possible tag (across the index’s tag set). The existence of these community projects means there’s precedent and available code for multi-label tag prediction in an open-source manner, which directly addresses the first problem (recommending tags for a new doc based on an indexed corpus). Many are active on GitHub and benefit from large datasets to pretrain. Each of the above solutions has its strengths and scope of applicability. Some (especially the search engines and vector databases) cover multiple problems – enabling semantic search, duplicate detection and clustering with some custom work – while others are highly specialized (focusing only on tagging or only on deduplication). By combining a few of them (e.g., using an embedding model with a vector index, or using a keyword extractor with a search engine’s faceting), one can build a robust system that tags documents, finds related documents, clusters them by content, and flags duplicates automatically. The projects listed are active as of 2025/2026 (or are foundational enough to still be relevant), and they represent the top-tier and most feasible tools for tackling these information retrieval and NLP tasks.
"""  
~~~~~~
`````````
  ]]</solutions_100>
  
  <constraints_text>[[
`````````constraints_text
~~~~~~
"""
url: `https://raw.githubusercontent.com/ib-bsb-br/ib-bsb-br.github.io/refs/heads/main/_posts/2024-07-20-vpc3588.md`

# hardinfo

## Computer

### Summary

#### Computer

- **Processor:** Rockchip RK3588 ; 4x ARM Cortex-A55 r2p0 (AArch64) + 4x ARM Cortex-A76 r4p0 (AArch64)
- **Memory:** 32535MB (6101MB used)
- **Machine Type:** Single-board computer
- **Operating System:** Debian GNU/Linux 11.11
- **User Name:** root (root)
- **Date/Time:** Fri 09 Jan 2026 09:46:16 AM -03
#### Display

- **Resolution:** 2560x1944 pixels
- **Display Adapter:** arm Mali family
- **OpenGL Renderer:** llvmpipe (LLVM 11.0.1, 128 bits)
- **Session Display Server:** The X.Org Foundation 1.20.11
#### Audio Devices

- **Audio Adapter:** rockchip-es8388 - rockchip-es8388i
- **Audio Adapter:** rockchip-hdmi0 - rockchip-hdmi0
#### Input Devices

- **remotectl-gpio:** Keyboard
- **rockchip-es8388i Headset:** Audio
- **Logitech USB Receiver:** Keyboard
- **Logitech USB Receiver Mouse:** Audio
- **Logitech USB Receiver Consumer Control:** Keyboard
- **adc-keys:** Keyboard
- **Logitech USB Receiver System Control:** Keyboard
- **rockchip-hdmi0 rockchip-hdmi0:** Audio
#### Printers (CUPS)

- **thermal:**
#### UDisks2

- **mmcblk0boot1:** sSCA128
- **mmcblk0:** sSCA128
- **mmcblk0boot0:** sSCA128
- **sda:** sMT-512

### Operating System

#### Version

- **Kernel:** Linux 5.10.198 (aarch64)
- **Command Line:**
    ```text
    storagemedia=emmc androidboot.storagemedia=emmc androidboot.mode=normal
    androidboot.verifiedbootstate=orange rw rootwait
    earlycon=uart8250,mmio32,0xfeb50000 console=ttyFIQ0 irqchip.gicv3_pseudo_nmi=0
    root=PARTUUID=614e0000-0000 rcupdate.rcu_expedited=1 rcu_nocbs=all
    androidboot.fwver=ddr-v1.18-9fa84341ce,spl-v1.13,bl31-v1.47,bl32-v1.15,uboot-310
    11-dirt-01/23/2025
    ```
- **Version:** #11 SMP Sat Aug 24 10:18:01 CST 2024
- **C Library:** GNU C Library / (Debian GLIBC 2.31-13+deb11u13) 2.31
- **Distribution:** Debian GNU/Linux 11.11 (bullseye)
#### Current Session

- **Computer Name:** linaro-alip
- **User Name:** root (root)
- **Language:** en_US.UTF-8 (en_US.UTF-8)
- **Home Directory:** /root
- **Desktop Environment:** Unknown (Window Manager: unknown)
#### Misc

- **Uptime:** 4 hours 9 minutes
- **Load Average:** 1.54, 1.42, 1.45

### Security

#### HardInfo2

- **HardInfo2 running as:** Superuser
- **User System Type:** Single User System
#### Health

- **Available entropy in /dev/random:** 256 bits (medium)
#### Hardening Features

- **ASLR:** Fully enabled (mmap base+stack+VDSO base+heap)
- **dmesg:** Access allowed (running as superuser)
#### Linux Security Modules

- **Modules available:** Unknown
- **SELinux status:** Not installed
#### CPU Vulnerabilities

- **gather_data_sampling:** Not affected
- **itlb_multihit:** Not affected
- **l1tf:** Not affected
- **mds:** Not affected
- **meltdown:** Not affected
- **mmio_stale_data:** Not affected
- **retbleed:** Not affected
- **spec_rstack_overflow:** Not affected
- **spec_store_bypass:** Mitigation: Speculative Store Bypass disabled via prctl
- **spectre_v1:** Mitigation: __user pointer sanitization
- **spectre_v2:** Mitigation: CSV2, BHB
- **srbds:** Not affected
- **tsx_async_abort:** Not affected

### Kernel Modules

#### Loaded Modules

- **aic8800_bsp:**
- **aic8800_fdrv:**

### Boots

#### Boots

- **Fri Jan 9 05:37:21 2026:** 5.10.198
- **Fri Jan 9 05:03:11 2026:** 5.10.198
- **Thu Jan 8 18:24:22 2026:** 5.10.198
- **Tue Jan 6 18:38:34 2026:** 5.10.198
- **Tue Jan 6 17:13:50 2026:** 5.10.198
- **Tue Jan 6 16:56:46 2026:** 5.10.198
- **Tue Jan 6 14:09:00 2026:** 5.10.198
- **Mon Jan 5 17:03:30 2026:** 5.10.198
- **Mon Jan 5 16:07:30 2026:** 5.10.198
- **Thu Jan 1 16:10:46 2026:** 5.10.198
- **Tue Dec 30 13:07:09 2025:** 5.10.198
- **Mon Dec 29 02:36:11 2025:** 5.10.198
- **Sun Dec 28 23:55:16 2025:** 5.10.198
- **Sun Dec 28 23:54:22 2025:** 5.10.198
- **Fri Dec 19 20:01:36 2025:** 5.10.198
- **Fri Dec 19 19:45:02 2025:** 5.10.198
- **Fri Dec 19 16:29:17 2025:** 5.10.198
- **Tue Dec 2 13:11:21 2025:** 5.10.198
- **Thu Nov 27 15:28:26 2025:** 5.10.198
- **Thu Nov 27 08:05:20 2025:** 5.10.198
- **Tue Nov 25 03:44:10 2025:** 5.10.198
- **Tue Nov 11 12:41:50 2025:** 5.10.198
- **Wed Oct 22 19:59:33 2025:** 5.10.198
- **Wed Oct 22 16:25:23 2025:** 5.10.198
- **Wed Oct 22 16:24:47 2025:** 5.10.198

### Languages

#### Available Languages

- **en_US.utf8:** English locale for the USA
- **pt_BR.utf8:** Portuguese locale for Brasil
- **zh_CN.utf8:** Chinese locale for Peoples Republic of China
- **C.UTF-8:** C locale

### Memory Usage

#### Memory

- **Active(anon):** 11404 KiB|Anonymous memory used more recently and not swapped out
- **Active(file):** 2555080 KiB|Pagecache memory been used more recently and not reclaimed
- **Active:** 2566484 KiB|Memory used more recently and not swapped out or reclaimed
- **AnonPages:** 4317828 KiB|Non-file backed pages mapped into userspace page tables
- **Bounce:** 0 KiB|Memory used for block device bounce buffers
- **Buffers:** 329208 KiB|Memory in buffer cache, temporary storage for raw disk blocks
- **Cached:** 6298704 KiB|Memory in the page cache (diskcache, shared memory, tmpfs and shmem)
- **CmaAllocated:** 4128 KiB|CmaAllocated
- **CmaFree:** 0 KiB|Free remaining memory in the CMA reserves
- **CmaReleased:** 4064 KiB|CmaReleased
- **CmaTotal:** 8192 KiB|Memory reserved for the Contiguous Memory Allocator (CMA)
- **CommitLimit:** 16267820 KiB|Total memory currently available to be allocated on the system
- **Committed_AS:** 16695132 KiB|The amount of memory presently allocated on the system
- **Dirty:** 984 KiB|Memory waiting to be written back to disk
- **Inactive(anon):** 4602716 KiB|Anonymous memory not been used and can be swapped out
- **Inactive(file):** 3681184 KiB|Pagecache memory reclaimable without huge performance impact
- **Inactive:** 8283900 KiB|Memory not been used recently and can be swapped out or reclaimed
- **KReclaimable:** 643048 KiB|Kernel allocations reclaimable under memory pressure
- **KernelStack:** 16400 KiB|Memory consumed by the kernel stacks of all tasks
- **Mapped:** 828828 KiB|Files which have been mmapped, such as libraries
- **MemAvailable:** 26803668 KiB|Available memory for allocation to any process, without swapping
- **MemFree:** 20235068 KiB|Free memory which is not used for anything
- **MemTotal:** 32535644 KiB|Total physical memory usable by the system
- **Mlocked:** 16 KiB|Pages locked to memory using the mlock() system call
- **NFS_Unstable:** 0 KiB|Previous counted pages which had been written to the server
- **PageTables:** 55984 KiB|Memory consumed by userspace page tables
- **Percpu:** 1536 KiB|Memory allocated to the percpu allocator
- **SReclaimable:** 611904 KiB|Part of Slab, that might be reclaimed, such as caches
- **SUnreclaim:** 66468 KiB|Part of Slab, that cannot be reclaimed on memory pressure
- **Shmem:** 391600 KiB|Total memory used by shared memory (shmem) and tmpfs
- **Slab:** 678372 KiB|In-kernel data structures cache
- **SwapCached:** 0 KiB|Memory present within main memory and in the swapfile
- **SwapFree:** 0 KiB|Virtual memory, remaining swap space available
- **SwapTotal:** 0 KiB|Virtual memory, total swap space available
- **Unevictable:** 95916 KiB|Unevictable pages can't be swapped out for a variety of reasons
- **VmallocChunk:** 0 KiB|Largest contiguous block of vmalloc area which is free
- **VmallocTotal:** 263061440 KiB|Total size of vmalloc virtual address space
- **VmallocUsed:** 61228 KiB|Amount of vmalloc area which is used
- **Writeback:** 0 KiB|Memory which is actively being written back to disk
- **WritebackTmp:** 0 KiB|Memory used by FUSE for temporary writeback buffers

### Filesystems

#### Mounted File Systems

- **/dev/root:** / | 68.69 % (4.3 GiB of 13.7 GiB)
- **devtmpfs:** /dev | 0.00 % (15.5 GiB of 15.5 GiB)
- **tmpfs:** /dev/shm | 0.78 % (15.4 GiB of 15.5 GiB)
- **tmpfs:** /run | 0.66 % (6.2 GiB of 6.2 GiB)
- **tmpfs:** /run/lock | 0.08 % (5.0 MiB of 5.0 MiB)
- **/dev/sda1:** /mnt/mSATA | 89.72 % (48.1 GiB of 468.4 GiB)
- **/dev/sda1:** /home/linaro | 89.72 % (48.1 GiB of 468.4 GiB)
- **/dev/mmcblk0p7:** /oem | 15.04 % (102.5 MiB of 120.6 MiB)
- **/dev/mmcblk0p8:** /userdata | 89.63 % (10.4 GiB of 99.9 GiB)
- **/dev/mmcblk0p8:** /opt | 89.63 % (10.4 GiB of 99.9 GiB)
- **/dev/mmcblk0p8:** /srv | 89.63 % (10.4 GiB of 99.9 GiB)
- **/dev/mmcblk0p8:** /usr/local | 89.63 % (10.4 GiB of 99.9 GiB)
- **/dev/mmcblk0p8:** /var/log | 89.63 % (10.4 GiB of 99.9 GiB)
- **tmpfs:** /run/user/1000 | 0.00 % (3.1 GiB of 3.1 GiB)
- **Filen:** /tmp/filen | 0.63 % (2.0 TiB of 2.0 TiB)

### Display

#### Session

- **Type:** x11
#### Wayland

- **Current Display Name:** (Not Available)
#### X Server

- **Current Display Name:** :0.0
- **Vendor:** The X.Org Foundation
- **Version:** 1.20.11
- **Release Number:** 12011000
#### Screens

- **Screen 0:** 2560x1944 pixels
#### Outputs (XRandR)

- **HDMI-1:** Connected; 2560x1080 pixels, offset (0, 0)
- **DP-1:** Connected; 1152x864 pixels, offset (0, 1080)
#### OpenGL (GLX)

- **Vendor:** Mesa/X.org
- **Renderer:** llvmpipe (LLVM 11.0.1, 128 bits)
- **Direct Rendering:** Yes
- **Version (Compatibility):** 3.1 Mesa 20.3.5
- **Shading Language Version (Compatibility):** 1.40
- **Version (Core):** 4.5 (Core Profile) Mesa 20.3.5
- **Shading Language Version (Core):** 4.50
- **Version (ES):** OpenGL ES 3.2 Mesa 20.3.5
- **Shading Language Version (ES):** OpenGL ES GLSL ES 3.20
- **GLX Version:** 1.4
#### Vulkan

- **Instance Version:** 1.2.162
- **Api Version:** 4194306 (1.0.2)
- **Driver Version:** 1 (0x0001)
- **Vendor:** Mesa
- **Device Type:** PHYSICAL_DEVICE_TYPE_CPU
- **Device Name:** llvmpipe (LLVM 11.0.1, 128 bits)
- **Driver Name:** llvmpipe
- **Driver Info:** Mesa 20.3.5 (LLVM 11.0.1)
- **Conformance Version:** 1.0.0.0

### Environment Variables

#### Environment Variables

- **ALACRITTY_LOG:** /tmp/Alacritty-447031.log
- **ALACRITTY_SOCKET:** /tmp/Alacritty-:0.0-447031.sock
- **ALACRITTY_WINDOW_ID:** 60817411
- **COLORTERM:** truecolor
- **DISPLAY:** :0.0
- **HOME:** /root
- **LANG:** en_US.UTF-8
- **LOGNAME:** root
- **LS_COLORS:**
- **MAIL:** /var/mail/root
- **OLDPWD:** /home/linaro
- **PATH:** /root/.cargo/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin
- **PWD:** /
- **SHELL:** /bin/bash
- **SHLVL:** 1
- **SUDO_COMMAND:** /usr/bin/x-terminal-emulator
- **SUDO_GID:** 1000
- **SUDO_UID:** 1000
- **SUDO_USER:** linaro
- **TERM:** alacritty
- **USER:** root
- **WINDOWID:** 60817411
- **XAUTHORITY:** /home/linaro/.Xauthority
- **_:** /usr/bin/hardinfo2

### Development

#### Scripting Languages

- **Gambas3 (gbr3):** Not found
- **Python (default):** Not found
- **Python2:** Not found
- **Python3:** 3.9.2
- **Perl:** 5.32.1
- **Rakudo (Perl6):** Not found
- **PHP:** Not found
- **Ruby:** Not found
- **Bash:** 5.1.4
- **JavaScript (Node.js):** Not found
- **awk:** 5.1.0
#### Compilers

- **C (GCC):** 10.2.1
- **C (Clang):** 11.0.1
- **D (dmd):** Not found
- **Gambas3 (gbc3):** Not found
- **Java:** 11.0.29
- **.NET:** Not found
- **Vala:** Not found
- **Haskell (GHC):** Not found
- **FreePascal:** Not found
- **Go:** 1.24.3
- **Rust:** 1.87.0
#### Tools

- **make:** 4.3
- **ninja:** 1.10.1
- **GDB:** Not found
- **LLDB:** Not found
- **strace:** 5.10
- **valgrind:** Not found
- **QMake:** 3.1
- **CMake:** 4.2.1
- **Gambas3 IDE:** Not found
- **Radare2:** Not found
- **ltrace:** Not found
- **Powershell:** 7.6.0

### Users

#### Users

- **_apt:**
- **_flatpak:** Flatpak system-wide installation helper
- **avahi:** Avahi mDNS daemon
- **avahi-autoipd:** Avahi autoip daemon
- **backup:** backup
- **bin:** bin
- **colord:** colord colour management daemon
- **daemon:** daemon
- **dnsmasq:** dnsmasq
- **games:** games
- **gnats:** Gnats Bug-Reporting System (admin)
- **irc:** ircd
- **jellyfin:** Jellyfin default user
- **lightdm:** Light Display Manager
- **linaro:** linaro
- **list:** Mailing List Manager
- **lp:** lp
- **mail:** mail
- **man:** man
- **messagebus:**
- **news:** news
- **nobody:** nobody
- **ntp:**
- **proxy:** proxy
- **pulse:** PulseAudio daemon
- **root:** root
- **saned:**
- **sshd:**
- **strongswan:**
- **sync:** sync
- **sys:** sys
- **systemd-coredump:** systemd Core Dumper
- **systemd-network:** systemd Network Management
- **systemd-resolve:** systemd Resolver
- **systemd-timesync:** systemd Time Synchronization
- **tss:** TPM software stack
- **uucp:** uucp
- **uuidd:**
- **www-data:** www-data

### Groups

#### Group

- **_flatpak:** 118
- **_ssh:** 110
- **adm:** 4
- **audio:** 29
- **avahi-autoipd:** 122
- **avahi:** 125
- **backup:** 34
- **bin:** 2
- **bluetooth:** 108
- **cdrom:** 24
- **colord:** 124
- **daemon:** 1
- **dialout:** 20
- **dip:** 30
- **disk:** 6
- **fax:** 21
- **floppy:** 25
- **games:** 60
- **gnats:** 41
- **hardinfo2:** 1001
- **i2c:** 112
- **input:** 105
- **irc:** 39
- **jellyfin:** 117
- **kmem:** 15
- **kvm:** 106
- **lightdm:** 116
- **linaro:** 1000
- **list:** 38
- **lp:** 7
- **lpadmin:** 119
- **mail:** 8
- **man:** 12
- **messagebus:** 104
- **netdev:** 111
- **news:** 9
- **nogroup:** 65534
- **ntp:** 109
- **operator:** 37
- **pardus-software:** 126
- **plugdev:** 46
- **proxy:** 13
- **pulse-access:** 115
- **pulse:** 114
- **rdma:** 113
- **render:** 107
- **root:** 0
- **saned:** 123
- **sasl:** 45
- **scanner:** 121
- **shadow:** 42
- **src:** 40
- **ssl-cert:** 120
- **staff:** 50
- **sudo:** 27
- **sys:** 3
- **systemd-coredump:** 998
- **systemd-journal:** 101
- **systemd-network:** 102
- **systemd-resolve:** 103
- **systemd-timesync:** 999
- **tape:** 26
- **tss:** 128
- **tty:** 5
- **users:** 100
- **utmp:** 43
- **uucp:** 10
- **uuidd:** 127
- **video:** 44
- **voice:** 22
- **www-data:** 33

## Devices

### Device Tree

#### Device Tree

- **Summary:**
- **Details:**
  - **Board**
  - **Model:** Rockchip RK3588 DXB LP4 V10 Board
  - **Serial Number:** "86bbb302f8210401"
  - **Compatible:** "rockchip,rk3588-vpc-vga-v10", "rockchip,rk3588"

- **Maps:**
- **Messages:**

### System DMI

#### DMI Unavailable

- **DMI is not available. Perhaps this platform does not provide DMI.:**

### Processor

#### Processors

- **SOC/Package Information:**
- **Details:**
  - **SOC/Package**
  - **Name:** Rockchip RK3588
  - **Description:** 4x ARM Cortex-A55 r2p0 (AArch64) + 4x ARM Cortex-A76 r4p0 (AArch64)
  - **Topology:** 3 physical processors; 8 cores; 8 threads
  - **Logical CPU Config:** 4x 2256.00 MHz + 4x 1800.00 MHz
  - **Distro and CPU Supported Profiles**
  - **HWCAPS:** aarch64
  - **Clocks**
  - **1800.00-1800.00 MHz:** 1x
  - **408.00-2256.00 MHz:** 2x
  - **Caches**
  - **Level 1 (Data):** 4x 32KB (128KB), 4-way set-associative, 128 sets
  - **Level 1 (Instruction):** 4x 32KB (128KB), 4-way set-associative, 128 sets
  - **Level 2 (Unified):** 4x 128KB (512KB), 4-way set-associative, 512 sets
  - **Level 3 (Unified):** 1x 3072KB (3072KB), 12-way set-associative, 4096 sets
  - **Level 1 (Data):** 2x 64KB (128KB), 4-way set-associative, 256 sets
  - **Level 1 (Instruction):** 2x 64KB (128KB), 4-way set-associative, 256 sets
  - **Level 2 (Unified):** 2x 512KB (1024KB), 8-way set-associative, 1024 sets
  - **Level 3 (Unified):** 1x 3072KB (3072KB), 12-way set-associative, 4096 sets
  - **Level 1 (Data):** 2x 64KB (128KB), 4-way set-associative, 256 sets
  - **Level 1 (Instruction):** 2x 64KB (128KB), 4-way set-associative, 256 sets
  - **Level 2 (Unified):** 2x 512KB (1024KB), 8-way set-associative, 1024 sets
  - **Level 3 (Unified):** 1x 3072KB (3072KB), 12-way set-associative, 4096 sets

- **ARM Cortex-A55 r2p0 (AArch64):** 1800.00 MHz
- **ARM Cortex-A55 r2p0 (AArch64):** 1800.00 MHz
- **ARM Cortex-A55 r2p0 (AArch64):** 1800.00 MHz
- **ARM Cortex-A55 r2p0 (AArch64):** 1800.00 MHz
- **ARM Cortex-A76 r4p0 (AArch64):** 2256.00 MHz
- **ARM Cortex-A76 r4p0 (AArch64):** 2256.00 MHz
- **ARM Cortex-A76 r4p0 (AArch64):** 2256.00 MHz
- **ARM Cortex-A76 r4p0 (AArch64):** 2256.00 MHz

### Graphics Processors

#### GPUs

- **dt-soc-gpu:** arm Mali family
- **Details:**
  - **Device Information**
  - **GPU:** Integrated (Rockchip RK3588)
  - **Location:** SOC
  - **Vendor:** ARM
  - **Device:** Mali family
  - **Clocks**
  - **Core:** 300.00-1000.00 MHz
  - **Frequency Scaling**
  - **Minimum:** 300000 kHz
  - **Maximum:** 1000000 kHz
  - **Transition Latency:** 0 ns
  - **Source:** Operating Points (OPPv2)
  - **Device Tree Node**
  - **Path:** /gpu@fb000000
  - **Compatible:** "arm,mali-bifrost"
  - **Status:** "okay"
  - **Name:** "gpu"

### Monitors

#### Monitors

- **card0-HDMI-A-1:** 28.8″ 25UM58G
- **Details:**
  - **Connection**
  - **DRM:** card0-HDMI-A-1
  - **Status:** connected enabled
  - **Signal Type:** Digital
  - **Interface:** [0] (Unspecified)
  - **Bits per Color Channel:** (Unspecified)
  - **Speaker Allocation:** Front left and right
  - **Output (Max)**
  - **VESA EDID STD:** 1152x1536@75Hz 67.0x28.0cm (28.6") progressive normal
  - **EIA/CEA-861 SVD:** 1920x1080@60Hz 67.3x28.4cm (28.8") progressive normal
  - **EDID Device**
  - **Vendor:** [PNP:GSM] Goldstar Company
  - **Name:** 25UM58G
  - **Model:** [5b98-01010101] 23448-16843009
  - **Serial:** (Unknown)
  - **Manufacture Date:** Week 1 of 2015
  - **EDID Meta**
  - **Data Size:** 256 bytes
  - **Version:** 1.3
  - **Extension Blocks:** 1
  - **Extended to:** EIA/CEA-861
  - **Checksum:** Ok
  - **EDID Descriptors**
  - **descriptor0:** ([00] detailed timing descriptor) {...}
  - **descriptor1:** ([00] detailed timing descriptor) {...}
  - **descriptor2:** ([fc] display name) 25UM58G
  - **descriptor3:** ([fd] display range limits) 00 00 00 fd 00 38 4b 1e 5a 18 00 0a 20 20 20 20 20 20
  - **Detailed Timing Descriptors (DTD)**
  - **dtd0:** 2560x1080@59Hz, 67.3x28.4cm (28.8") progressive normal (VESA EDID DTD)
  - **dtd1:** 1920x1080@60Hz, 67.3x28.4cm (28.8") progressive normal (VESA EDID DTD)
  - **dtd2:** 1920x1080@60Hz, 59.8x33.7cm (27.0") progressive normal (EIA/CEA-861 DTD)
  - **dtd3:** 1920x540@60Hz, 59.8x33.7cm (27.0") interlaced normal (EIA/CEA-861 DTD)
  - **dtd4:** 1280x720@60Hz, 59.8x33.7cm (27.0") progressive normal (EIA/CEA-861 DTD)
  - **dtd5:** 2560x1080@74Hz, 67.7x29.0cm (29.0") progressive normal (EIA/CEA-861 DTD)
  - **Established Timings Bitmap (ETB)**
  - **etb0:** 720x400@70Hz 67.0x28.0cm (28.6") progressive normal
  - **etb1:** 640x480@60Hz 67.0x28.0cm (28.6") progressive normal
  - **etb2:** 640x480@75Hz 67.0x28.0cm (28.6") progressive normal
  - **etb3:** 800x600@60Hz 67.0x28.0cm (28.6") progressive normal
  - **etb4:** 800x600@75Hz 67.0x28.0cm (28.6") progressive normal
  - **etb5:** 1024x768@60Hz 67.0x28.0cm (28.6") progressive normal
  - **etb6:** 1024x768@75Hz 67.0x28.0cm (28.6") progressive normal
  - **etb7:** 1280x1024@75Hz 67.0x28.0cm (28.6") progressive normal
  - **etb8:** 1152x870@75Hz 67.0x28.0cm (28.6") progressive normal
  - **Standard Timings (STD)**
  - **std0:** 1152x1536@75Hz 67.0x28.0cm (28.6") progressive normal
  - **std1:** 1280x1024@60Hz 67.0x28.0cm (28.6") progressive normal
  - **std2:** 1280x720@60Hz 67.0x28.0cm (28.6") progressive normal
  - **std3:** 1600x900@60Hz 67.0x28.0cm (28.6") progressive normal
  - **std4:** 1680x1050@60Hz 67.0x28.0cm (28.6") progressive normal
  - **E-EDID Extension Blocks**
  - **ext0:** ([02:v03] EIA/CEA-861 extension (CEA-EXT)) ok
  - **EIA/CEA-861 Data Blocks**
  - **cea_block0:** ([2] video) svds:9
  - **cea_block1:** ([1] audio) sads:1
  - **cea_block2:** ([4] speaker allocation) len:3
  - **cea_block3:** ([3] vendor specific) len:5 (OUI:000c03) -- 65 03 0c 00 10 00
  - **EIA/CEA-861 Short Audio Descriptors**
  - **sad0:** format:([1] LPCM) channels:2 rates:32, 44, 48 kHz depths: 16-bit, 20-bit, 24-bit
  - **EIA/CEA-861 Short Video Descriptors**
  - **svd0:** 1920x1080@60Hz progressive normal
  - **svd1:** 1280x720@60Hz progressive normal
  - **svd2:** 720x480@60Hz progressive normal
  - **svd3:** 1920x1080@50Hz interlaced normal
  - **svd4:** 720x576@50Hz progressive normal
  - **svd5:** 1920x1080@60Hz interlaced normal
  - **svd6:** 1920x1080@50Hz progressive normal
  - **svd7:** 640x480@60Hz progressive normal
  - **svd8:** 1280x720@50Hz progressive normal
  - **DisplayID Timings**
  - **(Empty List):**
  - **DisplayID Strings**
  - **(Empty List):**
  - **Hex Dump**
  - **Data:**
      ```text
      00ffffffffffff001e6d985b01010101
      0119010380431c78eaca95a6554ea126
      0f5054a54b80714f818081c0a9c0b300
      0101010101017e4800e0a0381f404040
      3a00a11c21000018023a801871382d40
      582c4500a11c2100001e000000fc0032
      35554d3538470a2020202020000000fd
      00384b1e5a18000a20202020202001f5
      02031cf1499004031412051f01132309
      07078301000065030c001000023a8018
      71382d40582c450056512100001e011d
      8018711c1620582c250056512100009e
      011d007251d01e206e28550056512100
      001e295900a0a038274030203a00a522
      2100001a000000ff0000000000000000
      000000000000000000000000000000a1
      ```

- **card0-Writeback-1:** (Unknown)
- **Details:**
  - **Connection**
  - **DRM:** card0-Writeback-1
  - **Status:** unknown disabled

- **card0-DP-1:** 18.5″ 1950W
- **Details:**
  - **Connection**
  - **DRM:** card0-DP-1
  - **Status:** connected enabled
  - **Signal Type:** Analog
  - **Interface:** [0] (Unspecified)
  - **Bits per Color Channel:** (Unspecified)
  - **Speaker Allocation:** (Unspecified)
  - **Output (Max)**
  - **VESA EDID STD:** 1280x720@60Hz 41.0x23.0cm (18.5") progressive normal
  - **VESA EDID DTD:** 1366x768@59Hz 41.0x23.0cm (18.5") progressive normal
  - **EDID Device**
  - **Vendor:** [PNP:AOC] AOC International
  - **Name:** 1950W
  - **Model:** [1950-00000000] 6480-0
  - **Serial:** (Unknown)
  - **Manufacture Date:** 2011
  - **EDID Meta**
  - **Data Size:** 128 bytes
  - **Version:** 1.4
  - **Extension Blocks:** 0
  - **Extended to:** (None)
  - **Checksum:** Ok
  - **EDID Descriptors**
  - **descriptor0:** ([00] detailed timing descriptor) {...}
  - **descriptor1:** ([00] detailed timing descriptor) {...}
  - **descriptor2:** ([fd] display range limits) 00 00 00 fd 00 32 4c 1e 3c 09 00 0a 20 20 20 20 20 20
  - **descriptor3:** ([fc] display name) 1950W
  - **Detailed Timing Descriptors (DTD)**
  - **dtd0:** 1366x768@59Hz, 41.0x23.0cm (18.5") progressive normal (VESA EDID DTD)
  - **dtd1:** 1360x768@60Hz, 41.0x23.0cm (18.5") progressive normal (VESA EDID DTD)
  - **Established Timings Bitmap (ETB)**
  - **etb0:** 720x400@70Hz 41.0x23.0cm (18.5") progressive normal
  - **etb1:** 640x480@60Hz 41.0x23.0cm (18.5") progressive normal
  - **etb2:** 640x480@67Hz 41.0x23.0cm (18.5") progressive normal
  - **etb3:** 640x480@72Hz 41.0x23.0cm (18.5") progressive normal
  - **etb4:** 640x480@75Hz 41.0x23.0cm (18.5") progressive normal
  - **etb5:** 800x600@56Hz 41.0x23.0cm (18.5") progressive normal
  - **etb6:** 800x600@60Hz 41.0x23.0cm (18.5") progressive normal
  - **etb7:** 800x600@72Hz 41.0x23.0cm (18.5") progressive normal
  - **etb8:** 800x600@75Hz 41.0x23.0cm (18.5") progressive normal
  - **etb9:** 832x624@75Hz 41.0x23.0cm (18.5") progressive normal
  - **etb10:** 1024x768@60Hz 41.0x23.0cm (18.5") progressive normal
  - **etb11:** 1024x768@70Hz 41.0x23.0cm (18.5") progressive normal
  - **etb12:** 1024x768@75Hz 41.0x23.0cm (18.5") progressive normal
  - **Standard Timings (STD)**
  - **std0:** 1280x720@60Hz 41.0x23.0cm (18.5") progressive normal
  - **std1:** 640x400@70Hz 41.0x23.0cm (18.5") progressive normal
  - **E-EDID Extension Blocks**
  - **(Empty List):**
  - **EIA/CEA-861 Data Blocks**
  - **(Empty List):**
  - **EIA/CEA-861 Short Audio Descriptors**
  - **(Empty List):**
  - **EIA/CEA-861 Short Video Descriptors**
  - **(Empty List):**
  - **DisplayID Timings**
  - **(Empty List):**
  - **DisplayID Strings**
  - **(Empty List):**
  - **Hex Dump**
  - **Data:**
      ```text
      00ffffffffffff0005e3501900000000
      00150104682917782a7b40a259559a25
      0f5054bfee0081c0310a010101010101
      010101010101662156aa51001e30468f
      33009ae61000001e662150b051001b30
      407036009ae61000001e000000fd0032
      4c1e3c09000a202020202020000000fc
      0031393530570a202020202020200058
      ```

### Memory Devices

#### Memory Device List

- **Result:** (Not available - See hint box below)

### PCI Devices

#### PCI Devices

- **0002:20:00.0:** RK3588
- **0002:21:00.0:** RTL8111/8168/8211/8411 PCI Express Gigabit Ethernet Controller

### USB Devices

#### USB Devices

- **001:001:** (null) 2.0 root hub
- **002:001:** (null) 2.0 root hub
- **002:002:** Terminus Technology Inc. FE 2.1 7-port Hub
- **003:001:** (null) 1.1 root hub
- **004:001:** (null) 1.1 root hub
- **005:001:** (null) 2.0 root hub
- **005:002:** Genesys Logic, Inc. Hub
- **005:003:** (null) Nano Receiver
- **006:001:** (null) 3.0 root hub
- **006:002:** Genesys Logic, Inc. Hub

### Firmware

#### SCA128

- **Version:** 0x100050912002b000
- **VendorId:** EMMC:0x0000df
- **Serial:** 0xa7e0b103
- **Protocol:** org.jedec.mmc
- **Plugin:** emmc
- **Icon:** media-memory
- **Guid:** 978380d9-bacb-5e2b-8063-cb1cbe533eaa
- **Guid:** 87547605-5433-51a2-b481-49204a05c695
- **Guid:** 31b7cf70-a297-543f-b512-815caa03cdad
- **Flags:**
    ```text
    [internal] Device cannot be removed easily
    [updatable] Device is updatable in this or any other mode
    [registered] Has been registered with other plugins
    ```
- **DeviceId:** a73b625424ee3cdda922614eee1cf65e8e8f76dc
- **Created:** 01/09/2026

### Printers

#### Printers (CUPS)

- **thermal:**

### Battery

#### Power Status

- **Power State:** AC
#### No batteries

- **No batteries found on this system:**

### Sensors

#### Temperature

- **temp1:** 49.00°C|soc_thermal
- **temp1:** 49.00°C|bigcore0_thermal
- **temp1:** 49.00°C|bigcore1_thermal
- **temp1:** 49.00°C|littlecore_thermal
- **temp1:** 48.08°C|center_thermal
- **temp1:** 48.08°C|gpu_thermal
- **temp1:** 48.08°C|npu_thermal
- **thermal_zone2:** 49.00°C|thermal
- **thermal_zone0:** 49.00°C|thermal
- **thermal_zone5:** 48.08°C|thermal
- **thermal_zone3:** 49.00°C|thermal
- **thermal_zone1:** 49.00°C|thermal
- **thermal_zone6:** 48.08°C|thermal
- **thermal_zone4:** 48.08°C|thermal
#### CPU Frequency

- **cpu0:** 1800.00 MHz|cpufreq
- **cpu1:** 1800.00 MHz|cpufreq
- **cpu2:** 1800.00 MHz|cpufreq
- **cpu3:** 1800.00 MHz|cpufreq
- **cpu4:** 2256.00 MHz|cpufreq
- **cpu5:** 2256.00 MHz|cpufreq
- **cpu6:** 2256.00 MHz|cpufreq
- **cpu7:** 2256.00 MHz|cpufreq
#### Drive Temperature

- **MT-512:** 33.00°C|udisks2

### Input Devices

#### Input Devices

- **remotectl-gpio:** Keyboard
- **rockchip-es8388i Headset:** Audio
- **Logitech USB Receiver:** Keyboard
- **Logitech USB Receiver Mouse:** Audio
- **Logitech USB Receiver Consumer Control:** Keyboard
- **adc-keys:** Keyboard
- **Logitech USB Receiver System Control:** Keyboard
- **rockchip-hdmi0 rockchip-hdmi0:** Audio

### Storage

#### UDisks2

- **mmcblk0boot1:** SCA128 | 116.5 GiB
- **mmcblk0:** SCA128 | 116.5 GiB
- **mmcblk0boot0:** SCA128 | 116.5 GiB
- **sda:** MT-512 | 476.9 GiB

### Resources

#### I/O Ports

- **00000000-000fffff:** pcie@fe170000
- **00001000-00001fff:** PCI Bus 0002:21
- **00001000-000010ff:** 0002:21:00.0
- **00001000-000010ff:** r8168
#### Memory

- **0010f000-0010f0ff:** 10f000.sram sram@10f000
- **00110000-00127fff:** ramoops:dmesg(0/1)
- **00128000-0013ffff:** ramoops:dmesg(1/1)
- **00140000-001bffff:** ramoops:console
- **001c0000-001effff:** ramoops:pmsg
- **00200000-083fffff:** System RAM
- **00400000-01a8ffff:** Kernel code
- **01a90000-0209ffff:** reserved
- **020a0000-0245ffff:** Kernel data
- **08300000-0832bfff:** reserved
- **09400000-efffffff:** System RAM
- **e9f00000-ee1a3fff:** reserved
- **f2200000-f2ffffff:** pcie@fe170000
- **f2200000-f22fffff:** PCI Bus 0002:21
- **f2200000-f2203fff:** 0002:21:00.0
- **f2200000-f2203fff:** r8168
- **f2204000-f2204fff:** 0002:21:00.0
- **f2204000-f2204fff:** r8168
- **f2300000-f230ffff:** 0002:20:00.0
- **fb000000-fb1fffff:** fb000000.gpu
- **fc00c100-fc3fffff:** fc000000.usb usb@fc000000
- **fc400000-fc407fff:** usb@fc400000
- **fc400000-fc407fff:** xhci-hcd.3.auto usb@fc400000
- **fc40c100-fc7fffff:** fc400000.usb usb@fc400000
- **fc800000-fc83ffff:** fc800000.usb usb@fc800000
- **fc840000-fc87ffff:** fc840000.usb usb@fc840000
- **fc880000-fc8bffff:** fc880000.usb usb@fc880000
- **fc8c0000-fc8fffff:** fc8c0000.usb usb@fc8c0000
- **fd880000-fd880fff:** fd880000.i2c i2c@fd880000
- **fd890000-fd89001f:** serial
- **fd8a0000-fd8a00ff:** fd8a0000.gpio gpio@fd8a0000
- **fda40000-fda400ff:** fda40000.pvtm pvtm@fda40000
- **fda50000-fda500ff:** fda50000.pvtm pvtm@fda50000
- **fda60000-fda600ff:** fda60000.pvtm pvtm@fda60000
- **fdab9000-fdab90ff:** fdab9000.iommu iommu@fdab9000
- **fdaba000-fdaba0ff:** fdab9000.iommu iommu@fdab9000
- **fdaca000-fdaca0ff:** fdab9000.iommu iommu@fdab9000
- **fdada000-fdada0ff:** fdab9000.iommu iommu@fdab9000
- **fdaf0000-fdaf00ff:** fdaf0000.pvtm pvtm@fdaf0000
- **fdb30000-fdb300ff:** fdb30000.pvtm pvtm@fdb30000
- **fdb50800-fdb5083f:** fdb50800.iommu iommu@fdb50800
- **fdb60f00-fdb60fff:** fdb60f00.iommu iommu@fdb60f00
- **fdb70f00-fdb70fff:** fdb70f00.iommu iommu@fdb70f00
- **fdb90480-fdb904bf:** fdb90480.iommu iommu@fdb90480
- **fdba0800-fdba083f:** fdba0800.iommu iommu@fdba0800
- **fdba4800-fdba483f:** fdba4800.iommu iommu@fdba4800
- **fdba8800-fdba883f:** fdba8800.iommu iommu@fdba8800
- **fdbac800-fdbac83f:** fdbac800.iommu iommu@fdbac800
- **fdbb0800-fdbb08ff:** fdbb0800.iommu iommu@fdbb0800
- **fdbdf000-fdbdf03f:** fdbdf000.iommu iommu@fdbdf000
- **fdbdf040-fdbdf07f:** fdbdf000.iommu iommu@fdbdf000
- **fdbef000-fdbef03f:** fdbef000.iommu iommu@fdbef000
- **fdbef040-fdbef07f:** fdbef000.iommu iommu@fdbef000
- **fdc38700-fdc3873f:** fdc38700.iommu iommu@fdc38700
- **fdc38740-fdc3877f:** fdc38700.iommu iommu@fdc38700
- **fdc48700-fdc4873f:** fdc48700.iommu iommu@fdc48700
- **fdc48740-fdc4877f:** fdc48700.iommu iommu@fdc48700
- **fdca0000-fdca05ff:** fdca0000.iommu iommu@fdca0000
- **fdd10000-fdd1ffff:** fdd10000.mipi0-csi2-hw csihost_regs
- **fdd20000-fdd2ffff:** fdd20000.mipi1-csi2-hw csihost_regs
- **fdd30000-fdd3ffff:** fdd30000.mipi2-csi2-hw csihost_regs
- **fdd40000-fdd4ffff:** fdd40000.mipi3-csi2-hw csihost_regs
- **fdd50000-fdd5ffff:** fdd50000.mipi4-csi2-hw csihost_regs
- **fdd60000-fdd6ffff:** fdd60000.mipi5-csi2-hw csihost_regs
- **fdd90000-fdd941ff:** fdd90000.vop regs
- **fdd95000-fdd95fff:** fdd90000.vop gamma_lut
- **fdd97e00-fdd97eff:** fdd97e00.iommu iommu@fdd97e00
- **fdd97f00-fdd97fff:** fdd97e00.iommu iommu@fdd97e00
- **fddf0000-fddf0fff:** fddf0000.i2s i2s@fddf0000
- **fddf4000-fddf4fff:** fddf4000.i2s i2s@fddf4000
- **fddf8000-fddf8fff:** fddf8000.i2s i2s@fddf8000
- **fde50000-fde53fff:** fde50000.dp dp@fde50000
- **fde80000-fde8ffff:** fde80000.hdmi hdmi@fde80000
- **fde90000-fde9ffff:** fde80000.hdmi hdmi@fde80000
- **fe060000-fe06ffff:** fe060000.dfi dfi@fe060000
- **fe170000-fe17ffff:** fe170000.pcie pcie-apb
- **fe210000-fe210fff:** fe210000.sata sata@fe210000
- **fe2d0000-fe2d3fff:** fe2d0000.mmc mmc@fe2d0000
- **fe2e0000-fe2effff:** fe2e0000.mmc mmc@fe2e0000
- **fe378000-fe3781ff:** fe378000.rng rng@fe378000
- **fe470000-fe470fff:** fe470000.i2s i2s@fe470000
- **fea10000-fea13fff:** dma-controller@fea10000
- **fea10000-fea13fff:** fea10000.dma-controller dma-controller@fea10000
- **fea30000-fea33fff:** dma-controller@fea30000
- **fea30000-fea33fff:** fea30000.dma-controller dma-controller@fea30000
- **fea50000-fea50fff:** fea50000.can can@fea50000
- **fea60000-fea60fff:** fea60000.can can@fea60000
- **fea90000-fea90fff:** fea90000.i2c i2c@fea90000
- **fead0000-fead0fff:** fead0000.i2c i2c@fead0000
- **feaf0000-feaf00ff:** feaf0000.watchdog watchdog@feaf0000
- **feb10000-feb10fff:** feb10000.spi spi@feb10000
- **feb20000-feb20fff:** feb20000.spi spi@feb20000
- **feb60000-feb6001f:** serial
- **feb80000-feb8001f:** serial
- **febb0000-febb001f:** serial
- **febc0000-febc001f:** serial
- **fec00000-fec003ff:** fec00000.tsadc tsadc@fec00000
- **fec10000-fec1ffff:** fec10000.saradc saradc@fec10000
- **fec20000-fec200ff:** fec20000.gpio gpio@fec20000
- **fec30000-fec300ff:** fec30000.gpio gpio@fec30000
- **fec40000-fec400ff:** fec40000.gpio gpio@fec40000
- **fec50000-fec500ff:** fec50000.gpio gpio@fec50000
- **fec80000-fec80fff:** fec80000.i2c i2c@fec80000
- **fec90000-fec90fff:** fec90000.i2c i2c@fec90000
- **fecc0000-fecc03ff:** fecc0000.otp otp@fecc0000
- **fed10000-fed13fff:** dma-controller@fed10000
- **fed10000-fed13fff:** fed10000.dma-controller dma-controller@fed10000
- **fed60000-fed61fff:** fed60000.hdmiphy hdmiphy@fed60000
- **fed80000-fed8ffff:** fed80000.phy phy@fed80000
- **fed90000-fed9ffff:** fed90000.phy phy@fed90000
- **feda0000-fedaffff:** feda0000.phy phy@feda0000
- **fedb0000-fedbffff:** fedb0000.phy phy@fedb0000
- **fedc0000-fedc7fff:** fedc0000.csi2-dphy0-hw csi2-dphy0-hw@fedc0000
- **fedc8000-fedcffff:** fedc8000.csi2-dphy1-hw csi2-dphy1-hw@fedc8000
- **fee00000-fee000ff:** fee00000.phy phy@fee00000
- **fee10000-fee100ff:** fee10000.phy phy@fee10000
- **ff001000-ff0effff:** ff001000.sram sram@ff001000
- **100000000-3fbffffff:** System RAM
- **3fc500000-3ffefffff:** System RAM
- **400000000-7ffffffff:** System RAM
- **7d8a00000-7fb5fffff:** reserved
- **7fb633000-7fb692fff:** reserved
- **7fb693000-7fb693fff:** reserved
- **7fb694000-7fb783fff:** reserved
- **7fb786000-7fb787fff:** reserved
- **7fb788000-7fb789fff:** reserved
- **7fb78a000-7fb79afff:** reserved
- **7fb79b000-7fb870fff:** reserved
- **7fb871000-7ffffffff:** reserved
- **980000000-9bfffffff:** pcie@fe170000
- **a40800000-a40bfffff:** fe170000.pcie pcie-dbi
#### IRQ

- **13:** GICv3 26 Level arch_timer
- **14:** GICv3 321 Level rk_timer
- **15:** GICv3 23 Level arm-pmu
- **16:** GICv3 105 Level dmc
- **17:** GICv3 126 Level fb000000.gpu
- **18:** GICv3 125 Level fb000000.gpu
- **19:** GICv3 124 Level fb000000.gpu
- **20:** GICv3 247 Level ehci_hcd:usb1
- **21:** GICv3 248 Level ohci_hcd:usb3
- **22:** GICv3 250 Level ehci_hcd:usb2
- **23:** GICv3 251 Level ohci_hcd:usb4
- **24:** GICv3 425 Level rockchip_usb2phy
- **25:** GICv3 423 Level rockchip_usb2phy
- **26:** GICv3 424 Level rockchip_usb2phy
- **27:** GICv3 349 Level fd880000.i2c
- **29:** GICv3 142 Level fdab9000.iommu, fdab0000.npu
- **30:** GICv3 143 Level fdab9000.iommu, fdab0000.npu
- **31:** GICv3 144 Level fdab9000.iommu, fdab0000.npu
- **32:** GICv3 152 Level fdb50000.vepu
- **33:** GICv3 151 Level fdb51000.avsd-plus, fdb50400.vdpu
- **34:** GICv3 150 Level fdb50800.iommu
- **35:** GICv3 146 Level fdb60f00.iommu, rga3_core0
- **36:** GICv3 147 Level fdb70f00.iommu, rga3_core1
- **37:** GICv3 148 Level rga2
- **38:** GICv3 161 Level fdb90000.jpegd
- **39:** GICv3 162 Level fdb90480.iommu
- **40:** GICv3 154 Level fdba0000.jpege-core
- **41:** GICv3 153 Level fdba0800.iommu
- **42:** GICv3 156 Level fdba4000.jpege-core
- **43:** GICv3 155 Level fdba4800.iommu
- **44:** GICv3 158 Level fdba8000.jpege-core
- **45:** GICv3 157 Level fdba8800.iommu
- **46:** GICv3 160 Level fdbac000.jpege-core
- **47:** GICv3 159 Level fdbac800.iommu
- **48:** GICv3 149 Level fdbb0800.iommu, fdbb0000.iep
- **49:** GICv3 133 Level fdbd0000.rkvenc-core
- **50:** GICv3 131 Level fdbdf000.iommu
- **51:** GICv3 132 Level fdbdf000.iommu
- **52:** GICv3 136 Level fdbe0000.rkvenc-core
- **53:** GICv3 134 Level fdbef000.iommu
- **54:** GICv3 135 Level fdbef000.iommu
- **55:** GICv3 127 Level fdc38100.rkvdec-core
- **56:** GICv3 128 Level fdc38700.iommu
- **57:** GICv3 129 Level fdc48100.rkvdec-core
- **58:** GICv3 130 Level fdc48700.iommu
- **59:** GICv3 141 Level fdca0000.iommu
- **60:** GICv3 175 Level rockchip-mipi-csi2-hw
- **61:** GICv3 176 Level rockchip-mipi-csi2-hw
- **62:** GICv3 177 Level rockchip-mipi-csi2-hw
- **63:** GICv3 178 Level rockchip-mipi-csi2-hw
- **64:** GICv3 179 Level rockchip-mipi-csi2-hw
- **65:** GICv3 180 Level rockchip-mipi-csi2-hw
- **66:** GICv3 181 Level rockchip-mipi-csi2-hw
- **67:** GICv3 182 Level rockchip-mipi-csi2-hw
- **68:** GICv3 183 Level rockchip-mipi-csi2-hw
- **69:** GICv3 184 Level rockchip-mipi-csi2-hw
- **70:** GICv3 185 Level rockchip-mipi-csi2-hw
- **71:** GICv3 186 Level rockchip-mipi-csi2-hw
- **72:** GICv3 188 Level fdd97e00.iommu, fdd90000.vop
- **73:** GICv3 217 Level i2s
- **74:** GICv3 193 Level fde50000.dp
- **75:** GICv3 201 Level fde80000.hdmi
- **77:** GICv3 203 Level fde80000.hdmi
- **78:** GICv3 204 Level fde80000.hdmi
- **79:** GICv3 392 Level dw-hdmi-qp-hpd
- **80:** GICv3 305 Level ahci[fe210000.sata]
- **81:** GICv3 236 Level dw-mci
- **82:** GICv3 237 Level mmc0
- **84:** GICv3 212 Level i2s
- **85:** GICv3 118 Level fea10000.dma-controller
- **86:** GICv3 119 Level fea10000.dma-controller
- **87:** GICv3 120 Level fea30000.dma-controller
- **88:** GICv3 121 Level fea30000.dma-controller
- **89:** GICv3 373 Level can0
- **90:** GICv3 374 Level can1
- **91:** GICv3 350 Level fea90000.i2c
- **92:** GICv3 354 Level fead0000.i2c
- **93:** GICv3 347 Edge feaf0000.watchdog
- **94:** GICv3 359 Level feb10000.spi
- **95:** GICv3 360 Level feb20000.spi
- **101:** GICv3 429 Level rockchip_thermal
- **102:** GICv3 430 Level fec10000.saradc
- **103:** GICv3 355 Level fec80000.i2c
- **104:** GICv3 356 Level fec90000.i2c
- **105:** GICv3 122 Level fed10000.dma-controller
- **106:** GICv3 123 Level fed10000.dma-controller
- **112:** GICv3 426 Level rockchip_usb2phy
- **113:** GICv3 218 Level i2s
- **114:** GICv3 219 Level i2s
- **115:** GICv3 275 Level pcie-sys
- **120:** GICv3 455 Edge debug-signal
- **121:** GICv3 365 Level debug
- **122:** GICv3 140 Level av1d-master
- **126:** rockchip_gpio_irq 7 Level rk806
- **129:** rk806 7 Level rk806_vb_low
- **130:** GICv3 252 Level dwc3
- **131:** GICv3 253 Level xhci-hcd:usb5
- **132:** rockchip_gpio_irq 10 Edge remotectl
- **142:** ITS-MSI 285212680 Edge PCIe PME
- **143:** rockchip_gpio_irq 26 Edge headset_detect
- **144:** rockchip_gpio_irq 0 Edge bt_default_wake_host_irq
- **145:** rockchip_gpio_irq 27 Edge dw-dp-hpd
- **146:** ITS-MSI 285736960 Edge enP2p33s0
- **IPI0:** Rescheduling interrupts
- **IPI1:** Function call interrupts
- **IPI2:** CPU stop interrupts
- **IPI3:** CPU stop (for crash dump) interrupts
- **IPI4:** Timer broadcast interrupts
- **IPI5:** IRQ work interrupts
- **IPI6:** CPU wake-up interrupts
- **Err:**

## Network

### Interfaces

#### Network Interfaces

- **lo:** 127.0.0.1|47187.08MiB|47187.08MiB
- **can0:** |0.00MiB|0.00MiB
- **can1:** |0.00MiB|0.00MiB
- **enP2p33s0:** |0.00MiB|0.00MiB
- **wlan0:** 192.168.1.8|661.15MiB|1527.60MiB

### IP Connections

#### Connections

- **127.0.0.1:38133:** LISTEN | 0.0.0.0:* | tcp
- **0.0.0.0:21:** LISTEN | 0.0.0.0:* | tcp
- **0.0.0.0:22:** LISTEN | 0.0.0.0:* | tcp
- **0.0.0.0:631:** LISTEN | 0.0.0.0:* | tcp
- **0.0.0.0:8096:** LISTEN | 0.0.0.0:* | tcp
- **192.168.1.8:50074:** ESTABLISHED | 140.82.113.26:443 | tcp
- **192.168.1.8:36712:** ESTABLISHED | 140.82.114.26:443 | tcp
- **192.168.1.8:38274:** ESTABLISHED | 146.0.41.231:443 | tcp
- **127.0.0.1:49988:** CLOSE_WAIT | 127.0.0.1:631 | tcp
- **:::5555:** LISTEN | :::* | tcp6
- **:::21:** LISTEN | :::* | tcp6
- **:::22:** LISTEN | :::* | tcp6
- **:::631:** LISTEN | :::* | tcp6
- **2804:214:4013:844:35174:** ESTABLISHED | 2a06:98c1:3107::681:443 | tcp6
- **2804:214:4013:844:44986:** ESTABLISHED | 2800:3f0:4001:836:::443 | tcp6
- **2804:214:4013:844:42060:** ESTABLISHED | 2a06:98c1:3107::681:443 | tcp6
- **2804:214:4013:844:48860:** ESTABLISHED | 2a06:98c1:3108::ac4:443 | tcp6
- **0.0.0.0:68:** 0.0.0.0:* | udp
- **192.168.1.8:123:** 0.0.0.0:* | udp
- **127.0.0.1:123:** 0.0.0.0:* | udp
- **0.0.0.0:123:** 0.0.0.0:* | udp
- **0.0.0.0:500:** 0.0.0.0:* | udp
- **0.0.0.0:34302:** 0.0.0.0:* | udp
- **0.0.0.0:1701:** 0.0.0.0:* | udp
- **0.0.0.0:4500:** 0.0.0.0:* | udp
- **0.0.0.0:5353:** 0.0.0.0:* | udp
- **0.0.0.0:7359:** 0.0.0.0:* | udp
- **fe80::2c1a:54b6:d89:123:** :::* | udp6
- **2804:214:4013:8446::123:** :::* | udp6
- **2804:214:4013:8446::123:** :::* | udp6
- **::1:123:** :::* | udp6
- **:::123:** :::* | udp6
- **:::500:** :::* | udp6
- **:::546:** :::* | udp6
- **2804:214:4013:844:49766:** ESTABLISHED | 2800:3f0:4001:836:::443 | udp6
- **:::4500:** :::* | udp6
- **:::54197:** :::* | udp6
- **:::5353:** :::* | udp6
- **2804:214:4013:844:55352:** ESTABLISHED | 2a06:98c1:3106::681:443 | udp6

### Routing Table

#### IP routing table

- **0.0.0.0 / 192.168.1.1:** 0.0.0.0 | UG | wlan0
- **192.168.1.0 / 0.0.0.0:** 255.255.255.0 | U | wlan0

### ARP Table

#### ARP Table

- **192.168.1.1:** e4:c0:e2:ec:3e:f7 | wlan0

### DNS Servers

#### Name Servers

- **8.8.8.8:** dns.google
- **1.1.1.1:** one.one.one.one

### Statistics

#### IP

- **:** Forwarding: 2
- **:** 547980 total packets received
- **:** 12 with invalid addresses
- **:** 0 forwarded
- **:** 0 incoming packets discarded
- **:** 546504 incoming packets delivered
- **:** 227013 requests sent out
- **:** 2484 outgoing packets dropped
- **:** 187 dropped because of missing route
#### ICMP

- **:** 1 ICMP messages received
- **:** 0 input ICMP message failed
- **:** ICMP input histogram:
- **:** echo replies: 1
- **:** 177 ICMP messages sent
- **:** 0 ICMP messages failed
- **:** ICMP output histogram:
- **:** destination unreachable: 176
- **:** echo requests: 1
#### ICMPMSG

- **:** InType0: 1
- **:** OutType3: 176
- **:** OutType8: 1
#### TCP

- **:** 703 active connection openings
- **:** 4 passive connection openings
- **:** 1 failed connection attempts
- **:** 77 connection resets received
- **:** 7 connections established
- **:** 2507077 segments received
- **:** 2229634 segments sent out
- **:** 280 segments retransmitted
- **:** 32 bad segments received
- **:** 11011 resets sent
- **:** InCsumErrors: 13
#### UDP

- **:** 11435 packets received
- **:** 176 packets to unknown port received
- **:** 0 packet receive errors
- **:** 5751 packets sent
- **:** 0 receive buffer errors
- **:** 0 send buffer errors
- **:** IgnoredMulti: 4790
#### UDPLITE

#### TCPEXT

- **:** 145 TCP sockets finished time wait in fast timer
- **:** 1576 delayed acks sent
- **:** Quick ack mode was activated 741 times
- **:** 1801026 packet headers predicted
- **:** 268320 acknowledgments not containing data payload received
- **:** 144416 predicted acknowledgments
- **:** TCPSackRecovery: 9
- **:** Detected reordering 1069 times using SACK
- **:** Detected reordering 5 times using time stamp
- **:** 5 congestion windows partially recovered using Hoe heuristic
- **:** 6 congestion windows recovered without slow start after partial ack
- **:** TCPLostRetransmit: 88
- **:** 5 fast retransmits
- **:** 3 retransmits in slow start
- **:** TCPTimeouts: 383
- **:** TCPLossProbes: 151
- **:** TCPBacklogCoalesce: 170887
- **:** TCPDSACKOldSent: 741
- **:** TCPDSACKOfoSent: 1
- **:** TCPDSACKRecv: 133
- **:** 261 connections reset due to unexpected data
- **:** 60 connections reset due to early user close
- **:** 35 connections aborted due to timeout
- **:** TCPDSACKIgnoredNoUndo: 64
- **:** TCPSpuriousRTOs: 3
- **:** TCPSackShiftFallback: 997
- **:** TCPRcvCoalesce: 210048
- **:** TCPOFOQueue: 59028
- **:** TCPOFOMerge: 1
- **:** TCPChallengeACK: 14
- **:** TCPSYNChallenge: 19
- **:** TCPSpuriousRtxHostQueues: 97
- **:** TCPAutoCorking: 919
- **:** TCPWantZeroWindowAdv: 25059
- **:** TCPSynRetrans: 14
- **:** TCPOrigDataSent: 1401064
- **:** TCPHystartTrainDetect: 1
- **:** TCPHystartTrainCwnd: 16
- **:** TCPHystartDelayDetect: 32
- **:** TCPHystartDelayCwnd: 3291
- **:** TCPACKSkippedSeq: 23
- **:** TCPACKSkippedChallenge: 5
- **:** TCPWinProbe: 1
- **:** TCPKeepAlive: 1920
- **:** TCPDelivered: 1399536
- **:** TCPAckCompressed: 51771
- **:** TcpTimeoutRehash: 352
- **:** TcpDuplicateDataRehash: 36
- **:** TCPDSACKRecvSegs: 133
#### IPEXT

- **:** InTruncatedPkts: 1
- **:** InMcastPkts: 243
- **:** OutMcastPkts: 124
- **:** InBcastPkts: 4790
- **:** OutBcastPkts: 3
- **:** InOctets: 758137089
- **:** OutOctets: 108976539
- **:** InMcastOctets: 37229
- **:** OutMcastOctets: 16708
- **:** InBcastOctets: 405474
- **:** OutBcastOctets: 234
- **:** InNoECTPkts: 547980

### Shared Directories

#### SAMBA

- **printers:** /var/spool/samba
- **print$:** /var/lib/samba/printers
#### NFS

- **No NFS exports:**

## Benchmarks

### CPU Blowfish (Single-thread)

#### CPU Blowfish (Single-thread)

- **Intel Core i7-4765T:** 8.40|8x 2001.00 MHz
- **AMD A8-5500:** 8.32|4x 3200,00 MHz
- **Intel Xeon E5-2660 0 (Dual):** 8.18|32x 2200.00 MHz
- **Intel Xeon Bronze 3204:** 8.11|6x 1900,00 MHz
- **Intel Core i7-1060NG7:** 8.09|8x 3800,00 MHz
- **AMD Athlon II X4 630:** 8.05|4x 2800.00 MHz
- **Intel Core i3-3120M:** 7.94|4x 2500.00 MHz
- **AMD A10-9600P:** 7.87|4x 2400.00 MHz
- **Intel Core i3-4100M:** 7.87|4x 2500.00 MHz
- **AMD Ryzen 3 3200U:** 7.73|4x 2600.00 MHz
- **Intel Celeron 3865U:** 7.64|2x 1800.00 MHz
- **Intel Pentium Dual E2220:** 7.61|2x 2403.00 MHz
- Rockchip RK3588 | 7.53|4x 2256.00 MHz + 4x 1800.00 MHz
- **Intel Core 2 Duo T8300:** 7.46|2x 2400.00 MHz
- **Intel Core 2 Duo T9500:** 7.40|2x 2601.00 MHz
- **Intel Xeon E5420:** 7.33|1x 2493.97 MHz + 1x 2493.86 MHz + 1x 2493.71 MHz + 1x 2493.71 MHz
- **Intel Core 2 Duo T9400:** 7.24|2x 2534.00 MHz
- **Intel Core i7 Q 720:** 7.18|8x 1600,00 MHz
- **Intel Core 2 6400:** 7.07|2x 2133.00 MHz
- **Intel Pentium G630T:** 7.04|2x 2300.00 MHz
- **Intel Celeron G3900T:** 6.96|2x 2600,00 MHz
- **AMD 3015e:** 6.93|4x 1200.00 MHz
- **Loongson-3A6000-HV:** 6.86|1x 2600.00 MHz
- **Intel Core 2 T7400:** 6.85|2x 2167.00 MHz
- **Pentium T4500:** 6.84|2x 2300.00 MHz

### CPU Blowfish (Multi-thread)

#### CPU Blowfish (Multi-thread)

- **Intel Core i3-7300:** 4x 4000,00 MHz | 55.07
- **AMD FX-8300:** 8x 3300.00 MHz | 54.38
- **Intel Xeon E3-1225 v5:** 4x 3700,00 MHz | 54.04
- **Intel Core i7-2820QM:** 8x 3400,00 MHz | 53.49
- **Intel Core i7 860:** 8x 2800.00 MHz | 51.73
- **Intel Pentium Gold G5420:** 4x 3800.00 MHz | 51.50
- **Intel Core i5-6500TE:** 4x 3300.00 MHz | 49.99
- **Intel Core i3-7100:** 4x 3900.00 MHz | 48.07
- **Intel Core i3-6100:** 4x 3700.00 MHz | 47.99
- **Intel Core i7-4700HQ:** 8x 3400.00 MHz | 46.39
- **AMD FX-6300:** 6x 3500.00 MHz | 46.37
- **AMD Phenom II X6 1055T:** 6x 2800.00 MHz | 45.88
- Rockchip RK3588 | 4x 2256.00 MHz + 4x 1800.00 MHz | 44.97
- **Intel Core i3-7100T:** 4x 3400.00 MHz | 43.27
- **Intel Xeon E5335 (Dual):** 8x 2000,00 MHz | 42.96
- **Intel Core i7-2635QM:** 4x 2000.00 MHz | 42.60
- **Intel Pentium Gold G5400T:** 4x 3100.00 MHz | 42.54
- **Intel Core i5-3570:** 4x 3800.00 MHz | 42.32
- **Intel Core i5-4690:** 4x 3900.00 MHz | 42.06
- **Intel Core i7-7600U:** 4x 3900.00 MHz | 41.66
- **Intel Core i5-4590:** 4x 3700.00 MHz | 41.37
- **Intel Core i7-6650U:** 4x 3400,00 MHz | 40.75
- **Intel Xeon E3-1226 v3:** 4x 3700.00 MHz | 40.53
- **Intel Core i5-7300U:** 4x 3500.00 MHz | 40.01
- **AMD A10-5800K:** 4x 3800.00 MHz | 39.78

### CPU Blowfish (Multi-core)

#### CPU Blowfish (Multi-core)

- **AMD Ryzen 3 4100:** 8x 4105.00 MHz | 52.78
- **Intel Core i7 6770HQ:** 8x 3500.00 MHz | 50.80
- **Intel Core i7-1185G7:** 8x 4800.00 MHz | 50.62
- **Intel Xeon E3-1276 v3:** 8x 4000.00 MHz | 49.29
- **Intel Core i7-4790:** 8x 4000.00 MHz | 47.19
- **Intel Xeon E3-1241 v3:** 8x 3900.00 MHz | 46.11
- **Intel Xeon E5-2637 v2:** 8x 3800,00 MHz | 45.82
- **Intel Xeon E3-1270 v3:** 8x 3900.00 MHz | 45.80
- **Intel Core i5-7300HQ:** 4x 3500.00 MHz | 45.63
- **Intel Core i5-4670:** 4x 3800.00 MHz | 45.39
- **AMD Phenom II X6 1055T:** 6x 2800.00 MHz | 45.17
- **Intel Xeon E3-1230 V2:** 8x 3700.00 MHz | 44.59
- Rockchip RK3588 | 4x 2256.00 MHz + 4x 1800.00 MHz | 44.53
- **Intel Xeon E5-1607 v4:** 4x 3100.00 MHz | 42.16
- **AMD Athlon X4 845:** 4x 3500,00 MHz | 41.89
- **AMD Ryzen 5 PRO 2400G:** 8x 3600,00 MHz | 41.59
- **Intel Xeon E5-2620 v2:** 12x 2600.00 MHz | 40.87
- **Intel Core i7-3720QM:** 8x 3600.00 MHz | 40.59
- **Intel Core i7 860:** 8x 2800.00 MHz | 38.66
- **Intel Core i7-3615QM:** 8x 3300.00 MHz | 38.62
- **Intel Core i7-4800MQ:** 8x 2700.00 MHz | 38.38
- **AMD A10-7870K:** 4x 3900.00 MHz | 36.97
- **Intel Core i7-4770HQ:** 8x 3400.00 MHz | 35.48
- **AMD FX-4300:** 4x 3800,00 MHz | 34.99
- **AMD A8-5600K:** 4x 3600.00 MHz | 34.69

### CPU Zlib

#### CPU Zlib

- **Intel Core i5-1345U:** 4x 4700,00 MHz + 8x 3500,00 MHz | 62.11
- **Intel Core i7-4910MQ:** 8x 3900.00 MHz | 61.09
- **AMD Ryzen 7 3750H:** 8x 2300.00 MHz | 60.84
- **Intel Xeon E5-2620 0:** 12x 2500.00 MHz | 60.21
- **Intel Core i7-11390H:** 4x 5000.00 MHz + 4x 4800.00 MHz | 60.04
- **AMD Ryzen 5 PRO 2400GE:** 8x 2300.00 MHz | 59.41
- **Intel Core i7 6770HQ:** 8x 3500.00 MHz | 59.18
- **AMD Ryzen 5 PRO 2500U:** 8x 2000.00 MHz | 59.18
- **Intel Core i5-2550K:** 4x 5900.00 MHz | 57.76
- **Intel Xeon E31280:** 8x 3900,00 MHz | 55.76
- **Intel Core i7-6700HQ:** 8x 3500.00 MHz | 55.34
- **AMD Phenom II X6 1090T:** 6x 3200.00 MHz | 54.36
- Rockchip RK3588 | 4x 2256.00 MHz + 4x 1800.00 MHz | 53.98
- **Intel Xeon E5410 (Dual):** 8x 2333,00 MHz | 53.28
- **Intel 300:** 4x 5000.00 MHz | 52.19
- **Loongson-3A6000:** 1x 2500.00 MHz | 51.17
- **Intel Core i5-4590:** 4x 3700.00 MHz | 50.15
- **Intel Xeon X5679:** 12x 3200,00 MHz | 49.88
- **Intel Core i5-8279U:** 4x 4100.00 MHz | 48.52
- **Intel Core i7-3612QM:** 8x 3100.00 MHz | 48.02
- **Intel Core i5-3550:** 4x 3700.00 MHz | 46.67
- **Intel Core i5-4670S:** 4x 3800.00 MHz | 46.34
- **Intel Xeon W3550:** 8x 3060,00 MHz | 44.98
- **Intel Core i5-6600T:** 4x 3500.00 MHz | 44.74
- **AMD Phenom II X6 1045T:** 6x 2700.00 MHz | 44.33

### CPU CryptoHash

#### CPU CryptoHash

- **AMD FX-8320E:** 8x 3800,00 MHz | 84.11
- **Intel Core i5-9600K:** 6x 4600.00 MHz | 82.79
- **AMD Ryzen 9 3900:** 24x 4359.00 MHz | 81.75
- **Intel Xeon E3-1275 v3:** 8x 3900.00 MHz | 80.92
- **AMD Ryzen 5 3600:** 12x 3600.00 MHz | 78.31
- **Intel Xeon E5-2690 0:** 16x 2900.00 MHz | 77.25
- **AMD Ryzen 5 5625U:** 12x 2300.00 MHz | 76.47
- **Intel Xeon E5-2678 v3:** 24x 3300,00 MHz | 75.66
- **AMD FX-8350:** 8x 4000.00 MHz | 74.55
- **Intel Core i5-10600K:** 12x 4900.00 MHz | 73.36
- **AMD Ryzen 5 5500GT:** 12x 4457.00 MHz | 72.11
- **Intel Core i9-10900E:** 20x 4700,00 MHz | 71.40
- Rockchip RK3588 | 4x 2256.00 MHz + 4x 1800.00 MHz | 70.00
- **Intel Core i5-11500:** 12x 4600,00 MHz | 69.58
- **Intel Core i7-14650HX:** 4x 5200.00 MHz + 12x 5000.00 MHz + 8x 3700.00 MHz | 69.03
- **AMD Ryzen 7 PRO 5850U:** 16x 4507.00 MHz | 67.98
- **AMD Ryzen 5 5500U:** 12x 4056.00 MHz | 67.50
- **Intel Core i9-9820X:** 4x 4200,00 MHz + 16x 4100,00 MHz | 67.30
- **Qualcomm Snapdragon X1E-78-100:** 12x 3417.00 MHz | 66.68
- **Intel Xeon D-1521:** 8x 2401.00 MHz | 66.60
- **Intel Core i7-8850H:** 12x 4300.00 MHz | 64.78
- **AMD Ryzen 5 3600X:** 12x 3800.00 MHz | 64.56
- **Apple T6001 (M1 Max):** 8x 3036.00 MHz + 2x 2064.00 MHz | 63.70
- **Intel Core i7 975:** 8x 3334,00 MHz | 63.16
- **Intel Core i7 880:** 8x 3068.00 MHz | 63.00

### CPU Fibonacci

#### CPU Fibonacci

- **Intel Celeron J4125:** 4x 2700.00 MHz | 701.42
- **AMD A8-6600K:** 4x 3900,00 MHz | 691.30
- **AMD A12-9720P:** 4x 2700,00 MHz | 682.67
- **Intel Core i5-4210M:** 4x 3200.00 MHz | 672.10
- **Intel Xeon 5130 (Dual):** 1x 1995.11 MHz + 1x 1995.09 MHz + 1x 1995.08 MHz + 1x 1993.38 MHz | 668.57
- **Intel Core i7 Q 740:** 8x 1734.00 MHz | 666.54
- **Intel Core i7 Q 840:** 8x 1867,00 MHz | 650.54
- **AMD FX-8320E:** 8x 3800,00 MHz | 645.37
- **Intel Core i3-4360:** 4x 3700.00 MHz | 599.81
- **Intel Xeon L5640:** 12x 2266.75 MHz | 577.50
- **Intel Core i5-5200U:** 4x 2700.00 MHz | 542.83
- **Intel Core i5-2415M:** 4x 2900.00 MHz | 540.59
- Rockchip RK3588 | 4x 2256.00 MHz + 4x 1800.00 MHz | 540.03
- **AMD Athlon II X2 255:** 2x 3100,00 MHz | 514.56
- **AMD A10-9620P:** 4x 2500,00 MHz | 493.97
- **Intel Pentium 3805U:** 2x 1900.00 MHz | 486.75
- **Intel Core i3 550:** 4x 3200.00 MHz | 486.29
- **Intel Core i5 M 450:** 4x 2400.00 MHz | 469.95
- **AMD A6-9500:** 2x 3500.00 MHz | 461.64
- **Intel Pentium 4417U:** 4x 2300.00 MHz | 460.80
- **Intel Pentium G6950:** 2x 2800.00 MHz | 454.91
- **Intel Celeron 1037U:** 2x 1800.00 MHz | 449.90
- **AMD Athlon 7750:** 2x 2700.00 MHz | 448.45
- **Intel Core i5-4250U:** 4x 1901.00 MHz | 441.62
- **Intel Core i7 M 640:** 4x 2800.00 MHz | 437.16

### CPU N-Queens

#### CPU N-Queens

- **Intel Pentium J2900:** 4x 2665.00 MHz | 520.92
- **Intel Pentium N3540:** 4x 2665.00 MHz | 506.52
- **Intel Core i3-2120T:** 4x 2600,00 MHz | 499.52
- **AMD 3015e:** 4x 1200.00 MHz | 496.92
- **Intel Core i5-4330M:** 4x 3500.00 MHz | 479.34
- **Intel Core i5-5350U:** 4x 2900.00 MHz | 473.25
- **AMD A6-3600:** 4x 2100.00 MHz | 462.88
- **AMD A10-5700:** 4x 3400,00 MHz | 454.76
- **Intel Celeron G5905T:** 2x 3300.00 MHz | 453.08
- **Intel Celeron J4125:** 4x 2700.00 MHz | 449.54
- **Intel Core i7-3537U:** 4x 3100.00 MHz | 447.21
- **Intel Core i3-3220T:** 4x 2800.00 MHz | 446.80
- Rockchip RK3588 | 4x 2256.00 MHz + 4x 1800.00 MHz | 446.52
- **Intel Core i5-4300U:** 4x 2900.00 MHz | 444.95
- **Intel Pentium Gold G7400:** 4x 3700.00 MHz | 439.28
- **Intel Core i3-3120M:** 4x 2500.00 MHz | 438.53
- **Intel Core i5-7440HQ:** 4x 2801,00 MHz | 438.41
- **Intel Core m7-6Y75:** 4x 3100,00 MHz | 436.36
- **Intel Pentium G3240:** 2x 3100.00 MHz | 435.69
- **Intel Core i5-3210M:** 4x 3100.00 MHz | 431.56
- **Intel Pentium G3258:** 2x 3200,00 MHz | 422.81
- **AMD PRO A8-8600B R6:** 4x 1600.00 MHz | 418.71
- **Intel Pentium G2130:** 2x 3200.00 MHz | 411.36
- **Intel Celeron N2920:** 4x 1999,00 MHz | 400.60
- **Intel Core i5-4570R:** 4x 3200.00 MHz | 399.24

### FPU FFT

#### FPU FFT

- **Intel Xeon X5670:** 12x 2927.00 MHz | 110.13
- **Intel Core i5-6440HQ:** 4x 3500.00 MHz | 109.88
- **Intel Xeon L5630 (Dual):** 1x 2394.28 MHz + 1x 2394.10 MHz + 13x 2128.09 MHz + 1x 1912.40 MHz | 108.26
- **Intel Xeon E3-1275 V2:** 8x 3900,00 MHz | 107.63
- **AMD Ryzen 7 3700U:** 8x 2300.00 MHz | 107.31
- **Intel Core i5-3340:** 4x 3300.00 MHz | 105.74
- **Intel Core i7-3840QM:** 4x 3800.00 MHz | 105.37
- **Intel Core i5-6400T:** 4x 2800.00 MHz | 105.17
- **AMD FX-8300:** 8x 3300.00 MHz | 102.62
- **Intel Xeon E31280:** 8x 3900,00 MHz | 102.10
- **Intel Xeon E5345 (Dual):** 8x 2333.00 MHz | 101.85
- **Intel Xeon X5650:** 12x 2661.00 MHz | 100.47
- Rockchip RK3588 | 4x 2256.00 MHz + 4x 1800.00 MHz | 100.00
- **Intel Core i5-1035G1:** 8x 3600.00 MHz | 97.62
- **Intel Core i7-3615QM:** 8x 3300.00 MHz | 96.38
- **Intel Core i5-8250U:** 8x 3400.00 MHz | 91.19
- **AMD Athlon X4 860K:** 4x 3700.00 MHz | 89.29
- **AMD Phenom II X6 1090T:** 6x 3200.00 MHz | 88.50
- **Intel Core i7-4960HQ:** 8x 3800.00 MHz | 84.18
- **Intel Core i7 975:** 8x 3334,00 MHz | 82.63
- **Intel Core i7 960:** 8x 3193.00 MHz | 81.62
- **Intel Pentium Gold G6400:** 4x 4000.00 MHz | 79.43
- **AMD Ryzen 5 3450U:** 8x 2100.00 MHz | 77.96
- **Intel Core i7-3820QM:** 8x 3700,00 MHz | 76.97
- **AMD A10-5800K:** 4x 3800.00 MHz | 74.81

### FPU Raytracing (Single-thread)

#### FPU Raytracing (Single-thread)

- **AMD A10-5700:** 4x 3400,00 MHz | 1156.45
- **Intel Core i3-2120T:** 4x 2600,00 MHz | 1154.50
- **Intel Core i3-3110M:** 4x 2400.00 MHz | 1150.09
- **Intel Celeron 3215U:** 2x 1700.00 MHz | 1136.10
- **AMD Phenom II X6 1090T:** 6x 3200.00 MHz | 1130.41
- **Intel Xeon X5675 (Dual):** 24x 3060.00 MHz | 1121.86
- **AMD Opteron X3421:** 4x 2100.00 MHz | 1118.20
- **Intel Celeron G1620:** 2x 2700,00 MHz | 1106.15
- **Intel Xeon X3430:** 4x 2401.00 MHz | 1102.80
- **Intel Pentium Silver N5030:** 4x 3100.00 MHz | 1097.13
- **Intel Xeon E5620 (Dual):** 16x 2527.00 MHz | 1089.37
- **AMD A10 PRO-7800B R7:** 4x 3500.00 MHz | 1075.76
- Rockchip RK3588 | 4x 2256.00 MHz + 4x 1800.00 MHz | 1071.50
- **Intel Xeon E5-2630 0:** 12x 2800.00 MHz | 1061.40
- **AMD FX-6200:** 6x 3800,00 MHz | 1048.20
- **AMD A10-9600P:** 4x 2400.00 MHz | 1040.58
- **AMD Athlon II X2 270:** 2x 3400,00 MHz | 1019.92
- **Intel Xeon E5-2651 v2 (Dual):** 48x 2200.00 MHz | 1016.40
- **Intel Core i7 930:** 8x 2801,00 MHz | 1009.40
- **AMD Athlon II X2 280:** 2x 3607,00 MHz | 1002.20
- **Intel Core i3-2348M:** 4x 2300,00 MHz | 984.26
- **Intel Celeron N4120:** 4x 2600.00 MHz | 981.10
- **Intel Celeron 1005M:** 2x 1900,00 MHz | 933.56
- **Intel Core i7 Q 840:** 8x 1867,00 MHz | 927.30
- **AMD Athlon II X2 255:** 2x 3100,00 MHz | 923.50

### Internal Network Speed

#### Internal Network Speed

- **AMD Ryzen 7 7435HS:** 16x 4553,00 MHz | 57.28
- **Intel Xeon w3-2425:** 4x 4400.00 MHz + 4x 4300.00 MHz + 4x 4200.00 MHz | 57.21
- **Intel Core i3-8350K:** 4x 4000.00 MHz | 56.56
- **Intel Core i9-11950H:** 4x 5000.00 MHz + 12x 4900.00 MHz | 56.28
- **Intel Xeon Gold 5122:** 8x 3700.00 MHz | 54.98
- **Intel Xeon E3-1230 v5:** 8x 3800.00 MHz | 53.89
- **Intel Core i7-7567U:** 4x 4000.00 MHz | 53.44
- **AMD Ryzen 9 5900X:** 24x 4951.00 MHz | 53.18
- **Intel Core i5-9300H:** 8x 4100.00 MHz | 52.51
- **Intel Core i9-8950HK:** 12x 4800.00 MHz | 52.31
- **Intel Celeron G6900T:** 2x 2800,00 MHz | 51.21
- **Intel Xeon Gold 5416S (Dual):** 64x 4000.00 MHz | 51.03
- Rockchip RK3588 | 4x 2256.00 MHz + 4x 1800.00 MHz | 50.89
- **Intel Core i5-11260H:** 12x 4400.00 MHz | 50.70
- **Intel Xeon E3-1240 v5:** 8x 3900,00 MHz | 50.55
- **AMD Ryzen 7 7735U:** 16x 4819.00 MHz | 50.50
- **INTEL XEON SILVER 4514Y (Dual):** 64x 3400,00 MHz | 50.41
- **AMD Ryzen 7 PRO 6850H:** 16x 4787.00 MHz | 50.33
- **Intel Core i7-10750H:** 12x 5000.00 MHz | 49.05
- **AMD Ryzen 9 6900HX:** 16x 4935.00 MHz | 48.97
- **Intel Xeon E5-2650 v3 (Dual):** 40x 3000,00 MHz | 48.61
- **Intel Core i7-8705G:** 8x 4100.00 MHz | 48.56
- **Intel Core i5-4690K:** 4x 3500.00 MHz | 47.88
- **AMD Ryzen 9 5900XT:** 32x 4980.00 MHz | 47.86
- **AMD Ryzen 5 7535HS:** 12x 4603.00 MHz | 47.42

### SysBench CPU (Single-thread)

#### SysBench CPU (Single-thread)

- **Intel Core i7-1185G7:** 8x 4800.00 MHz | 3214.93
- **Intel Core i7-13700T:** 4x 4900,00 MHz + 12x 4800,00 MHz + 8x 3600,00 MHz | 3172.00
- **Intel N97:** 4x 3600,00 MHz | 3167.94
- **Intel Xeon Gold 5416S (Dual):** 64x 4000.00 MHz | 3167.89
- **Intel Core i5-13420H:** 8x 4600.00 MHz + 4x 3400.00 MHz | 3166.51
- **Intel Core i5-13500H:** 8x 4700.00 MHz + 8x 3500.00 MHz | 3165.21
- **Intel Core i7-1255U:** 4x 4700.00 MHz + 8x 3500.00 MHz | 3147.73
- **Intel Core i3-1315U:** 4x 4500,00 MHz + 4x 3300,00 MHz | 3105.68
- **Intel Core i5-1334U:** 4x 4600.00 MHz + 8x 3400.00 MHz | 3086.04
- **Intel Core i7-13800H:** 4x 5200,00 MHz + 8x 5000,00 MHz + 8x 4000,00 MHz | 2988.00
- **Intel Core i7-12650H:** 4x 4700.00 MHz + 8x 4600.00 MHz + 4x 3500.00 MHz | 2749.16
- **Intel Core i5-1035G4:** 8x 3700.00 MHz | 2636.77
- Rockchip RK3588 | 4x 2256.00 MHz + 4x 1800.00 MHz | 2578.71
- **Intel Core i5-1155G7:** 8x 4500.00 MHz | 2539.30
- **AMD Ryzen AI 9 H 365:** 20x 2000,00 MHz | 2267.00
- **AMD Ryzen 7 3800X:** 16x 3900.00 MHz | 2124.67
- **AMD Ryzen 9 4900H:** 16x 3300.00 MHz | 2103.00
- **AMD Ryzen 7 4700G:** 16x 3600,00 MHz | 1992.00
- **AMD Ryzen 5 3500:** 6x 4120.00 MHz | 1982.26
- **AMD Ryzen Threadripper 2950X:** 32x 3500.00 MHz | 1944.00
- **AMD Ryzen 5 PRO 4650GE:** 12x 3300,00 MHz | 1920.00
- **AMD Ryzen 5 3500X:** 6x 3600.00 MHz | 1871.71
- **AMD Ryzen 5 PRO 4650U:** 12x 2100.00 MHz | 1862.23
- **AMD Ryzen 5 PRO 2400G:** 8x 3600,00 MHz | 1853.50
- **AMD Ryzen 7 PRO 4750U:** 16x 1700.00 MHz | 1807.78

### SysBench CPU (Multi-thread)

#### SysBench CPU (Multi-thread)

- **AMD Ryzen 7 4800H:** 16x 2900.00 MHz | 16391.77
- **Intel Core i5-11260H:** 12x 4400.00 MHz | 16382.37
- **Intel Core i5-1345U:** 4x 4700,00 MHz + 8x 3500,00 MHz | 16336.25
- **AMD Ryzen Threadripper 1900X:** 16x 3800.00 MHz | 16305.39
- **Intel Core i3-1215UL:** 4x 4400.00 MHz + 4x 3300.00 MHz | 16129.26
- **Intel Xeon w3-2425:** 4x 4400.00 MHz + 4x 4300.00 MHz + 4x 4200.00 MHz | 15913.50
- **Intel Xeon E5-2680 v3:** 24x 2500.00 MHz | 15823.67
- **Intel Xeon E5-2673 v3:** 24x 2400.00 MHz | 15726.89
- **AMD Ryzen 7 PRO 1700X:** 16x 3400.00 MHz | 14483.86
- **Intel Xeon E5-2620 v3 (Dual):** 24x 3200.00 MHz | 14373.04
- **Intel Xeon E5-2630 v2 (Dual):** 24x 3100.00 MHz | 13940.68
- **Intel Core i5-10600KF:** 12x 4800.00 MHz | 13821.31
- Rockchip RK3588 | 4x 2256.00 MHz + 4x 1800.00 MHz | 13487.63
- **Intel Core i7-5960X:** 16x 4000.00 MHz | 13430.12
- **AMD Ryzen 5 PRO 4650G:** 12x 4308.00 MHz | 13326.52
- **AMD Ryzen 5 PRO 4650GE:** 12x 3300,00 MHz | 12109.00
- **AMD Ryzen 7 PRO 4750GE:** 16x 3100,00 MHz | 11591.00
- **Intel Xeon W-2235:** 12x 3800.00 MHz | 11587.74
- **Intel Xeon X5570 (Dual):** 16x 2927.00 MHz | 11406.38
- **AMD Ryzen 5 2600:** 2x 3900.00 MHz + 1x 3899.00 MHz + 2x 3898.00 MHz + 1x 3896.00 MHz + 1x 3894.00 MHz + 1x 3891.00 MHz + 1x 3884.00 MHz + 1x 3883.00 MHz + 1x 3860.00 MHz + 1x 3857.00 MHz | 11350.71
- **Intel Core i7-10750H:** 12x 5000.00 MHz | 11202.39
- **Intel Core i7-9700:** 8x 4700.00 MHz | 10715.85
- **Intel Core i3-1125G4:** 8x 3700.00 MHz | 10266.59
- **Qualcom SD-720G:** 2x 2323.00 MHz + 6x 1804.00 MHz | 10036.00
- **Intel N97:** 4x 3600,00 MHz | 9988.52

### SysBench Memory (Single-thread)

#### SysBench Memory (Single-thread)

- **Intel Core m5-6Y54:** 4x 2700.00 MHz | 3948.40
- **Intel Core i5-2450M:** 4x 2500.00 MHz | 3930.16
- **Intel Xeon E5-2648L v2:** 20x 2500.00 MHz | 3819.84
- **Intel Core i5 M 520:** 4x 2400.00 MHz | 3816.14
- **Intel Pentium G620:** 2x 2600.00 MHz | 3734.14
- **AMD FX-8320E:** 8x 3800,00 MHz | 3580.17
- **AMD Athlon II X2 280:** 2x 3607,00 MHz | 3575.00
- **Intel Core i3-5010U:** 4x 2000.00 MHz | 3519.48
- **Intel Xeon L5630 (Dual):** 16x 2134,00 MHz | 3499.00
- **Intel Core 2 Quad Q9300:** 4x 2500,00 MHz | 3444.00
- **AMD A6-9500:** 2x 3500.00 MHz | 3398.12
- **Intel Pentium E5200:** 2x 2495.00 MHz | 3397.45
- Rockchip RK3588 | 4x 2256.00 MHz + 4x 1800.00 MHz | 3388.11
- **RockChip RK3588 (Pi5B):** 4x 2400.00 MHz + 4x 1800.00 MHz | 3341.70
- **Intel Core i3 M 370:** 4x 2399.00 MHz | 3341.38
- **Intel Pentium 3805U:** 2x 1900.00 MHz | 3317.80
- **AMD A8-5500:** 4x 3200,00 MHz | 3307.00
- **Intel Core i7-7920HQ:** 8x 4100.00 MHz | 3298.01
- **AMD Ryzen 5 5500U:** 12x 4056.00 MHz | 3240.18
- **Intel Pentium 3825U:** 4x 1900,00 MHz | 3216.81
- **Intel Xeon E5606 (Dual):** 8x 2128.00 MHz | 3208.71
- **AMD Athlon II X4 630:** 4x 2800,00 MHz | 3092.00
- **Intel Celeron 3865U:** 2x 1800.00 MHz | 3033.78
- **Intel Pentium 3558U:** 2x 1700.00 MHz | 2992.53
- **AMD FX-8150:** 8x 3600,00 MHz | 2992.00

### SysBench Memory (Multi-thread)

#### SysBench Memory (Multi-thread)

- **Intel Core i7-4710MQ:** 8x 3500.00 MHz | 13142.21
- **Intel Core Ultra 5 225:** 6x 5000.00 MHz + 4x 4400.00 MHz | 13121.21
- **Intel Core i5-8600:** 6x 4300.00 MHz | 13055.82
- **Intel Core i7-6700HQ:** 8x 3500.00 MHz | 12869.22
- **Intel Xeon W-3223:** 4x 4200.00 MHz + 12x 4000.00 MHz | 12746.51
- **Intel Core i5-9400:** 6x 4100.00 MHz | 12697.92
- **AMD Ryzen 9 7900X:** 24x 5733.00 MHz | 12543.94
- **Intel Core i3-3225:** 4x 3300.00 MHz | 12523.33
- **Intel Core i5-3380M:** 4x 3600.00 MHz | 12426.10
- **Intel Xeon X5687 (Dual):** 1x 3592.96 MHz + 1x 3590.81 MHz + 1x 3590.79 MHz + 8x 3590.79 MHz + 1x 3590.77 MHz + 1x 3589.21 MHz + 1x 3451.98 MHz + 1x 3391.67 MHz + 1x 2999.73 MHz | 12340.74
- **AMD Ryzen Threadripper PRO 5995WX:** 128x 4573.00 MHz | 12319.45
- **Intel Core i5-1245U:** 4x 4400.00 MHz + 8x 3300.00 MHz | 12241.41
- Rockchip RK3588 | 4x 2256.00 MHz + 4x 1800.00 MHz | 12189.97
- **Intel Core i7-8650U:** 8x 1900.00 MHz | 12155.33
- **Intel Core i5-3570:** 4x 3800.00 MHz | 12006.74
- **Intel Xeon W-2133:** 12x 3900.00 MHz | 11827.11
- **Intel Core i3-4330:** 4x 3500.00 MHz | 11765.98
- **Intel Xeon E3-1225 V2:** 4x 3600.00 MHz | 11699.07
- **Intel Core i5-4278U:** 4x 3100.00 MHz | 11657.59
- **Intel Core i3-8130U:** 4x 3400.00 MHz | 11582.86
- **Intel Core i7-2670QM:** 8x 3100.00 MHz | 11230.42
- **Intel Core i7-4850HQ:** 8x 3500.00 MHz | 11155.72
- **Intel Core i5-4570:** 4x 3600.00 MHz | 11046.39
- **Intel Core i5-4690T:** 4x 3500,00 MHz | 10971.00
- **Intel Xeon X5677 (Dual):** 16x 3459.00 MHz | 10920.35

### GPU Drawing

#### GPU Drawing

- **NVIDIA GeForce GTX 950M:** 490.04
- **Intel HD 6000:** 488.86
- **AMD REDWOOD:** 486.50
- **NVIDIA G84M:** 486.23
- **Intel CoffeeLake-H GT2:** 484.86
- **Intel 82G965 Integrated:** 482.29
- **NVIDIA GeForce GT 630:** 481.27
- **NVIDIA GeForce MX150:** 480.00
- **NVIDIA GeForce 8400 GS Rev. 2:** 478.98
- **Glenfly Arise-GT10C0:** 475.50
- **NVIDIA RTX A6000:** 472.59
- **AMD Raven Ridge:** 472.36
- Integrated (Rockchip RK3588) | 466.97
- **Intel Arc A750:** 460.73
- **NVIDIA GeForce GTX 980:** 458.44
- **NVIDIA GeForce RTX 2080 Ti Rev. A:** 454.39
- **NVIDIA GeForce GTX TITAN X:** 452.45
- **AMD Radeon R7 360:** 447.74
- **Intel Arc B570:** 447.20
- **Intel Meteor Lake-P:** 445.66
- **NVIDIA GeForce 320M:** 443.68
- **Quadro M4000M:** 443.53
- **AMD Custom GPU 0932:** 443.06
- **Quadro K2000:** 441.24
- **NVIDIA GeForce GTX 750 v2:** 438.25

### GPU OpenGL Drawing

#### GPU OpenGL Drawing

- **ATI RV350:** 70.86
- **NVIDIA GeForce 7600 GT:** 70.25
- **NVIDIA Quadro FX 880M:** 64.58
- **NVIDIA UHD 710:** 63.75
- **AMD PALM:** 62.99
- **NVIDIA GeForce GTX 570 Rev. 2:** 62.57
- **ATI RS600:** 60.61
- **NVIDIA GeForce GTX 560 Ti:** 58.20
- **NVIDIA GeForce GTX 670M:** 57.71
- **NVIDIA Quadro NVS 290:** 55.51
- **NVIDIA Quadro FX 1800:** 53.89
- **NVIDIA GeForce Go 6600:** 52.64
- **NVIDIA GeForce 8600M GS:** 49.22
- **Intel 965GM:** 49.00
- **NVIDIA GeForce G102M:** 47.94
- **ATI RV370:** 46.92
- **NVIDIA GeForce 9300M GS:** 45.91
- **NVIDIA GeForce 8800M GTS:** 42.98
- **NVIDIA GeForce 9200M GS:** 41.33
- **NVIDIA GeForce 505:** 40.13
- **NVIDIA Quadro NVS 135M:** 34.80
- Integrated (Rockchip RK3588) | 32.81
- **NVIDIA Quadro NVS 160M:** 28.76
- **NVIDIA GeForce 6200:** 25.22
- **NVIDIA GeForce 7150M / nForce 630M:** 20.57

### GPU Vulkan Drawing

#### GPU Vulkan Drawing

- **AMD RS780:** 91.19
- **NVIDIA GeForce GT 630 Rev. 2:** 91.00
- **BONAIRE:** 86.92
- **GeForce 9800 GT:** 86.48
- **AMD RV610:** 85.56
- **NVIDIA GeForce 9200M GS:** 67.00
- **NVIDIA NVS 3100M:** 67.00
- **NVIDIA Quadro 2000:** 65.00
- **Intel G41:** 61.55
- **NVIDIA GeForce RTX 5080:** 60.52
- **NV86:** 58.00
- **NVIDIA GeForce 9300M GS:** 58.00
- Integrated (Rockchip RK3588) | 52.54
- **NVIDIA GeForce 8800 GS:** 51.49
- **NVIDIA GeForce 9400M:** 51.00
- **NVIDIA GeForce 9400:** 51.00
- **NVIDIA GeForce 9300 GE:** 47.00
- **Intel GM45:** 46.09
- **NVIDIA GeForce GT 220M:** 46.00
- **GeForce 8400M GS:** 44.00
- **NVIDIA GeForce GT 130:** 43.00
- **AMD BC-250:** 41.67
- **NVIDIA GeForce G102M:** 41.00
- **NVIDIA GeForce 7025 / nForce 630a:** 37.00
- **Intel G45/G43:** 35.19

### Storage R/W Speed

#### Storage R/W Speed

- **Lexar 120GB SSD:** 232.77
- **120GB SSD:** 230.45
- **TEAM TM4PS7512G:** 227.16
- **Samsung APPLE SSD SM256C:** 225.91
- **SSD 1T:** 225.42
- **Kingston SA400M8240G:** 223.67
- **R5SL120G:** 221.37
- **Apacer AS510S 64GB:** 219.12
- **SanDisk SSD PLUS 2000GB:** 212.39
- **LITE-ON LITEON CV1-8B128:** 210.69
- **Fanxiang S101 1TB:** 205.17
- **TS128GSSD370S:** 204.76
- NoHomePath | 204.11
- **TEAM T2532TB:** 203.35
- **ADATA SU655:** 199.82
- **R5SL480G:** 196.27
- **SEAGATE ST1000VX000-1ES162:** 192.29
- **MMC64G:** 190.31
- **Micron M4-CT064M4SSD2:** 185.86
- **M.2 1TB:** 180.36
- **Toshiba THNSFK128GCS8:** 174.44
- **Q500S 120GB:** 168.20
- **SEAGATE ST1000DM010-2EP102:** 164.91
- **SEAGATE ST1000LX015-1U7172:** 163.34
- **SCA64G:** 153.28

### Cache/Memory

#### Cache/Memory

- **Intel Core i7-2600K:** 28754.00|8x 6300.00 MHz
- **Intel Core i7-2600S:** 27688.41|8x 3800.00 MHz
- **Intel Xeon E5-2637 v2:** 27624.03|8x 3800,00 MHz
- **Intel Core i7-3720QM:** 26741.11|8x 3600,00 MHz
- **Intel Core i3-3225:** 26109.95|4x 3300.00 MHz
- **Intel Core i7 870:** 24755.71|8x 2934,00 MHz
- **AMD Ryzen Embedded R1505G:** 24321.46|4x 2400,00 MHz
- **AMD Ryzen 3 3300U:** 24105.07|4x 2100.00 MHz
- **Intel Core i3-5020U:** 23943.94|4x 2100,00 MHz
- **Intel Core i5-2300:** 23768.91|4x 3100,00 MHz
- **Intel Core i7-3930K:** 23608.44|12x 3800.00 MHz
- **Intel Xeon W5590:** 23399.02|1x 3459,00 MHz + 3x 3457,00 MHz
- Rockchip RK3588 | 22913.70|4x 2256.00 MHz + 4x 1800.00 MHz
- **Intel Celeron 7305:** 22897.80|1x 1100.00 MHz + 4x 900.00 MHz
- **Rockchip RK3588:** 22794.23|4x 2400.00 MHz + 4x 1800.00 MHz
- **Intel Xeon E3-1265L V2:** 22682.70|8x 3500.00 MHz
- **Intel Pentium G850:** 22108.01|2x 2900,00 MHz
- **Intel Core i7-2635QM:** 21742.69|8x 2900.00 MHz
- **AMD A10-7890K:** 21515.28|4x 4100,00 MHz
- **Intel Core i3-3130M:** 21499.68|4x 2600,00 MHz
- **Intel Core i3 550:** 21391.34|4x 3200,00 MHz
- **Intel Xeon E5-1603 0:** 21142.39|4x 2800.00 MHz
- **Intel Core i7-3610QM:** 21127.43|8x 2300.00 MHz
- **Intel Celeron N5095A:** 21120.74|4x 2900,00 MHz
- **AMD Ryzen 3 2200U:** 21062.17|4x 2500.00 MHz

# VPC-3588 Mainboard Specification

## Changelog

* **1.0.0 — 2023-04-24**: Chinese and English merged version.
* **1.0.1 — 2023-05-26**: Reset button update to the mainboard picture; increased system interface definitions.
* **1.1.0 — 2023-06-16**: Updated based on V2.0 hardware version.
* **1.1.1 — 2023-08-07**: Updated description of the extended serial port and PCIe×4 hard disk slot.

## Contents

1. [RK3588 Brief](#1-rk3588-brief)
2. [Product Overview](#2-product-overview)
3. [Specification List](#3-specification-list)
4. [Interface Definition](#4-interface-definition)
5. [Physical Size](#5-physical-size)
6. [Assemble Precautions](#6-assemble-precautions)
7. [Software Guide](#7-software-guide)

---

## 1. RK3588 Brief

* **CPU:** Octa-core 64-bit big.LITTLE (4× Cortex-A76 + 4× Cortex-A55), up to 2.4 GHz (8 nm).
* **GPU:** ARM Mali-G610 MC4; supports OpenGL ES 1.1/2.0/3.1/3.2, OpenCL 1.1/1.2/2.0, Vulkan 1.1/1.2; 2D acceleration.
* **NPU:** 6 TOPS; supports INT4/INT8/INT16/FP16/BF16/TF32.
* **Multimedia:** Decode H.265/H.264/AV1/VP9/AVS2 up to 8K@60 fps; encode H.264/H.265 up to 8K@30 fps.
* **Display:** Multi-display up to 8K@60 fps; interfaces: eDP/DP/HDMI 2.1/MIPI.
* **Video input:** MIPI CSI-2 (4×4 lanes or 4×2 + 2×4 lanes) and DVP; 32 MP ISP with HDR/3DNR; HDMI 2.0 input up to 4K@60 fps.
* **High-speed I/O:** PCIe 3.0/2.0, SATA 3.0, RGMII, USB Type-C, USB 3.1/2.0.
  *Note: CPU internal features; board support depends on described interfaces.*

## 2. Product Overview

The VPC-3588 mainboard uses Rockchip RK3588 (low-power, high-performance) with 4×A76 + 4×A55 and Mali-G610 MC4 up to 2.4 GHz. It supports 8K@60 fps decode and 8K@60 fps output. The compact design targets ultra-thin applications, integrating easily into final products such as digital signage, touch systems, consumer electronics, and entertainment systems.

## 3. Specification List

| Function / Interface  | Detailed Description                                                                                                     |
| --------------------- | ------------------------------------------------------------------------------------------------------------------------ |
| **CPU**               | RK3588 (4×A76 + 4×A55), up to 2.4 GHz                                                                                    |
| **DDR**               | LPDDR4 2 GB (options: 4/8/16/32 GB)                                                                                      |
| **Storage**           | On-board eMMC 16 GB (scalable to 128 GB)                                                                                 |
| **LVDS**              | 30-pin dual LVDS (VESA/JEITA) up to 1080p                                                                                |
| **MIPI-DSI**          | 31-pin FPC up to 1920×1200                                                                                               |
| **HDMI Output**       | HDMI 2.1 up to 8K                                                                                                        |
| **HDMI Input**        | HDMI 2.0/1.4b up to 1080p@60                                                                                             |
| **eDP**               | 20-pin, 1–4 lanes up to 4K@60 (optional)                                                                                 |
| **VGA Output**        | DB-15 + 9-pin header up to 1080p                                                                                         |
| **4K LCD (VBO)**      | 51-pin VBO display interface                                                                                             |
| **Amplifier Output**  | Dual 6 W @ 8 Ω                                                                                                           |
| **Headphone Out**     | Stereo (3.5 mm jack)                                                                                                     |
| **MIC In**            | Differential MIC (3.5 mm jack)                                                                                           |
| **USB**               | 4 horizontal (USB 3.0 Host×3 + USB 3.0 OTG×1), 1 vertical USB 3.0; 7 pin-headers (USB 2.0 Hub×6 + USB 2.0 Host×1 direct) |
| **Serial Ports**      | 1× TTL; 2× TTL/RS-232/RS-485; 2× TTL/RS-232; 4× extended TTL/RS-232                                                      |
| **Camera**            | USB camera up to 8 MP                                                                                                    |
| **Wi-Fi**             | SDIO Wi-Fi 6 (802.11a/b/g/n/ac/ax), 2.4/5 GHz                                                                            |
| **Bluetooth**         | Serial BT supporting v2.1+EDR/v3.0/v3.0+HS/v4.0/v5.0                                                                     |
| **Ethernet**          | 1× RJ45 10/100/1000M + 4-pin PoE header                                                                                  |
| **m-PCIe 4G**         | 1× industry-standard m-PCIe                                                                                              |
| **Backlight Control** | 2× headers (LVDS & eDP) with on/off and brightness control                                                               |
| **GPIO**              | Up to 8× GPIO (3.3 V)                                                                                                    |
| **I²C Bus**           | I²C header (e.g., capacitive touch)                                                                                      |
| **Security**          | 1× Tamper port                                                                                                           |
| **mSATA**             | 1× standard mSATA                                                                                                        |
| **SATA**              | 4× SATA 3.0 + power headers                                                                                              |
| **PCIe SSD**          | 1× 64-pin splint PCIe x4 hard-disk socket (optional)                                                                     |
| **RTC**               | CR1220, low-power RTC with timer/alarm                                                                                   |
| **LED**               | Red: standby; Green: running                                                                                             |
| **Fans**              | SYS fan power; CPU fan power                                                                                             |
| **Buttons**           | Recovery, Power, Reset                                                                                                   |
| **DC Input**          | 9–15 V DC                                                                                                                |
| **Environment**       | −20 °C to 70 °C; 0–95% RH (non-condensing)                                                                               |
| **Size**              | 170×170×17 mm; PCB top height 15.5 mm                                                                                    |
| **OS**                | Android 12; Linux Qt / Ubuntu 20.04 / Debian 11                                                                          |

## 4. Interface Definition

### J1 — DC-12 V Socket

* Barrel jack: **positive outer, negative inner**; inner pin Ø 2.0 mm, outer ring Ø 5.5 mm.

### J2 — Micro-SIM Card Socket

* Standard Micro-SIM. Insert with the gap facing outward.

### J3 — SATA Power Supply Header 2 (SIP 2.0 mm; square pad = Pin 1)

|                                                                                                  Pin | Definition | Note              |
| ---------------------------------------------------------------------------------------------------: | ---------- | ----------------- |
|                                                                                                    1 | 12V        | Power output 12 V |
|                                                                                                    2 | GND        | Power ground      |
|                                                                                                    3 | GND        | Power ground      |
|                                                                                                    4 | 5V         | Power output 5 V  |
| **Note:** Max output current for 12 V and 5 V is ≤ 1 A. For 3.5″ HDDs, use external power if needed. |            |                   |

### J4 — SATA Power Supply Header 1 (SIP 2.0 mm; square pad = Pin 1)

|                                  Pin | Definition | Note              |
| -----------------------------------: | ---------- | ----------------- |
|                                    1 | 12V        | Power output 12 V |
|                                    2 | GND        | Power ground      |
|                                    3 | GND        | Power ground      |
|                                    4 | 5V         | Power output 5 V  |
| **Note:** Same current limits as J3. |            |                   |

### J5 — I²C Bus Header (SIP 2.0 mm; square pad = Pin 1)

| Pin | Definition | Note                        |
| --: | ---------- | --------------------------- |
|   1 | GND        | Digital ground              |
|   2 | INT        | Interrupt in (3.3 V)        |
|   3 | SDA        | I²C data                    |
|   4 | SCL        | I²C clock                   |
|   5 | RST        | Mainboard reset out (3.3 V) |
|   6 | 3V3        | +3.3 V power out            |

### J6 — m-PCIe 4G Socket

* Standard m-PCIe 4G; USB signals sourced from **Hub USB-6**.

### J7 — 3-Pole Headphone Out (CTIA 3.5 mm)

* Supports insert detection for speaker mute.

### J8 — CAN Header 0 (SIP 2.0 mm; square pad = Pin 1)

|                                                            Pin | Definition                        |
| -------------------------------------------------------------: | --------------------------------- |
|                                                              1 | VSS (DGND)                        |
|                                                              2 | L (Data)                          |
|                                                              3 | H (Data)                          |
|                                                              4 | VCC (default 3.3 V; 5 V optional) |
| **Note:** Software interface `can0`. Requires U9500 populated. |                                   |

### J9 — Speaker Header (SIP 2.0 mm; square pad = Pin 1)

| Pin | Definition |
| --: | ---------- |
|   1 | R+         |
|   2 | R−         |
|   3 | L−         |
|   4 | L+         |

### J10 — HDMI Output

* Standard HDMI out.

### J12 — CAN Header 1 (SIP 2.0 mm; square pad = Pin 1)

|                                                                                                      Pin | Definition                        |
| -------------------------------------------------------------------------------------------------------: | --------------------------------- |
|                                                                                                        1 | VSS (DGND)                        |
|                                                                                                        2 | L (Data)                          |
|                                                                                                        3 | H (Data)                          |
|                                                                                                        4 | VCC (default 3.3 V; 5 V optional) |
| **Note:** Software interface `can1`. Populate U9828. For dual CAN, populate U9500 (CAN0) + U9828 (CAN1). |                                   |

### J13 — KIO Keypad Header (SIP 2.0 mm; square pad = Pin 1)

| Pin | Definition     |
| --: | -------------- |
|   1 | 3V3            |
|   2 | K1 (GPIO #152) |
|   3 | K2 (GPIO #153) |
|   4 | K3 (GPIO #154) |
|   5 | K4 (GPIO #155) |
|   6 | K5 (GPIO #156) |
|   7 | K6 (GPIO #157) |
|   8 | K7 (GPIO #129) |
|   9 | K8 (GPIO #130) |
|  10 | GND            |

### J14 — Front Panel Header (DIP 2.54 mm; square pad = Pin 1)

| Pin | Definition      | Pin | Definition         |
| --: | --------------- | --: | ------------------ |
|   1 | HD+ (Run LED +) |   2 | LED+ (Power LED +) |
|   3 | HD− (Run LED −) |   4 | LED− (Power LED −) |
|   5 | RES−            |   6 | PW−                |
|   7 | RES+            |   8 | PW+                |
|   9 | GND             |  10 | Null               |

### J15 — LVDS Header (DIP 2.0 mm; square pad = Pin 1)

| Pin | Definition | Pin | Definition |
| --: | ---------- | --: | ---------- |
|   1 | VLCD       |   2 | VLCD       |
|   3 | VLCD       |   4 | GND        |
|   5 | GND        |   6 | GND        |
|   7 | RXO0−      |   8 | RXO0+      |
|   9 | RXO1−      |  10 | RXO1+      |
|  11 | RXO2−      |  12 | RXO2+      |
|  13 | GND        |  14 | GND        |
|  15 | RXOC−      |  16 | RXOC+      |
|  17 | RXO3−      |  18 | RXO3+      |
|  19 | RXE0−      |  20 | RXE0+      |
|  21 | RXE1−      |  22 | RXE1+      |
|  23 | RXE2−      |  24 | RXE2+      |
|  25 | GND        |  26 | GND        |
|  27 | RXEC−      |  28 | RXEC+      |
|  29 | RXE3−      |  30 | RXE3+      |

### J17 — PoE PD Header (SIP 2.0 mm; square pad = Pin 1)

|                                                                                                                                                              Pin | Definition                 |
| ---------------------------------------------------------------------------------------------------------------------------------------------------------------: | -------------------------- |
|                                                                                                                                                                1 | CT1 (Transformer Center 1) |
|                                                                                                                                                                2 | CT2                        |
|                                                                                                                                                                3 | CT3                        |
|                                                                                                                                                                4 | CT4                        |
| **Note:** Power for the PoE receiver comes via J61 (Ethernet). Requires external PoE PD board to convert to 12 V. Supported: 1/2(+) & 3/6(−) or 4/5(+) & 7/8(−). |                            |

### J18 — HDMI Input

* Standard HDMI input.

### J19 — LVDS Backlight Control (SIP 2.0 mm; square pad = Pin 1)

| Pin | Definition | Note                        |
| --: | ---------- | --------------------------- |
|   1 | 12V        | If > 2 A, use external 12 V |
|   2 | 12V        | If > 2 A, use external 12 V |
|   3 | EN         | Default output 5 V          |
|   4 | ADJ        | 3.3 V square wave (1 kHz)   |
|   5 | GND        | Power ground                |
|   6 | GND        | Power ground                |

### J20 — eDP Header (DIP 2.0 mm; square pad = Pin 1)

| Pin | Definition | Pin | Definition |
| --: | ---------- | --: | ---------- |
|   1 | VLCD       |   2 | VLCD       |
|   3 | GND        |   4 | GND        |
|   5 | TX0−       |   6 | TX0+       |
|   7 | TX1−       |   8 | TX1+       |
|   9 | TX2−       |  10 | TX2+       |
|  11 | TX3−       |  12 | TX3+       |
|  13 | GND        |  14 | GND        |
|  15 | AUX−       |  16 | AUX+       |
|  17 | GND        |  18 | GND        |
|  19 | 3.3V       |  20 | NC         |

### J21 — eDP VLCD Voltage Select (DIP 2.0 mm; square pad = Pin 1)

* Short **1–2** → VLCD @ **12 V** (J20)
* Short **3–4** → VLCD @ **5 V** (J20)
* Short **5–6** → VLCD @ **3.3 V** (J20)
  *Set according to panel requirements. Incorrect settings may damage panel/board.*

### J22 — Wi-Fi Antenna IPEX

* Standard IPEX 3 dBi connector (⌀ 2.0 mm).

### J23 — MIPI Panel FPC (FPC-0.3 mm, 31-pin, Top/Bottom contact)

|                                                                                                        Pin | Definition    | Pin | Definition                                        |
| ---------------------------------------------------------------------------------------------------------: | ------------- | --: | ------------------------------------------------- |
|                                                                                                          1 | LED+          |   2 | LED+                                              |
|                                                                                                          3 | LED+          |   4 | NC                                                |
|                                                                                                          5 | LED−          |   6 | LED−                                              |
|                                                                                                          7 | LED−          |   8 | LED−                                              |
|                                                                                                          9 | GND           |  10 | GND                                               |
|                                                                                                         11 | MIPI_D2P      |  12 | MIPI_D2N                                          |
|                                                                                                         13 | GND           |  14 | MIPI_D1P                                          |
|                                                                                                         15 | MIPI_D1N      |  16 | GND                                               |
|                                                                                                         17 | MIPI_CKP      |  18 | MIPI_CKN                                          |
|                                                                                                         19 | GND           |  20 | MIPI_D0P                                          |
|                                                                                                         21 | MIPI_D0N      |  22 | GND                                               |
|                                                                                                         23 | MIPI_D3P      |  24 | MIPI_D3N                                          |
|                                                                                                         25 | GND           |  26 | VDD-1V8 *(default NC; fit R9232 = 0 Ω to enable)* |
|                                                                                                         27 | RESET (1.8 V) |  28 | GND                                               |
|                                                                                                         29 | VDD-1V8       |  30 | VDD-3V3                                           |
|                                                                                                         31 | VDD-3V3       |     |                                                   |
| *Backlight current default ≈ 160 mA. With (200/160)×2 ≈ 2.5 Ω, use **2× 2.49 Ω 0603** for R117 and R9223.* |               |     |                                                   |

### J24 — eDP Backlight Control (SIP 2.0 mm; square pad = Pin 1)

| Pin | Definition | Note                        |
| --: | ---------- | --------------------------- |
|   1 | 12V        | If > 2 A, use external 12 V |
|   2 | 12V        | If > 2 A, use external 12 V |
|   3 | EN         | Default output 5 V          |
|   4 | ADJ        | 3.3 V square wave (1 kHz)   |
|   5 | GND        | Power ground                |
|   6 | GND        | Power ground                |

### J25 — DC-12 V Input Header (SIP 2.54 mm; square pad = Pin 1)

|                     Pin | Definition  |
| ----------------------: | ----------- |
|                       1 | GND         |
|                       2 | GND         |
|                       3 | 12V (input) |
|                       4 | 12V (input) |
| *Same input as J1/J39.* |             |

### J26 — VGA Output Jack

* DB-15 VGA output.

### J27 — Dual-Tier LED

* Lower LED: power; Upper LED: software-controlled activity.

### J29 — USB 2.0 DIP Header (DIP 2.54 mm; square pad = Pin 1)

|                               Pin | Definition | Pin | Definition |
| --------------------------------: | ---------- | --: | ---------- |
|                                 1 | 5V         |   2 | 5V         |
|                                 3 | D−         |   4 | D−         |
|                                 5 | D+         |   6 | D+         |
|                                 7 | GND        |   8 | GND        |
|                                 9 | Null       |  10 | GND        |
| *Hub×7 expansion of USB20_HOST1.* |            |     |            |

### J30 — USB 2.0 DIP Header (DIP 2.54 mm; square pad = Pin 1)

|                               Pin | Definition | Pin | Definition |
| --------------------------------: | ---------- | --: | ---------- |
|                                 1 | 5V         |   2 | 5V         |
|                                 3 | D−         |   4 | D−         |
|                                 5 | D+         |   6 | D+         |
|                                 7 | GND        |   8 | GND        |
|                                 9 | Null       |  10 | GND        |
| *Hub×7 expansion of USB20_HOST1.* |            |     |            |

### J31 — USB 2.0 Host Header (SIP 2.0 mm; square pad = Pin 1)

|                               Pin | Definition |
| --------------------------------: | ---------- |
|                                 1 | GND        |
|                                 2 | D+         |
|                                 3 | D−         |
|                                 4 | 5V         |
| *Hub×7 expansion of USB20_HOST1.* |            |

### J33 — USB 2.0 Host Header (SIP 2.0 mm; square pad = Pin 1)

|                               Pin | Definition |
| --------------------------------: | ---------- |
|                                 1 | GND        |
|                                 2 | D+         |
|                                 3 | D−         |
|                                 4 | 5V         |
| *Hub×7 expansion of USB20_HOST1.* |            |

### J34 — USB 2.0 Host Direct (SIP 2.0 mm; square pad = Pin 1)

|                        Pin | Definition |
| -------------------------: | ---------- |
|                          1 | GND        |
|                          2 | D+         |
|                          3 | D−         |
|                          4 | 5V         |
| *Direct from USB20_HOST0.* |            |

### J35 — Double USB 3.0 Type-A

### J36 — USB 3.0 Type-A

### J37 — Dual USB 3.0 Type-A

* **Note (J37):** Upper port = OTG pass-through (default firmware flashing/debug); Lower port = USB 3.0 1×4 Hub group.

### J38 — LVDS VLCD Voltage Select (DIP 2.0 mm; square pad = Pin 1)

* Short **1–2** → VLCD @ **12 V** (J15)
* Short **3–4** → VLCD @ **5 V** (J15)
* Short **5–6** → VLCD @ **3.3 V** (J15)
  *Set per panel; incorrect settings can cause damage.*

### J39 — DC-12 V Input Header (SIP 2.0 mm; square pad = Pin 1)

| Pin | Definition         |
| --: | ------------------ |
|   1 | 12V DC in (9–15 V) |
|   2 | 12V DC in (9–15 V) |
|   3 | GND                |
|   4 | GND                |

### J40 — mSATA Socket

* Standard mSATA module support.

### J41 — SYS Fan Power (SIP 2.0 mm; square pad = Pin 1, GPIO #76 low-active)

| Pin | Definition     |
| --: | -------------- |
|   1 | GND            |
|   2 | 12V (switched) |

### J42 — CPU Fan Power (SIP 2.0 mm; square pad = Pin 1, GPIO #77 low-active)

| Pin | Definition     |
| --: | -------------- |
|   1 | GND            |
|   2 | 12V (switched) |

### J43–J46 — SATA Data Sockets 1–4

* Standard 7-pin SATA data.

### J47 — MIC-IN Header (SIP 2.0 mm; square pad = Pin 1)

| Pin | Definition       |
| --: | ---------------- |
|   1 | DET (Mic detect) |
|   2 | MIC-R (Right)    |
|   3 | GND              |

### J48 — VBO 4K LCD Cable Socket

* I-PEX, 0.5 mm, 51-pin (bottom contact; square pad = Pin 1).

### J49 — Tamper Header (SIP 2.0 mm; square pad = Pin 1)

| Pin | Definition                                |
| --: | ----------------------------------------- |
|   1 | − (Line cathode)                          |
|   2 | + (High/low input; read 1/0; SW GPIO #10) |

### J50 — Data Serial Port 0 (SIP 2.0 mm; square pad = Pin 1)

* Default level: **RS-485**; configurable to TTL/RS-232.
  RS-232 if **U9825** mounted; RS-485 if **U9858** mounted.
  Device node: **`/dev/ttyS0`**.
  | Pin | Definition |
  |---:|---|
  | 1 | GND |
  | 2 | RX|A (TTL/RS-232/RS-485) |
  | 3 | TX|B (TTL/RS-232/RS-485) |
  | 4 | VCC (3.3 V default; 5 V option) |

### J51 — Data Serial Port 2 (SIP 2.0 mm; square pad = Pin 1)

* Level: **TTL only**. Device node: **`/dev/ttyS2`**.
  | Pin | Definition |
  |---:|---|
  | 1 | GND |
  | 2 | RX (TTL) |
  | 3 | TX (TTL) |
  | 4 | VCC (3.3 V default; 5 V option) |
  *Note:* If repurposing the debug UART, custom software is required. Startup logs are output for the first ~5 s at power-on; peer must handle this.

### J52 — Data Serial Port 3 (SIP 2.0 mm; square pad = Pin 1)

* Default level: **TTL**; configurable to RS-232/RS-485.
  RS-232 if **U9825**; RS-485 if **U9823**.
  Device node: **`/dev/ttyS3`**.
  | Pin | Definition |
  |---:|---|
  | 1 | GND |
  | 2 | RX|A (TTL/RS-232/RS-485) |
  | 3 | TX|B (TTL/RS-232/RS-485) |
  | 4 | VCC (3.3 V default; 5 V option) |

### J53 — Data Serial Port 5 (SIP 2.0 mm; square pad = Pin 1)

* Default level: **RS-232**; configurable to TTL (RS-232 if **U9826**).
  Device node: **`/dev/ttyS5`**.
  | Pin | Definition |
  |---:|---|
  | 1 | GND |
  | 2 | RX (TTL/RS-232) |
  | 3 | TX (TTL/RS-232) |
  | 4 | VCC (3.3 V default; 5 V option) |

### J54 — Data Serial Port 8 (SIP 2.0 mm; square pad = Pin 1)

* Default level: **RS-232**; configurable to TTL (RS-232 if **U9826**).
  Device node: **`/dev/ttyS8`**.
  | Pin | Definition |
  |---:|---|
  | 1 | GND |
  | 2 | RX (TTL/RS-232) |
  | 3 | TX (TTL/RS-232) |
  | 4 | VCC (3.3 V default; 5 V option) |

### J55 — Extended Serial Port 1 (SIP 2.0 mm; square pad = Pin 1)

* TTL 3.3 V optional; configurable to RS-232 (**U36**).
  Device node: **`/dev/ttyP0`**.
  | Pin | Definition |
  |---:|---|
  | 1 | GND |
  | 2 | RX (TTL/RS-232) |
  | 3 | TX (TTL/RS-232) |
  | 4 | VCC (3.3 V default; 5 V option) |

### J56 — Extended Serial Port 2 (SIP 2.0 mm; square pad = Pin 1)

* TTL 3.3 V optional; configurable to RS-232 (**U36**).
  Device node: **`/dev/ttyP1`**.
  | Pin | Definition |
  |---:|---|
  | 1 | GND |
  | 2 | RX (TTL/RS-232) |
  | 3 | TX (TTL/RS-232) |
  | 4 | VCC (3.3 V default; 5 V option) |

### J57 — Extended Serial Port 3 (SIP 2.0 mm; square pad = Pin 1)

* TTL 3.3 V optional; configurable to RS-232 (**U37**).
  Device node: **`/dev/ttyP2`**.
  | Pin | Definition |
  |---:|---|
  | 1 | GND |
  | 2 | RX (TTL/RS-232) |
  | 3 | TX (TTL/RS-232) |
  | 4 | VCC (3.3 V default; 5 V option) |

### J58 — Extended Serial Port 4 (SIP 2.0 mm; square pad = Pin 1)

* TTL 3.3 V optional; configurable to RS-232 (**U37**).
  Device node: **`/dev/ttyP3`**.
  | Pin | Definition |
  |---:|---|
  | 1 | GND |
  | 2 | RX (TTL/RS-232) |
  | 3 | TX (TTL/RS-232) |
  | 4 | VCC (3.3 V default; 5 V option) |

### J59 — MIC-IN Jack

* 3.5 mm; supports 2-segment and 3-segment microphones.

### J60 — Audio Extension Interface (Front-panel audio; DIP 2.54 mm; square pad = Pin 1)

| Pin | Definition | Pin | Definition |
| --: | ---------- | --: | ---------- |
|   1 | MIC-L      |   2 | GND        |
|   3 | MIC-R      |   4 | SENSE      |
|   5 | HP-R       |   6 | MIC-DEL    |
|   7 | GND        |   8 | Null       |
|   9 | HP-L       |  10 | HP-DEL     |

### J61 — RJ45 Internal Gigabit Ethernet

* Internal RJ45 jack.

### J62 — VGA Output Header (DIP 2.54 mm; square pad = Pin 1)

| Pin | Definition | Pin | Definition |
| --: | ---------- | --: | ---------- |
|   1 | RED        |   2 | GND        |
|   3 | GRN        |   4 | GND        |
|   5 | BLUE       |   6 | GND        |
|   7 | HS         |   8 | DATA       |
|   9 | VS         |  10 | CLK        |

### PCIe×4 — 64-Pin Splint Hard-Disk Socket

* PCIe x4 slot mainly for PCIe SSD expansion.

### Recovery Mode Button (SW1)

* Hold ~3 s during power-on to enter recovery.

### System Reset Button (SW2)

* Click to reboot.

---

## 5. Physical Size

* PCB: **170 mm × 170 mm**; mounting hole Ø **3.3 mm**.
* For detailed dimensions, request the DXF from the manufacturer.

## 6. Assemble Precautions

1. Relative humidity: **10–90%**, non-condensing.
2. Operating temperature: **−20 °C to 70 °C**.
3. Storage temperature: **−40 °C to 70 °C**.
4. Use ESD protection during assembly/transport.
5. Keep interface cables short to maintain signal integrity.
6. Do not bend or stress the board during assembly.
7. Avoid shorts between the mainboard and peripherals.
8. For LVDS/eDP panels: verify panel voltage/current and connector Pin-1 orientation.
9. For backlight: verify required voltage and current.
10. For USB/GPIO/Serial/I²C/SPI/HDMI, ensure peripheral I/O levels and current are compatible. Power pins on general headers ≤ **100 mA**; USB power pins ≤ **500 mA**.
11. Supply power via the power input jack/header and size current budget for total peripherals. **Do not** power from the backlight connector.
12. Communication modules should be ≥ **5 mm** from metal housings to reduce interference.

## 7. Software Guide

* The mainboard supports dual/triple-display combinations among **LVDS/eDP/MIPI/HDMI/VGA**. For specific multi-display combos, obtain the corresponding patches from the manufacturer.

**Serial device nodes:**

| Port                | Device Node                   |
| ------------------- | ----------------------------- |
| J50                 | `/dev/ttyS0`                  |
| J51                 | `/dev/ttyS2`                  |
| J52                 | `/dev/ttyS3`                  |
| J53                 | `/dev/ttyS5`                  |
| J54                 | `/dev/ttyS8`                  |
| J55                 | `/dev/ttyP0` or `/dev/ttyS10` |
| J56                 | `/dev/ttyP1` or `/dev/ttyS11` |
| J57                 | `/dev/ttyP2` or `/dev/ttyS12` |
| J58                 | `/dev/ttyP3` or `/dev/ttyS13` |
"""  
~~~~~~
`````````  
  ]]</constraints_text>
  
</input_data>

<instructions>
  <instruction>1) Extract an environment summary from [[constraints_text]] as factual key-value fields (arch, OS, language runtimes, RAM, swap, and any runtime that is explicitly missing).</instruction>
  <instruction>2) Parse [[solutions_100]] into a normalized list of 100 items with fields: id, original_rank, name (if present), description, declared_tools, declared_runtimes/services (search in external knowledge if those informations are not present).</instruction>
  <instruction>3) For each solution, label which domains it covers: {tag_recommendation, similarity_search, document_clustering, near_duplicate_detection}. Use “partial” when the solution only provides a building block.</instruction>
  <instruction>4) Build candidate bundles by combining complementary solutions so every bundle fully covers all 4 domains. Keep bundles minimal.</instruction>
  <instruction>5) For each bundle, write an explicit “capability mapping”:
    - name the approach for each domain (e.g., TF-IDF keywords, supervised classifier, HNSW ANN, k-means, HDBSCAN, MinHash, SimHash),
    - list explicit tools/libraries drawn from the included solutions,
    - if any tool/runtimes are unstated, leverage external knowledge via search queries.</instruction>
  <instruction>6) Score each bundle:
    - compute ease_of_deployment strictly via the <ease_of_deployment><rubric>,
    - compute coverage strictly via the <coverage><rubric>,
    - include a bullet “scoring_rationale” with (a) the applied rubric penalties/bonuses and (b) supporting evidence quotes.</instruction>
  <instruction>7) Rank bundles by: ease_of_deployment DESC, then coverage DESC, then fewer services/toolchains.</instruction>
  <instruction>8) For each ranked bundle, provide:
    - included_solution_ids,
    - excluded_but_relevant_solution_ids (high-ranked originals you did not use) with reasons tied to constraints or redundancy,
    - a pragmatic implementation plan (high-level steps) and a plain-text pipeline diagram.
    Do not claim specific packages exist in Debian repos unless explicitly stated in [[constraints_text]] or [[solutions_100]] or found within external knowledge search results.</instruction>
</instructions>